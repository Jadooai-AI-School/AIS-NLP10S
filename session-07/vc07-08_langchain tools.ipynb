{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain Examples Notebook: Tools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from ais_utils.Model_from_LC_Ollama import get_chatLLM, get_chatOpenAI, get_chatLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHECKING IF GPU IS ENABLED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Code to Check LLM functioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chatllm = get_chatOpenAI()\n",
    "llm = get_chatLLM('llama3.2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### --- 2. Tools ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semantic Scholar API Tool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start by installing semanticscholar api\n",
    "#!pip install --upgrade --quiet  semanticscholar\n",
    "#!pip install --upgrade --quiet  langchainhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tjamil/Desktop/CLASS/AIS-NLP10S/.venv/lib/python3.12/site-packages/langsmith/client.py:278: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "instructions = \"\"\"You are an expert researcher.\"\"\"\n",
    "base_prompt = hub.pull(\"langchain-ai/openai-functions-template\")\n",
    "prompt = base_prompt.partial(instructions=instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.semanticscholar.tool import SemanticScholarQueryRun\n",
    "\n",
    "tools = [SemanticScholarQueryRun()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the tool calling agent\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "\n",
    "# Create an agent executor by passing in the agent and tools\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `semanticscholar` with `{'query': 'large language model biases and mitigation techniques papers'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mPublished year: 2024\n",
      "Title: A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models\n",
      "Authors: S. Tonmoy, S. M. M. Zaman, Vinija Jain, Anku Rani, Vipula Rawte, Aman Chadha, Amitava Das\n",
      "Abstract: As Large Language Models (LLMs) continue to advance in their ability to write human-like text, a key challenge remains around their tendency to hallucinate generating content that appears factual but is ungrounded. This issue of hallucination is arguably the biggest hindrance to safely deploying these powerful LLMs into real-world production systems that impact people's lives. The journey toward widespread adoption of LLMs in practical settings heavily relies on addressing and mitigating hallucinations. Unlike traditional AI systems focused on limited tasks, LLMs have been exposed to vast amounts of online text data during training. While this allows them to display impressive language fluency, it also means they are capable of extrapolating information from the biases in training data, misinterpreting ambiguous prompts, or modifying the information to align superficially with the input. This becomes hugely alarming when we rely on language generation capabilities for sensitive applications, such as summarizing medical records, financial analysis reports, etc. This paper presents a comprehensive survey of over 32 techniques developed to mitigate hallucination in LLMs. Notable among these are Retrieval Augmented Generation (Lewis et al, 2021), Knowledge Retrieval (Varshney et al,2023), CoNLI (Lei et al, 2023), and CoVe (Dhuliawala et al, 2023). Furthermore, we introduce a detailed taxonomy categorizing these methods based on various parameters, such as dataset utilization, common tasks, feedback mechanisms, and retriever types. This classification helps distinguish the diverse approaches specifically designed to tackle hallucination issues in LLMs. Additionally, we analyze the challenges and limitations inherent in these techniques, providing a solid foundation for future research in addressing hallucinations and related phenomena within the realm of LLMs.\n",
      "\n",
      "\n",
      "Published year: 2024\n",
      "Title: Large Language Model Prompting Techniques for Advancement in Clinical Medicine\n",
      "Authors: Krish Shah, Andrew Y Xu, Yatharth Sharma, Mohammed Daher, Christopher L. Mcdonald, B. Diebo, Alan H. Daniels\n",
      "Abstract: Large Language Models (LLMs have the potential to revolutionize clinical medicine by enhancing healthcare access, diagnosis, surgical planning, and education. However, their utilization requires careful, prompt engineering to mitigate challenges like hallucinations and biases. Proper utilization of LLMs involves understanding foundational concepts such as tokenization, embeddings, and attention mechanisms, alongside strategic prompting techniques to ensure accurate outputs. For innovative healthcare solutions, it is essential to maintain ongoing collaboration between AI technology and medical professionals. Ethical considerations, including data security and bias mitigation, are critical to their application. By leveraging LLMs as supplementary resources in research and education, we can enhance learning and support knowledge-based inquiries, ultimately advancing the quality and accessibility of medical care. Continued research and development are necessary to fully realize the potential of LLMs in transforming healthcare.\n",
      "\n",
      "\n",
      "Published year: 2024\n",
      "Title: Biases and Trustworthiness Challenges with Mitigation Strategies for Large Language Models in Healthcare\n",
      "Authors: Maneeha Rani, Bhupesh Kumar Mishra, Dhavalkumar Thakker, Muhammad Babar, Will Jones, Ahmad Din\n",
      "Abstract: Rapid innovations in Large Language Models (LLMs) have resulted in remarkably efficient decision-making and learning capacities, specifically in critical sectors such as healthcare. Domain-specific LLMs are progressively being designed for medical pre-screening and diagnostic procedures in healthcare. Despite these advancements, LLMs persist as opaque systems l\u001b[0m\u001b[32;1m\u001b[1;3m**Biases in Large Language Models:**\n",
      "\n",
      "1. **Hallucinations**: generating content that appears factual but is ungrounded.\n",
      "2. **Bias towards certain groups**: perpetuating existing social biases and stereotypes.\n",
      "3. **Lack of diversity in training data**: leading to limited understanding of diverse perspectives and experiences.\n",
      "4. **Overfitting to dominant narratives**: reinforcing dominant ideologies and suppressing minority voices.\n",
      "\n",
      "**Mitigation Techniques:**\n",
      "\n",
      "1. **Retrieval Augmented Generation (RAG)**: using retrieval mechanisms to generate more accurate outputs.\n",
      "2. **Prompt Engineering**: designing prompts that encourage more informative and nuanced responses.\n",
      "3. **Data Augmentation**: increasing diversity in training data to reduce bias.\n",
      "4. **Debiasing Techniques**: applying techniques such as debiasing word embeddings and adversarial training.\n",
      "\n",
      "**Challenges:**\n",
      "\n",
      "1. **Evaluating fairness and accuracy**: developing effective metrics to assess the performance of LLMs.\n",
      "2. **Addressing domain-specific biases**: adapting mitigation techniques to specific domains, such as healthcare.\n",
      "3. **Balancing trade-offs between accuracy and interpretability**: finding ways to balance the need for accurate outputs with the need for interpretable models.\n",
      "\n",
      "**Future Research Directions:**\n",
      "\n",
      "1. **Developing more robust evaluation metrics**: creating effective metrics to assess the fairness and accuracy of LLMs.\n",
      "2. **Investigating the impact of bias on decision-making**: exploring the consequences of biased LLMs in high-stakes applications.\n",
      "3. **Designing more transparent and explainable models**: developing techniques to provide insights into the decision-making processes of LLMs.\n",
      "\n",
      "**Real-World Applications:**\n",
      "\n",
      "1. **Healthcare**: using LLMs for medical diagnosis, patient education, and clinical decision support.\n",
      "2. **Education**: leveraging LLMs for personalized learning, content creation, and educational resources.\n",
      "3. **Customer Service**: applying LLMs to improve customer service, such as chatbots and virtual assistants.\n",
      "\n",
      "**Key Takeaways:**\n",
      "\n",
      "1. **Addressing bias is crucial**: developing techniques to mitigate biases in LLMs is essential for ensuring fairness and accuracy.\n",
      "2. **Transparency and explainability are key**: designing models that provide insights into their decision-making processes is critical for building trust.\n",
      "3. **Ongoing evaluation and improvement are necessary**: regularly assessing the performance of LLMs and adapting mitigation techniques to address emerging challenges.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response = agent_executor.invoke(\n",
    "    {\n",
    "        \"input\": \"What are some biases in the large language models? How have people tried to mitigate them? \"\n",
    "        \"show me a list of papers and techniques. Based on your findings write new research questions (show in bulleted list)\"\n",
    "        \"to work on. Break down the task into subtasks for search. Use the search tool\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(response['input'])\n",
    "#print(response['output'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
