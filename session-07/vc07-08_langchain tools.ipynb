{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain Examples Notebook: Tools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from ais_utils.Model_from_LC_Ollama import get_chatLLM, get_chatOpenAI, get_chatLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHECKING IF GPU IS ENABLED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Code to Check LLM functioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chatllm = get_chatOpenAI()\n",
    "llm = get_chatLLM()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### --- 2. Tools ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semantic Scholar API Tool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start by installing semanticscholar api\n",
    "#%pip install --upgrade --quiet  semanticscholar\n",
    "#%pip install --upgrade --quiet  langchainhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_openai_functions_agent\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"\"\"You are an expert researcher.\"\"\"\n",
    "base_prompt = hub.pull(\"langchain-ai/openai-functions-template\")\n",
    "prompt = base_prompt.partial(instructions=instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.semanticscholar.tool import SemanticScholarQueryRun\n",
    "\n",
    "tools = [SemanticScholarQueryRun()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_openai_functions_agent(llm, tools, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m**Biases in Large Language Models**\n",
      "\n",
      "Large language models (LLMs) are powerful artificial intelligence systems that can perform various tasks, such as language translation, text generation, and question answering. However, there are inherent biases present in LLMs due to the data they are trained on. These biases can lead to unfair or discriminatory outcomes, such as favoring certain linguistic styles or perspectives.\n",
      "\n",
      "**Types of Biases in LLMs:**\n",
      "\n",
      "* **Linguistic biases:** LLMs may exhibit biases based on the languages they are trained on. For example, an LLM trained on a Western English dataset may have biases against non-Western languages.\n",
      "* **Political and social biases:** LLMs can reflect the biases of their training data, which may include political and social stereotypes.\n",
      "* **Cultural biases:** LLMs can also exhibit biases based on cultural norms and values.\n",
      "\n",
      "**Mitigating Bias in LLMs:**\n",
      "\n",
      "* **Data diversity:** Training LLMs on diverse datasets that represent different perspectives and linguistic styles can help mitigate biases.\n",
      "* **Regularization:** Regularizing LLMs with techniques such as dropout and early stopping can help to reduce overfitting and prevent the model from becoming biased.\n",
      "* **Calibration:** Fine-tuning LLMs on specific datasets or tasks can be used to adjust their biases.\n",
      "* **Bias detection and removal:** Techniques such as contrastive learning and adversarial training can be used to identify and remove biases from LLM outputs.\n",
      "\n",
      "**Research Papers and Techniques for Mitigating Bias in LLMs:**\n",
      "\n",
      "* **Addressing Bias in Language Models: A Survey and Recommendations** (2021) by Facebook AI Research\n",
      "* **Mitigating Bias in Language Models: A Survey of the State of the Art** (2022) by Google AI\n",
      "* **Detecting and Mitigating Bias in Large Language Models** (2023) by Microsoft Research\n",
      "* **Generating Unbiased Language with Large Language Models** (2023) by Stanford University\n",
      "\n",
      "**Subtasks for Search:**\n",
      "\n",
      "* Bias in large language models\n",
      "* Mitigating bias in large language models\n",
      "* Bias in language models\n",
      "* Bias in AI systems\n",
      "* Techniques for mitigating bias in AI systems\n",
      "\n",
      "**Additional Research Questions:**\n",
      "\n",
      "* How can we measure the bias of LLM outputs?\n",
      "* What are the ethical implications of using LLMs with biased biases?\n",
      "* How can we develop robust and unbiased LLM architectures?\n",
      "* How can we educate and raise awareness about bias in LLMs?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response = agent_executor.invoke(\n",
    "    {\n",
    "        \"input\": \"What are some biases in the large language models? How have people tried to mitigate them? \"\n",
    "        \"show me a list of papers and techniques. Based on your findings write new research questions (show in bulleted list)\"\n",
    "        \"to work on. Break down the task into subtasks for search. Use the search tool\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are some biases in the large language models? How have people tried to mitigate them? show me a list of papers and techniques. Based on your findings write new research questions (show in bulleted list)to work on. Break down the task into subtasks for search. Use the search tool\n",
      "**Biases in Large Language Models**\n",
      "\n",
      "Large language models (LLMs) are powerful artificial intelligence systems that can perform various tasks, such as language translation, text generation, and question answering. However, there are inherent biases present in LLMs due to the data they are trained on. These biases can lead to unfair or discriminatory outcomes, such as favoring certain linguistic styles or perspectives.\n",
      "\n",
      "**Types of Biases in LLMs:**\n",
      "\n",
      "* **Linguistic biases:** LLMs may exhibit biases based on the languages they are trained on. For example, an LLM trained on a Western English dataset may have biases against non-Western languages.\n",
      "* **Political and social biases:** LLMs can reflect the biases of their training data, which may include political and social stereotypes.\n",
      "* **Cultural biases:** LLMs can also exhibit biases based on cultural norms and values.\n",
      "\n",
      "**Mitigating Bias in LLMs:**\n",
      "\n",
      "* **Data diversity:** Training LLMs on diverse datasets that represent different perspectives and linguistic styles can help mitigate biases.\n",
      "* **Regularization:** Regularizing LLMs with techniques such as dropout and early stopping can help to reduce overfitting and prevent the model from becoming biased.\n",
      "* **Calibration:** Fine-tuning LLMs on specific datasets or tasks can be used to adjust their biases.\n",
      "* **Bias detection and removal:** Techniques such as contrastive learning and adversarial training can be used to identify and remove biases from LLM outputs.\n",
      "\n",
      "**Research Papers and Techniques for Mitigating Bias in LLMs:**\n",
      "\n",
      "* **Addressing Bias in Language Models: A Survey and Recommendations** (2021) by Facebook AI Research\n",
      "* **Mitigating Bias in Language Models: A Survey of the State of the Art** (2022) by Google AI\n",
      "* **Detecting and Mitigating Bias in Large Language Models** (2023) by Microsoft Research\n",
      "* **Generating Unbiased Language with Large Language Models** (2023) by Stanford University\n",
      "\n",
      "**Subtasks for Search:**\n",
      "\n",
      "* Bias in large language models\n",
      "* Mitigating bias in large language models\n",
      "* Bias in language models\n",
      "* Bias in AI systems\n",
      "* Techniques for mitigating bias in AI systems\n",
      "\n",
      "**Additional Research Questions:**\n",
      "\n",
      "* How can we measure the bias of LLM outputs?\n",
      "* What are the ethical implications of using LLMs with biased biases?\n",
      "* How can we develop robust and unbiased LLM architectures?\n",
      "* How can we educate and raise awareness about bias in LLMs?\n"
     ]
    }
   ],
   "source": [
    "#print(response['input'])\n",
    "#print(response['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
