{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain Examples Notebook: Tools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from ais_utils.Model_from_LC_Ollama import get_chatLLM, get_chatOpenAI, get_chatLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHECKING IF GPU IS ENABLED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Code to Check LLM functioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chatllm = get_chatOpenAI()\n",
    "llm = get_chatLLM('llama3.2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### --- 2. Tools ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semantic Scholar API Tool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start by installing semanticscholar api\n",
    "#!pip install --upgrade --quiet  semanticscholar\n",
    "#!pip install --upgrade --quiet  langchainhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tjamil/Desktop/CLASS/AIS-NLP10S/session-07/.conda/lib/python3.11/site-packages/langsmith/client.py:278: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "instructions = \"\"\"You are an expert researcher.\"\"\"\n",
    "base_prompt = hub.pull(\"langchain-ai/openai-functions-template\")\n",
    "prompt = base_prompt.partial(instructions=instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.semanticscholar.tool import SemanticScholarQueryRun\n",
    "\n",
    "tools = [SemanticScholarQueryRun()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the tool calling agent\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "\n",
    "# Create an agent executor by passing in the agent and tools\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `semanticscholar` with `{'query': 'large language model biases and mitigation techniques papers'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mPublished year: 2024\n",
      "Title: A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models\n",
      "Authors: S. Tonmoy, S. M. M. Zaman, Vinija Jain, Anku Rani, Vipula Rawte, Aman Chadha, Amitava Das\n",
      "Abstract: As Large Language Models (LLMs) continue to advance in their ability to write human-like text, a key challenge remains around their tendency to hallucinate generating content that appears factual but is ungrounded. This issue of hallucination is arguably the biggest hindrance to safely deploying these powerful LLMs into real-world production systems that impact people's lives. The journey toward widespread adoption of LLMs in practical settings heavily relies on addressing and mitigating hallucinations. Unlike traditional AI systems focused on limited tasks, LLMs have been exposed to vast amounts of online text data during training. While this allows them to display impressive language fluency, it also means they are capable of extrapolating information from the biases in training data, misinterpreting ambiguous prompts, or modifying the information to align superficially with the input. This becomes hugely alarming when we rely on language generation capabilities for sensitive applications, such as summarizing medical records, financial analysis reports, etc. This paper presents a comprehensive survey of over 32 techniques developed to mitigate hallucination in LLMs. Notable among these are Retrieval Augmented Generation (Lewis et al, 2021), Knowledge Retrieval (Varshney et al,2023), CoNLI (Lei et al, 2023), and CoVe (Dhuliawala et al, 2023). Furthermore, we introduce a detailed taxonomy categorizing these methods based on various parameters, such as dataset utilization, common tasks, feedback mechanisms, and retriever types. This classification helps distinguish the diverse approaches specifically designed to tackle hallucination issues in LLMs. Additionally, we analyze the challenges and limitations inherent in these techniques, providing a solid foundation for future research in addressing hallucinations and related phenomena within the realm of LLMs.\n",
      "\n",
      "\n",
      "Published year: 2024\n",
      "Title: Large Language Model Prompting Techniques for Advancement in Clinical Medicine\n",
      "Authors: Krish Shah, Andrew Y Xu, Yatharth Sharma, Mohammed Daher, Christopher L. Mcdonald, B. Diebo, Alan H. Daniels\n",
      "Abstract: Large Language Models (LLMs have the potential to revolutionize clinical medicine by enhancing healthcare access, diagnosis, surgical planning, and education. However, their utilization requires careful, prompt engineering to mitigate challenges like hallucinations and biases. Proper utilization of LLMs involves understanding foundational concepts such as tokenization, embeddings, and attention mechanisms, alongside strategic prompting techniques to ensure accurate outputs. For innovative healthcare solutions, it is essential to maintain ongoing collaboration between AI technology and medical professionals. Ethical considerations, including data security and bias mitigation, are critical to their application. By leveraging LLMs as supplementary resources in research and education, we can enhance learning and support knowledge-based inquiries, ultimately advancing the quality and accessibility of medical care. Continued research and development are necessary to fully realize the potential of LLMs in transforming healthcare.\n",
      "\n",
      "\n",
      "Published year: 2024\n",
      "Title: Biases and Trustworthiness Challenges with Mitigation Strategies for Large Language Models in Healthcare\n",
      "Authors: Maneeha Rani, Bhupesh Kumar Mishra, Dhavalkumar Thakker, Muhammad Babar, Will Jones, Ahmad Din\n",
      "Abstract: Rapid innovations in Large Language Models (LLMs) have resulted in remarkably efficient decision-making and learning capacities, specifically in critical sectors such as healthcare. Domain-specific LLMs are progressively being designed for medical pre-screening and diagnostic procedures in healthcare. Despite these advancements, LLMs persist as opaque systems l\u001b[0m\u001b[32;1m\u001b[1;3m**Biases in Large Language Models:**\n",
      "\n",
      "1. **Hallucinations**: generating content that appears factual but is ungrounded.\n",
      "2. **Bias towards majority groups**: favoring the perspectives of dominant groups over minority ones.\n",
      "3. **Lack of diversity in training data**: leading to limited understanding of diverse cultures, experiences, and viewpoints.\n",
      "4. **Misinformation and disinformation**: spreading false or misleading information.\n",
      "5. **Stereotyping and bias in language**: perpetuating negative stereotypes and biases.\n",
      "\n",
      "**Mitigation Strategies:**\n",
      "\n",
      "1. **Data curation and diversification**: ensuring training data is representative and diverse.\n",
      "2. **Regular auditing and testing**: identifying and addressing biases and hallucinations.\n",
      "3. **Human oversight and feedback**: incorporating human evaluation to correct errors and improve performance.\n",
      "4. **Explainability techniques**: developing methods to understand and interpret model decisions.\n",
      "5. **Diverse and inclusive development teams**: ensuring a range of perspectives and experiences in the development process.\n",
      "\n",
      "**Applications in Healthcare:**\n",
      "\n",
      "1. **Clinical decision support systems**: using LLMs to provide accurate diagnoses and treatment recommendations.\n",
      "2. **Patient education and engagement**: leveraging LLMs to improve patient understanding and adherence to treatment plans.\n",
      "3. **Medical research and literature analysis**: applying LLMs to analyze large amounts of medical data and identify patterns.\n",
      "4. **Chatbots and virtual assistants**: using LLMs to provide empathetic and informative support to patients.\n",
      "\n",
      "**Future Research Directions:**\n",
      "\n",
      "1. **Developing more transparent and explainable LLMs**: improving understanding of model decisions and biases.\n",
      "2. **Investigating the impact of bias on healthcare outcomes**: studying the effects of biased LLMs on patient care and treatment.\n",
      "3. **Creating more diverse and inclusive training datasets**: ensuring LLMs are trained on representative data that reflects diverse populations.\n",
      "4. **Evaluating the effectiveness of mitigation strategies**: assessing the impact of different approaches to addressing biases and hallucinations in LLMs.\n",
      "\n",
      "By acknowledging these challenges and exploring effective mitigation strategies, we can harness the power of Large Language Models to improve healthcare outcomes and advance medical research.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response = agent_executor.invoke(\n",
    "    {\n",
    "        \"input\": \"What are some biases in the large language models? How have people tried to mitigate them? \"\n",
    "        \"show me a list of papers and techniques. Based on your findings write new research questions (show in bulleted list)\"\n",
    "        \"to work on. Break down the task into subtasks for search. Use the search tool\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(response['input'])\n",
    "#print(response['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
