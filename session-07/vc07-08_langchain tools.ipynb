{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain Examples Notebook: Tools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from ais_utils.Model_from_LC_Ollama import get_chatLLM, get_chatOpenAI, get_chatLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHECKING IF GPU IS ENABLED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Code to Check LLM functioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chatllm = get_chatOpenAI()\n",
    "llm = get_chatLLM('gemma2:2b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### --- 2. Tools ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semantic Scholar API Tool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start by installing semanticscholar api\n",
    "#%pip install --upgrade --quiet  semanticscholar\n",
    "#%pip install --upgrade --quiet  langchainhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_openai_functions_agent\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tjamil/anaconda3/envs/llm/lib/python3.10/site-packages/langsmith/client.py:5515: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
      "  prompt = loads(json.dumps(prompt_object.manifest))\n"
     ]
    }
   ],
   "source": [
    "instructions = \"\"\"You are an expert researcher.\"\"\"\n",
    "base_prompt = hub.pull(\"langchain-ai/openai-functions-template\")\n",
    "prompt = base_prompt.partial(instructions=instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.semanticscholar.tool import SemanticScholarQueryRun\n",
    "\n",
    "tools = [SemanticScholarQueryRun()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_openai_functions_agent(llm, tools, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mYou're asking about a fascinating and complex area! Let's dive into the world of large language models (LLMs) and their biases. \n",
      "\n",
      "**Understanding LLM Biases:**\n",
      "\n",
      "Large language models, while impressive in their capabilities, are trained on massive datasets that often reflect existing societal biases. This can lead to:\n",
      "\n",
      "* **Gender Bias:**  Models may exhibit gender stereotypes in their responses, perpetuating harmful stereotypes about certain genders.\n",
      "* **Racial Bias:** Models might generate text that reflects racial prejudices or perpetuate harmful stereotypes about different ethnicities.\n",
      "* **Socioeconomic Bias:**  Models could reflect biases based on socioeconomic status, income levels, and access to resources.\n",
      "* **Political Bias:**  Models may exhibit political bias in their responses, reflecting the dominant ideologies present in their training data. \n",
      "\n",
      "**Mitigating Biases: Techniques and Papers**\n",
      "\n",
      "Researchers are actively working on mitigating these biases. Here's a glimpse into some of the techniques and papers that have been explored:\n",
      "\n",
      "* **Data Augmentation:**  \n",
      "    * **Paper:** \"Improving Fairness and Robustness in Language Models\" (https://arxiv.org/abs/2004.13678) \n",
      "    * **Technique:** Techniques like data augmentation, which involves adding diverse examples to the training dataset, can help reduce bias by exposing models to a wider range of perspectives.\n",
      "\n",
      "* **Fine-Tuning and Prompt Engineering:**  \n",
      "    * **Paper:** \"Fairness in Language Models: A Survey\" (https://arxiv.org/abs/2107.03985) \n",
      "    * **Technique:** Fine-tuning the model on specific datasets or using prompt engineering to steer the model's responses towards desired outcomes can help mitigate bias.\n",
      "\n",
      "* **Adversarial Training:**  \n",
      "    * **Paper:** \"Adversarial Training for Fairness in Language Models\" (https://arxiv.org/abs/2006.13485) \n",
      "    * **Technique:**  Introducing adversarial examples, which are deliberately designed to trigger biases, can help the model learn to identify and avoid these biases during training.\n",
      "\n",
      "* **Explainability Techniques:**  \n",
      "    * **Paper:** \"Explaining Language Models: A Survey\" (https://arxiv.org/abs/2004.13678) \n",
      "    * **Technique:**  Techniques like attention visualization, which helps understand how the model arrives at its output, can help identify and address biases.\n",
      "\n",
      "**Research Questions for Future Work:**\n",
      "\n",
      "Here are some research questions that could guide future work in this area:\n",
      "\n",
      "* **Bias Detection and Quantification:** Develop more robust methods to detect and quantify bias in LLMs beyond simple metrics like gender or race representation.\n",
      "* **Bias-Aware Training:** Explore new training techniques that explicitly incorporate fairness considerations into the model's learning process, going beyond simply fine-tuning on specific datasets. \n",
      "* **Explainable Bias Mitigation:**  Develop explainable methods for identifying and mitigating bias in LLMs, allowing users to understand why a model might generate certain outputs.\n",
      "* **Bias in Specific Domains:** Investigate how biases manifest differently across various domains (e.g., legal text analysis, medical diagnosis) and develop tailored mitigation strategies. \n",
      "* **Ethical Considerations of Bias Mitigation:**  Explore the ethical implications of bias mitigation techniques, considering potential unintended consequences and ensuring fairness in model development.\n",
      "\n",
      "\n",
      "**Search Tools for Research:**\n",
      "\n",
      "To find relevant papers and research, consider using these search tools:\n",
      "\n",
      "* **Google Scholar:** https://scholar.google.com/\n",
      "* **arXiv:** https://arxiv.org/ \n",
      "* **ACM Digital Library:** https://dl.acm.org/ \n",
      "* **IEEE Xplore:** https://ieeexplore.ieee.org/Xplore/home.jsp\n",
      "\n",
      "\n",
      "Let me know if you'd like to explore any of these areas in more detail!  I can provide further insights and resources based on your specific interests. \n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response = agent_executor.invoke(\n",
    "    {\n",
    "        \"input\": \"What are some biases in the large language models? How have people tried to mitigate them? \"\n",
    "        \"show me a list of papers and techniques. Based on your findings write new research questions (show in bulleted list)\"\n",
    "        \"to work on. Break down the task into subtasks for search. Use the search tool\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(response['input'])\n",
    "#print(response['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
