{"0": {"idx": 0, "content": "CD-ROM Document Database Standard  Ilisin T. Phillips  Department of Computer Science  Seattle University  Seattle, WA 98122  Abstract  The paper presents the design of a comprehensive  standard document database for machine-printed do-  cuments. Our eflort t o  produce a series of carefully  ground-truthed document databases to be issued on  CD-ROhfs is described in detail. The databases can  be uiilited by the OCR and document understanding  community as a common platform to develop, test and  evaluate their algorithms.  1 Introduction  Systems that do OCR or any aspect of Document  Image Understanding must work nearly perfectly over  a broad range of document conditions and types in or-  der to be really useful. To develop algorithms for OCR  or to develop algorithms for Document Image Under-  standing requires that the developer have a suitable  database of documents which are accurately ground-  truthed so that the free parameters of the algoritliins  can be estimated. Customers of OCR or Document  Image Understanding systems likewise must have a  suitable database in order that they may accurately  evaluate vendor proposed systems.  Database requirements for both the developer and  the customer are nearly identical. Therefore, in order  to help both developer and customer, there must. be  the creation of a comprehensive series of databases,  each specialized to a given subset of document types,  or intended as additions to already created databases.", "metadata": {"source": "data/2_phillips_1993_cdrom.pdf", "page_number": 1, "chunk": 0, "title": "CD-ROM document database standard"}}, "1": {"idx": 1, "content": "Throughout the history of OCR research, and docu-  ment layout and segmentation researchers, there has  been a need for common dat.a sets on which to de-  velop and compare the performance of the algorithms.  Some efforts have been made by researchers in the  OCR cominunity to make the data set,s on which their  algorithms have been tested available to the research  community. Unfortunately, many researchers remain  Su Chen and Robert. M. Haralick  Department of Electrical Engineering  University of Washington  Seattle, WA 98195  unwilling to do the same. Thus it becomes impos-  sible for a researcher to verify published results by  the process of replicating the algorithm or to com-  pare the performance of a competing algorithm that  she/he is developing. Even when data sets are avail-  able, the data sets are often tuned to the algorithm  that the researcher supplying the data sets has de-  veloped. Thus the researcher obtaining the data sets  has no alternative but to tune her/his algorithm to the  data set and this does not promote good research. It is  time for a series of comprehensive standard document  databases to be constructed and made them available  to researchers. Such databases would serve to provide  uniform platforms on which researchers could develop  and compare the recognition accuracy of their OCR  and document understanding algorithms.", "metadata": {"source": "data/2_phillips_1993_cdrom.pdf", "page_number": 1, "chunk": 1, "title": "CD-ROM document database standard"}}, "2": {"idx": 2, "content": "The cost for the creation of a document image  database is relatively high due to the care that must  exist in the creation process and the requirement for  near perfect accuracy. Therefore, it is worthwhile to  coiisider leveraging the cost of producing the ground  truth of the document images by also including in the  database some software to artificially degrade and dis-  tort document images in ways which approximate the  real degradations and distortions that documents un-  dergo as they get copied, recopied, faxed, etc. Such  degradation software can be used to generate con-  trolled degraded document pages for OCR algorithm  development as well as for performance evaluations  and testing of the algorithms.  Our efforts to create such a series of carefully  ground-truthed databases to be issued on CD-ROMs  is described in this paper. The paper presents the de-  sigil of a comprehensive standard document database  for machine-printed documents. An English version of  tlie database is currently under its construction at the  University of Washington and it is to be completed in  July, 1993.  0-8186-4960-7193  $3.00 0 1993 IEEE  .", "metadata": {"source": "data/2_phillips_1993_cdrom.pdf", "page_number": 1, "chunk": 2, "title": "CD-ROM document database standard"}}, "3": {"idx": 3, "content": "2 Requirement for Machine-printed  Document Database  2.1 Languages  In order to be useful for developers of OCR algo-  rithms and document understanding systems. The do-  cument databases to be coiistructed should reflect the  full range of machine-printed documents. The docu-  ment databases should also include documents in each  one of the world\u2019s major language and scripts, such as  Roman, Hebrew, Arabic and Farsi, Kanji, Hangul, De-  vnagiri and Cyrillic. Our efforts are restricted to the  Roman script and English language.  2.2  Document Types  The database series should at least include the fol-  lowing document types:  Articles: journals, proceedings, boolts, etc.;  Business letters and memorandums;  Newspapers/magazines;  Maps: street maps: terrain maps, etc.;  Forms;  Manuscripts;  Engineering CAD/CAM drawings;  Advertisements.  2.3 Document Format and Quality  For each type of document, a variety of documents  of various formats and quality needed to be present  in the database. These documents should be drawn  according to the frequency of their usage to satisfy the  requirements of performance evaluation and drawn so  that sufficient samples are present of each variety t>o  satisfy requirement of algorithm developers.", "metadata": {"source": "data/2_phillips_1993_cdrom.pdf", "page_number": 2, "chunk": 0, "title": "CD-ROM document database standard"}}, "4": {"idx": 4, "content": "2.4 Document Page Images  The database should included hot,li greyscale and  binary document images in TIFF formats, The  database should also include synthesized noise-free d e   cument images as well as various degraded document  images (degraded through both the real process and  simulation). All image files 011 t.he CD-ROM should  be compressed to save space.  2.5  Document Page Description  Page Attributes  The page attributes should include at least the lan-  guage and the script, the font information, the publi-  cation information, the page condition, the page lay-  out, and the character orientation and reading direc-  tion of the document page.  Zone Definition and Attributes  The zone definition should specify the shape, the size  and the location of each zone in a document page.  The zone attributes may include zone type (i.e., text  zone, figure zone, form, map, table, etc.), zone label  (i.e., title, page number, paragraph, author, footnote,  etc.), language, script, character orientation, reading  direction, font information and text alignment format.  2.6  Ground Truth  The database should provide two types of ground  truths. One is the character-based ground truth and  the other is the zone-based ground truth.  The character-based ground truth will include the  name, the size and the position of each individual char-  acter on the page.", "metadata": {"source": "data/2_phillips_1993_cdrom.pdf", "page_number": 2, "chunk": 1, "title": "CD-ROM document database standard"}}, "5": {"idx": 5, "content": "The zone-based ground truth will contain the char-  acter string with the line break for each text line  within a text zone.  2.7 Degradation Models  It is also necessary to develop document degrada-  tion models to provide researchers with a mechanism  for introducing random perturbations on noise-free  ideal images. These degradation models will be devel-  oped based on the kinds of degradation found in real-  life photocopying and FAX transmission, as well as,  coffee stains, ink bleeding, page aging, etc.. The do-  cument degradation software will simulate these real-  life degradations. This degradation software can be  used to generate controlled degraded document pages  for OCR algorithm development, as well as for per-  formance evaluations and testing of the algorithms.  Such synthetically degraded images give unlimited ex-  tension to the real data sets in the database without  having to provide additional ground truth for the gen-  erated documents.  479  .", "metadata": {"source": "data/2_phillips_1993_cdrom.pdf", "page_number": 2, "chunk": 2, "title": "CD-ROM document database standard"}}, "6": {"idx": 6, "content": "2.8  Software  The database should include data compression and  decompression software, OCR performance evaluation  software, degradation software, as well as utility tools  for the database.  3 CD-ROM English Document  Database: A Case Study  In the rest of the paper, we presents the design of  a machine-printed English document database. The  set of document pages will be selected from various  technical journals and reports.  3.1 Contents and Organization Overview  The English document database has two logical  compartments: the software compartment and the  document compartment. The software compartment  contains the data coinpression and decoiupression  software, the OCR performance evaluation software,  the photocopy degradation software, and the FAX  degradation software. The descriptions of this com-  partment is given in Section 3.2.  The document compartment contains all the docu-  ment page images, the page and zone attribute record  files, the zoning information files and ground truth  files. The descriptions of this compartment is given in  Section 3.3.  Since the document database will be packaged on a  CD-ROM. The file names and directory structures will  be in complete compliance with IS0 9660. The general  file name conventions for the document database are  given in Section 3.4.", "metadata": {"source": "data/2_phillips_1993_cdrom.pdf", "page_number": 3, "chunk": 0, "title": "CD-ROM document database standard"}}, "7": {"idx": 7, "content": "3.2  Software Compartment Contents  Compression and Dccornpression Software  All the scanned and simulated documents in bitclnap  (TIFF) format on the CD-ROM are supplied in com-  pressed form. The compression algorithm used is the  CCITT Group IV bi-level image compression stan-  dard. The user can then use the decompression pro-  gram provided in the CD-ROM to uncoinpress the  compressed data files.  OCR Performance Evaluation Software  OCR performance evaluation software will be devel-  oped and provided. Given a list of document. zone  IDS and OCR outputs of the zones, the algorithm  will evaluate the output of OCR algorithm against the  corresponding ground truth residing in the CD-ROM.  A set of contingency tables for characters and mis-  recognized words will be computed and output by the  algorithm. The user\u2019s manual of the software package  is given in [4].  Photocopy Degradation Software  Software is also provided that simulates two selected  document degradation models. One is Baird\u2019s degra-  dation model [2] and the other is currently developed  by researchers at the Intelligent Systems Laboratory.  Given a document file (binary image file) and degra-  dation model parameters, the user can run the photo-  copy degradation program to degrade the document as  desired. The requirement specification of the software  is given in [3].  3.3 Document Compartment Contents  Docuiiient Page Image Files  The document compartment includes a set of docu-  ment image files.", "metadata": {"source": "data/2_phillips_1993_cdrom.pdf", "page_number": 3, "chunk": 1, "title": "CD-ROM document database standard"}}, "8": {"idx": 8, "content": "Each document image corresponds  to one document page. The document images can be  classified into the following categories:  1. Scanned greyscale images from real documents.  2. Scanned binary images from real documents.  3. Synthesized noise-free binary image.  4. Degraded noise-free binary image.  The real document pages will also include a set of  documents which are taken from the set of synthe-  sized noise-free documents and degraded through real  processes - both by successively photocopying or FAX  transmission. The degraded document pages will also  include a set of synthetic degraded documents (for  convenience) that a.re degraded by the same degra-  dation software that will be provided in the database.  The source document of the degraded pages will come  from a set of selected pages from category 2 and 3.  Page Attribute Files  For each document page in the database, there is a  set descriptive attribute which describe the various at-  tributes of the page. Each document page type (jour-  nal, letter/memo, etc.) has its own set of attributes.  For technical journals/reports, the attributes include  480  .", "metadata": {"source": "data/2_phillips_1993_cdrom.pdf", "page_number": 3, "chunk": 2, "title": "CD-ROM document database standard"}}, "9": {"idx": 9, "content": "page condition, page bounding boxes, page contents,  page layout, font information, publication informa-  tion, etc. The journal page attributes definitions are  given in Section 4.  The advantage of defining a set of document page  attribute records per document type over a set of gen-  eral page attributes for all document types is that  it allows one to add another document type to the  database without any change to the database design.  Zone Attribute Files  Each document page will be zoned manually accord-  ing our zoning conventions [5]. Each zone in a page is  associated with a set of zone attributes that describes  the contents of the zone. We define a distinct zone at-  tribute record for each document type (journal, etc.).  The definitions of the zone attribute are given in Sec-  tion 4.5.  Ground Truth Files  The database provides the ground truth for a.11 text  zones on all document pa.ges.  (For mathematical  zones, the form of ground truth may be developed and  provided. For line-art, halftone zones, etc., there will  be no ground truth.) The format of the ground truth  is given as character sequences: the correct character  sequences (with line breaks between sequences) within  the zone.  In addition, each LaTeX generated document page  resides in the database, we provide a ground truth file  that contains character positions of all characters on  the page.", "metadata": {"source": "data/2_phillips_1993_cdrom.pdf", "page_number": 4, "chunk": 0, "title": "CD-ROM document database standard"}}, "10": {"idx": 10, "content": "The format of the ground truth is given in  a character position sequence: the sequence of char-  acters with the position (coordinates) and the size of  each character in the zone. (This format only for syn-  thesized and degraded document pa.ges arising from  the LaTeX generated documents in the database.  The ground truth files will be used by the OCR  evaluation software reside in the database. The soft-  ware evaluates the performance of OCR algorithms.  All special symbols will be represented in LaTeX-  alike syntax. The translation table will be provided.  Bounding Box Information Files  The database also provides bounding boxes informa-  tions for the page header, page footer, live matter and  each zone on a document page. The live matt.er of a  document page is the usable area of the page between  the margins [l]. Each bounding box will be repre-  sented as a rectangular region on a document page.  The definitions of the bounding box information files  are given in Section 4.  3.4 File Name Convention  The general file name conventions for the document  database are as follows: 1) A legal file name will con-  sist of at most 8 characters (26 capital English letters  and 10 digit numbers) followed by a period and a 3-  character extensions. 2) The first character of the file  name must be a capital English letter.  Under our current design, the document files have  additional file name constraints to make them more  identifiable.", "metadata": {"source": "data/2_phillips_1993_cdrom.pdf", "page_number": 4, "chunk": 1, "title": "CD-ROM document database standard"}}, "11": {"idx": 11, "content": "For example, the first four characters of  the file name represent the document page ID. The  last four characters of the file name are used to iden-  tify the category of the file (scanned binary, scanned  greyscale, page/zone attribute record file, page/zone  bounding box record file, ground truth, etc.). The  filename extension indicates the file format (.TIF for  image file, .TXT for ASCII file and .TEX for LaTeX  file).  4 Document Page Record Definitions  This section gives the definitions of all records that  constitute record files within the document compart-  ment of the CD-ROM.  4.1  Page Condition Record  This record includes attributes that describe the  visual conditions (or qualities) of a given document  page. The page condition record has the following  fields:  Record Field Definitions:  0 Document ID:  0 Degradation type: (original)(photocopy)(fax)  0 n-tli copy: (noise-free)(l)(\u2019i?)( )  0 Visible salt/pepper noises: (yes)(no)  0 Visible vertical streaks: (yes)(no)  0 Visible horizontal streaks: (yes)(no)  0 Extraneous symbols on the top: (yes)(no)  0 Extraneous symbols on the bottom: (yes)(no)  0 Extraneous symbols on the left: (yes)(no)  481  .", "metadata": {"source": "data/2_phillips_1993_cdrom.pdf", "page_number": 4, "chunk": 2, "title": "CD-ROM document database standard"}}, "12": {"idx": 12, "content": "0 Extraneous symbols on the right: (yes)(iio)  0 Page skewed on the left: (yes)(no)  0 Page skewed on the right: (yes)(no)  0 Page smeared on the left: (yes)(no)  0 Page smeared on the right: (yes)(no)  0 Visible page rotation: (yes)(no)  0 Page rotation angle (in degree):  0 Page rotation angle standard deviation:  4.2 Page Attribute Record  The following record fields define the set of descrip-  tive attributes which describe the various attributes of  a journal document page.  Record Field Definitions:  0 Document ID:  0 Document language: (English)  The value for this field is English for this  database. The field is provided for upward com-  patibility with future databases that we or others  might produce in languages other than English,  for ex. Kanji, Arabic etc.  0 Document script: (Roman)  0 Document type: (journal) (letter) (memo) (news)  0 Publication Information: This attributes contains  information about the name, the volume number,  the issue number and the publishing date of the  publication. It also has the corresponding page  number of the document page from the publica-  tion.  0 Multiple pages from the same article: (yes)(no)  A flag indicating whether multiple document  pages from the same article are included in the  database. The document pages within the same  article can be retrieved by reference to the pub-  lication name, volume and issue number of the  page.", "metadata": {"source": "data/2_phillips_1993_cdrom.pdf", "page_number": 5, "chunk": 0, "title": "CD-ROM document database standard"}}, "13": {"idx": 13, "content": "0 Text zone present: (yes)(no)  0 Special symbol present in text zone: (yes)(no)  The special symbols are defined as the syiiibols  other than the standard ASCII symbols.  0 Displayed Math zone present: (yes)(no)  0 Table zone present: (yes)(no)  0 Half-tone zone present: (yes)(no)  0 Drawing zone present: (yes)(no)  0 Page header present: (yes)(no)  0 Page footer present: (yes)(no)  0 Max number of text columns: The number of  equal-width text columns within of the live mat-  ter area of the document page.  0 Page Column layout: (regular) (combined-  columns)  0 Character orientation: (up-right) (rotated-right)  (rotated-left).  This field gives the orientation of characters  within the text line when the page is oriented to  up-right position.  0 Text reading direction: (left-right) (right-left)  (top-down) (bottom-up)  This field gives the text reading direction within  a text line when a page is oriented to up-right  position. For example, for a landscaped oriented  page, the page needed to be rotated to in up-right  position.  0 Dominant font type: (Serif)(Sans-Serif)  0 Dominant character spacing: (proportional)  (fixed)  0 Dominant font size (pts): (<< 9) (9-12) (13-18)  (19-24) (25-36) (>> 36)  0 Dominant font style: (plain) (bold) (italic)  (underline) (other)  Any combination of the font styles are allowed.  The word \u2018dominant\u2019 is defined as the most fre-  quently used font (type, style, size) in a given  page.", "metadata": {"source": "data/2_phillips_1993_cdrom.pdf", "page_number": 5, "chunk": 1, "title": "CD-ROM document database standard"}}, "14": {"idx": 14, "content": "4.3 Page Bounding Box Record  The page bounding box record defines the page  header area, page footer area and live matter area.  A page bounding box record has the following fields:  Record Field Definitions:  0 Document ID:  .", "metadata": {"source": "data/2_phillips_1993_cdrom.pdf", "page_number": 5, "chunk": 2, "title": "CD-ROM document database standard"}}, "15": {"idx": 15, "content": "0 Header area upper-left corner coords:  0 Header area lower-right corner coords:  0 Live matter area upper-left corner coords:  0 Live matter area lower-right corner coords:  0 Footer area upper-left corner coords:  Footer area lower-right corner coords:  4.4 Zone Bounding Box Record  The zone bounding box record defines each zone on  a document page.  Record Field Definitions:  0 Document ID:  0 Zone ID:  0 Zone upper-left corner coords:  Zone lower-right corner coords:  4.5  Zone Attribute Record  This attribute record describes a set of attributes  that are common to zones from a journal/report d e   cument page. The record has the following fields:  Record Field Definitions:  Document ID:  Zone ID:  Zone content:  The zone content can take on either of the fol-  lowing values: text, text with special symbols,  displayed math, table, half-tone, drawing, form,  ruling, bounding box, logo, map, advertisement,  announcement, handwriting and others.  Text zone label:  The zone label can be one of the following values:  text body, list item, drop cap, caption, abstract  body, abstract heading, section heading, synopsis,  highlight, pseudo-codes, reference heading, refer-  ence list item, footnote, author biography, page  header, page footer, page number, article title,  author, affiliation, diploma information, society  membership information, article submission in-  formation, abstract heading, abstract body, foot-  note heading, keyword heading, keyword body  and others.", "metadata": {"source": "data/2_phillips_1993_cdrom.pdf", "page_number": 6, "chunk": 0, "title": "CD-ROM document database standard"}}, "16": {"idx": 16, "content": "0 Text alignment within the zone:  This attribute defines the text alignment within  the zone. The types of text alignment are: left  aligned, center aligned, right aligned, justified,  justified hanging, left hanging.  0 Font information: This attributes defines the  dominant font type, character spacing, font size  and font style within the zone.  0 Character orientation:  0 Text reading direction:  0 Zone\u2019s column number:  This attribute describes the zone\u2019s column loca-  tion. A zone may be in the header area, footer  area and column number 1 of 1, 1 of 2 and etc.  0 Next zone ID within the same thread: (-)  (nil)  The zones of each document page can be grouped  into several logical units.  Within each logical  unit, the reading order is sequential. We call such  a logical unit as a semantic thread. This attribute  is used to indicate the reading order among the  zones that constitute a semantic thread. \u201cnil\u201d is  used to indicate the end of the semantic thread.  References  [l] Xerox Publishing Standards: A Manual of Style  and Design, A Xerox Press Book, Watson-Guptill  Publications/New York.  [2] H. S. Baird \u201cDocument Image Defect Models\u201d,  Structured Document Image Analysis, Springer  Verlag, N. Y., 1992, p546-556.  [3] T. Kanungo, Document Degradation Model Re-  quirement Specification, ISL Report, 1993, Univer-  sity of Washington.  [4] Su Chen, OCR Performance Evaluation Software  User\u2019s Manual, ISL Report, 1993, University of  Washington.  [5] I. T.", "metadata": {"source": "data/2_phillips_1993_cdrom.pdf", "page_number": 6, "chunk": 1, "title": "CD-ROM document database standard"}}, "17": {"idx": 17, "content": "Phillips and S. Chen, English Document  Database Zone Label Definitions and Examples,  ISL Report, 1993, University of Washington.  [GI I.T. Phillips, S. Chen, J. Ha and R.M. Haralick,  \u201cEnglish Document Database Design and Imple-  mentation Methodology\u201d, Proc. of the second An-  nual Symposium on Document Analysis and In for-  mution Retrieval, April 26-28, pp. 65-104, 1993.  483  .", "metadata": {"source": "data/2_phillips_1993_cdrom.pdf", "page_number": 6, "chunk": 2, "title": "CD-ROM document database standard"}}, "18": {"idx": 18, "content": "Evaluation of Header Metadata Extraction Approaches and  Tools for Scientific PDF Documents  Mario Lipinski, Kevin Yao, Corinna Breitinger, Joeran Beel, Bela Gipp  University of California, Berkeley, CA, USA  mario@lipinski.tk, {kzyao, breitinger, jbeel, gipp}@berkeley.edu ABSTRACT  This paper evaluates the performance of tools for the extraction of  metadata from scientific articles. Accurate metadata extraction is an  important task for automating the management of digital libraries.  This comparative study is a guide for developers looking to  integrate the most suitable and effective metadata extraction tool  into their software. We shed light on the strengths and weaknesses  of seven tools in common use. In our evaluation using papers from  the arXiv collection, GROBID delivered the best results, followed  by  Mendeley  Desktop.  SciPlore  Xtract,  PDFMeat,  and  SVMHeaderParse also delivered good results depending on the  metadata type to be extracted.  Categories and Subject Descriptors  H.3.4 [Information Storage and Retrieval]: Systems and Software  \u2013 performance evaluation (efficiency and effectiveness).  General Terms  Algorithms, Measurement, Performance, Experimentation.  Keywords  Information Retrieval; Metadata Extraction; Evaluation; PDF  1. INTRODUCTION  Obtaining structured metadata from documents, including title,  authors, and publication date, is important to support retrieval tasks,  e.g., in digital libraries [2].", "metadata": {"source": "data/13_lipinski_2018_evaluationof.pdf", "page_number": 1, "chunk": 0, "title": "Evaluation of Header Metadata Extraction Approaches and Tools for Scientific PDF Documents"}}, "19": {"idx": 19, "content": "Various tools exist to automatically  extract this information from PDF documents. However, the  extraction is error prone given that no standards specify how  metadata should be structured or formatted. Different style guides  imposed by various publishers and venues, as well as the scope of  metadata information provided by individual authors, increase the  difficulty of automatic metadata extraction. In this paper we  evaluate tools for metadata extraction from scientific articles.  A recent publication compared the metadata extraction capabilities  of Mendeley and ParsCit, concluding that Mendeley\u2019s two-staged  SVM solved the problem of metadata extraction for crowdsourced  bibliographic metadata management [5]. Several approaches for  extracting metadata have been proposed and independently  examined. Since studies used different methods and data sources for  their evaluation, a direct comparison is not possible. Currently, no  comprehensive evaluation of tools for metadata extraction exists.  Fundamental methods used for metadata extraction are stylistic  analysis, machine learning, and the use of knowledge bases.  Metadata extraction tools using stylistic analysis extract titles using  heuristics, e.g., font sizes and position information of examined  elements. Machine learning techniques for metadata extraction use  support vector machines (SVM), hidden Markov models (HMM),  or conditional random fields (CRF).", "metadata": {"source": "data/13_lipinski_2018_evaluationof.pdf", "page_number": 1, "chunk": 1, "title": "Evaluation of Header Metadata Extraction Approaches and Tools for Scientific PDF Documents"}}, "20": {"idx": 20, "content": "These approaches rely on  previous training and natural language processing. Knowledge base  approaches make use of databases, such as Google Scholar, or  pronoun repositories, e.g., lists of common names, to act as a crossreference to extracted entities. Software systems for metadata  extraction combine these methods.  The JISC ConnectedWorks project published an overview of  available software for processing PDF documents [6]. For our  study, we focus on tools that are freely available and do not require  user interaction so that the tools can be integrated into custom  projects. Although Mendeley Desktop cannot be included in custom  software, we include it in the evaluation as a widely used software  with metadata extraction capabilities. We did not include Zotero,  since its 25-document limit makes it unsuitable for bulk processing.  Also the recently developed Docear\u2019s PDF Inspector [4] was not  included since it was not yet available at the time of evaluation.  Table 1 gives an overview of tools for extracting header information  from PDF documents.  Table 1.", "metadata": {"source": "data/13_lipinski_2018_evaluationof.pdf", "page_number": 1, "chunk": 2, "title": "Evaluation of Header Metadata Extraction Approaches and Tools for Scientific PDF Documents"}}, "21": {"idx": 21, "content": "Tools for Metadata Extraction from PDF Documents  Name of Tool  Approach used   Link  Docear\u2019s PDF Inspector*  Style information analysis  http://docear.org   GROBID  CRF  https://github.com/kermitt2/grobid  Mendeley Desktop  SVM, web-based look-up  http://www.mendeley.com/  ParsCit  CRF  http://aye.comp.nus.edu.sg/parsCit/  PDFMeat  Queries Google Scholar, pdftotext  http://code.google.com/p/pdfmeat/  PDFSSA4MET  Structure/Syntax analysis of XML  http://code.google.com/p/pdfssa4met/  SciPlore Xtract  Style information analysis of XML  http://sciplore.org/  SVMHeaderParse  SVM  http://citeseerx.sourceforge.net/  Zotero*  Queries Google Scholar  http://zotero.org  * not evaluated  2. METHODOLOGY  To meet the suitability requirement, extraction tools must fulfill  three requirements. First, the tools had to provide an interface that  allowed the integration into custom projects. They either had to  provide a library for integration into other programs, or a standalone program that either accepted plain text or PDF as input.  Second, they were not allowed to require user interaction to allow  bulk processing. Third, the examined tools had to output machinereadable data, for example in XML format.    385 Konstanzer Online-Publikations-System (KOPS)  URL: http://nbn-resolving.de/urn:nbn:de:bsz:352-0-285622 Erschienen in: Proceedings of the 13th ACM/IEEE-CS joint conference on Digital libraries / J. Stephen Downie (Hrsg.). - New York :  ACM, 2013. - S. 385-386.", "metadata": {"source": "data/13_lipinski_2018_evaluationof.pdf", "page_number": 1, "chunk": 3, "title": "Evaluation of Header Metadata Extraction Approaches and Tools for Scientific PDF Documents"}}, "22": {"idx": 22, "content": "- ISBN 978-1-4503-2077-1 .", "metadata": {"source": "data/13_lipinski_2018_evaluationof.pdf", "page_number": 1, "chunk": 4, "title": "Evaluation of Header Metadata Extraction Approaches and Tools for Scientific PDF Documents"}}, "23": {"idx": 23, "content": "Table 2. Results (A100: First evaluation setup with 100 articles, B100: Second evaluation setup with 100 articles, B1153: Second evaluation  setup with 1,153 articles)    Title  Authors  Authors\u2018 last  names  Abstract  Year  A100  B100  B1153  A100  B100  B1153  B100  B1153  A100  B100  B1153  B100  B1153  GROBID  N/A  0.92  0.92  N/A  0.83  0.83  0.90  0.91  N/A  0.75  0.74  0.64  0.69  Mendeley Desktop  N/A  0.84  0.82  N/A  0.72  0.70  0.78  0.77  N/A  N/A  N/A  0.23  0.26  ParsCit  0.59  0.52  0.54  0.47  0.29  0.31  0.36  0.37  0.49  0.31  0.26  0.06  0.07  PDFSSA4MET  0.13  0.21  0.18  0.05  0.02  0.01  0.20  0.18  N/A  N/A  N/A  N/A  N/A  PDFMeat  0.60  N/A  N/A  0.6  N/A  N/A  N/A  N/A  0.14  N/A  N/A  N/A  N/A  SciPlore Xtract  0.76  0.81  0.78  N/A  N/A  N/A  N/A  N/A  N/A  N/A  N/A  N/A  N/A  SVMHeaderParse  0.50  0.57  0.61  0.64  0.70  0.73  0.74  0.76  0.37  0.64  0.64  0.21  0.20    Since the tools are written in different languages and have different  output formats, we created a Java framework to provide a uniform  interface for the variety of tools. The framework requires a PDF file  as input and converts it, if required by the tool, to plain text using  pdftotext. The output is wrapped into a unified Java data structure  that stores the extracted fields.  To evaluate the tools, we compiled a test collection from arXiv.org,  a scientific publication archive, which contains articles from various  disciplines with various document formatting.", "metadata": {"source": "data/13_lipinski_2018_evaluationof.pdf", "page_number": 2, "chunk": 0, "title": "Evaluation of Header Metadata Extraction Approaches and Tools for Scientific PDF Documents"}}, "24": {"idx": 24, "content": "Given the diverse  document styles in the arXiv collection, it provides a good data  source to test the performance of metadata extraction tools on  articles in Physics, Mathematics, Computer Science, Quantitative  Biology, Quantitative Finance and Statistics. In examining  publications only from these fields, the results might not apply to  other fields. By using arXiv\u2019s API, we obtained 1,153 random PDF  articles including their metadata, dated from 2006 to 2010.  We performed three evaluations with two independent test setups.  For the first setup, we randomly chose 100 articles from the test  collection and the metadata extracted from the PDF documents was  manually compared against the metadata from arXiv. We created a  scoring scheme to assess the performance of the individual tools on  the following metadata types: title, authors, and abstract. Each field  was scored individually. For every field a score of 1 was given if the  extracted metadata matched the reference data. A score of 0 was  assigned if the field was extracted incorrectly. For title, authors, and  abstracts a reduced score of 0.5 was allotted if the data was  retrieved, but some characters, such as accents or ligatures produced  problems. If only a fraction of the correct data was detected, a score  of 0.25 was allotted.  For the second test setup, we performed two evaluations. First, we  used the 100 documents from the first setup and second we used all  1,153 documents.", "metadata": {"source": "data/13_lipinski_2018_evaluationof.pdf", "page_number": 2, "chunk": 1, "title": "Evaluation of Header Metadata Extraction Approaches and Tools for Scientific PDF Documents"}}, "25": {"idx": 25, "content": "In this setup, we developed a program to  automatically determine the scores for extraction of title, author full  names, author last names, abstract, and publication year. The  program used the Levenshtein distance and normalized it using the  length of the reference value. The resulting score approximately fits  the percent match. The scores for all documents from the test  collection were averaged.  3. RESULTS  Table 2 shows the results of the evaluations. In the following, we  point out noteworthy results and give their accuracy scores.  GROBID performs best; 0.92 for titles, 0.83 for authors, 0.91 for  authors\u2019 last names, 0.74 for abstracts, and 0.69 for publication date.  We believe that by working directly with PDF files without losing  information in preprocessing, and by accurately engineered models,  GROBID has an advantage over other methods. Mendeley Desktop  ranks second for all extracted metadata types (0.82 for titles, 0.70  for author first names, and 0.77 for author last names). Mendeley  leverages its extensive online database to enrich the extracted  metadata. While the authors in [5] claim that Mendeley\u2019s two-stage  SVM performs best, our results show that GROBID\u2019s CRF  implementation can deliver better metadata extraction without  consulting external resources.  SciPlore Xtract shows a good accuracy of 0.78 in extraction of the  title.", "metadata": {"source": "data/13_lipinski_2018_evaluationof.pdf", "page_number": 2, "chunk": 2, "title": "Evaluation of Header Metadata Extraction Approaches and Tools for Scientific PDF Documents"}}, "26": {"idx": 26, "content": "By taking into account the article\u2019s style information (font  sizes and layout information), SciPlore Xtract can gain an advantage  over other tools that ignore this information [3]. Nevertheless, the  low score of 0.18 for PDFSSA4MET demonstrates that relying  solely on font size is insufficient. Looking at the data revealed that  PDFSSA4MET often extracted arXiv\u2019s document ID banner in the  left margin as the title, so the tool may be at a disadvantage for the  chosen test collection.  SVMHeaderParse delivered good results for extracting author  names (0.73) and abstracts (0.64). The results for SVMHeaderParse  and ParsCit for extracting titles and abstracts slightly differed  between our two test setups. We believe that different versions of  the tool for transforming PDFs into plain text can affect the  performance of these tools. PDFMeat delivered relatively good  results of 0.60 on title and author extraction. The relative good  quality of the extracted data may result from incorporating results  from Google Scholar [1].  The evaluation framework, including test collection, the ground  truth and the test-software, is available from the authors by request.  4. ACKNOWLEDGEMENTS  We greatly acknowledge support by the VIGRE Program to work  on this project at the Department of Statistics at the University of  California, Berkeley.  5. REFERENCES  [1]  Aumueller, D. 2009. Retrieving metadata for your local  scholarly papers.", "metadata": {"source": "data/13_lipinski_2018_evaluationof.pdf", "page_number": 2, "chunk": 3, "title": "Evaluation of Header Metadata Extraction Approaches and Tools for Scientific PDF Documents"}}, "27": {"idx": 27, "content": "[2]  Beel, J., Gipp, B., Langer, S., Genzmehr, M., Wilde, E.,  N\u00fcrnberger, A. and Pitman, J. 2011. Introducing Mr. DLib, a  Machine-readable Digital Library. JCDL\u201811.  [3]  Beel, J., Gipp, B., Shaker, A. and Friedrich, N. 2010. SciPlore  Xtract: Extracting Titles from Scientific PDF Documents by  Analyzing Style Information (Font Size). ECDL\u201910.  [4]  Beel, J., Langer, S., Genzmehr, M. and M\u00fcller, C. 2013.  Docears PDF Inspector: Title Extraction from PDF files.  JCDL'13.  [5]  Granitzer, M., Hristakeva, M., Knight, R. and Jack, K. 2012.  A Comparison of Metadata Extraction Techniques for  Crowdsourced Bibliographic Metadata Management. SAC'12.  [6]  JISC ConnectedWorks Project 2010. Research on existing  PDF processors. University of Cambridge.    386 .", "metadata": {"source": "data/13_lipinski_2018_evaluationof.pdf", "page_number": 2, "chunk": 4, "title": "Evaluation of Header Metadata Extraction Approaches and Tools for Scientific PDF Documents"}}, "28": {"idx": 28, "content": "See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/327344872 Fault-Prone Java Method Analysis Focusing on Pair of Local Variables with Confusing Names Conference Paper \u00b7 August 2018 DOI: 10.1109/SEAA.2018.00033 CITATIONS 8 READS 93 5 authors, including: Hirohisa Aman Ehime University 91 PUBLICATIONS\u00a0\u00a0\u00a0339 CITATIONS\u00a0\u00a0\u00a0 SEE PROFILE Sousuke Amasaki Okayama Prefectural University 101 PUBLICATIONS\u00a0\u00a0\u00a0710 CITATIONS\u00a0\u00a0\u00a0 SEE PROFILE Tomoyuki Yokogawa Okayama Prefectural University 84 PUBLICATIONS\u00a0\u00a0\u00a0326 CITATIONS\u00a0\u00a0\u00a0 SEE PROFILE All content following this page was uploaded by Hirohisa Aman on 31 August 2018. The user has requested enhancement of the downloaded file. .", "metadata": {"source": "data/16_ktashima_2018_faultprone.pdf", "page_number": 1, "chunk": 0, "title": ""}}, "29": {"idx": 29, "content": "Fault-Prone Java Method Analysis Focusing on Pair of Local Variables with Confusing Names Keiichiro Tashima\u2217, Hirohisa Aman\u2020, Sousuke Amasaki\u2021, Tomoyuki Yokogawa\u2021 and Minoru Kawahara\u2020 \u2217Department of Computer Science, Faculty of Engineering, Ehime University Matsuyama, Ehime, 790\u20138577 Japan \u2020Center for Information Technology, Ehime University Matsuyama, Ehime, 790\u20138577 Japan \u2021Faculty of Computer Science and Systems Engineering, Okayama Prefectural University Soja, Okayama, 719\u20131197 Japan Abstract\u2014Giving a name to a local variable is usually a programmer\u2019s discretion. Since it depends on the programmer\u2019s preference and experience, there is a lot of individual variation which may cause a variability in the code quality such as the readability. While there have been studies on the naming of local variables in the past, a relationship of names among local variables within a method (function) has not been well-discussed. This paper focuses on a pair of local variables with similar, confusing names, e.g., \u201clineIndex\u201d vs. \u201clineIndent.\u201d Since such local variables are confusable with each other, the presence of such a confusing pair may be related to the fault-proneness of the method.", "metadata": {"source": "data/16_ktashima_2018_faultprone.pdf", "page_number": 2, "chunk": 0, "title": ""}}, "30": {"idx": 30, "content": "An empirical analysis for \ufb01ve major open source Java projects is conducted, and the following results are reported: (1) a method having a confusing variable pair is about 1.1 \u2013 2.6 times more fault-prone than a method having only dissimilar (nonconfusing) pairs; (2) the proposed metric of how confusing the local variables are is equivalent to or better than the conventional cyclomatic complexity in predicting fault-prone methods. I. INTRODUCTION A proper quality management of source code is essential to a successful software development and maintenance. Since a coding activity is an intellectual task by a programmer, i.e., a human being, an individual variation in the code quality would be inevitable. One of the most diverse artifacts in source code is a local variable in a method (function). Since the requirements speci\ufb01cation and the design documents usually do not specify the declaration and use of local variables, programmers can freely decide them during their coding activity. Different programmers may give different names to a local variable for the same purpose. Such an individual difference causes a variation of the code quality such as the readability. In general, names (identi\ufb01ers) are important information source to comprehend the code [1].", "metadata": {"source": "data/16_ktashima_2018_faultprone.pdf", "page_number": 2, "chunk": 1, "title": ""}}, "31": {"idx": 31, "content": "In other words, the quality of name plays a signi\ufb01cant role to lead the code readers to a proper understanding of the code; meaningless or improper names degrade the source code [2], and may cause a fault creation or an overlooking of fault. There have been studies focusing on variable names (or identi\ufb01ers in the more general sense) in the past [3], [4], [5], [6], [7], [8], [9]. For example, they analyzed the length and the composition of local variable\u2019s name, and discussed the relationships with the code comprehension or the code quality. While these previous work presented actionable results, their main focuses were on the \u201cindividual names\u201d but not the \u201crelationships between names.\u201d As a novel point of view, we focus on the similarity between local variables\u2019 names. If there is a pair of local variables with highly-similar names, they would be confusable with each other: for example, \u201clineIndex\u201d vs. \u201clineIndent.\u201d The presence of such a confusing pair may have an impact on the code quality. In this paper, we propose to quantify the degree to which local variables\u2019 names are confusing, and perform an empirical study to examine its impact on the fault-proneness of methods. The remainder of this paper is organized as follows. Section II brie\ufb02y describes the related work focusing on local variables\u2019 names, and presents our research motivation.", "metadata": {"source": "data/16_ktashima_2018_faultprone.pdf", "page_number": 2, "chunk": 2, "title": ""}}, "32": {"idx": 32, "content": "Section III introduces the key notion of \u201cconfusing names\u201d of local variables and de\ufb01nes a metric to measure the degree to which two names are confusing. Section IV reports and discusses the results of our empirical study. Finally, Section V presents the conclusion of this paper and our future work. II. RELATED WORK Lawrie et al. [3] focused on the composition of variable\u2019s name, and conducted a comparative survey on the program comprehension with using three types of names: a fullyspelled word, an abbreviated word and a single character, e.g., \u201cindex,\u201d \u201cidx\u201d and \u201ci,\u201d respectively. They showed that the understandability of a name decreases in the order of a fully-spelled one, an abbreviated one and a single-character one, but there is no signi\ufb01cant difference between the fullyspelled name and its abbreviated one in the comprehension by programmers. A similar trend is supported by the large-scale experiment which was conducted and reported by Scanniello et al. [4]. Kawamoto and Mizuno [5] focused on the length of identi\ufb01er and reported that a class having a long identi\ufb01er tends to be fault-prone. Aman et al. [7] also reported that a method having a local variable with a long name is changeprone and cannot survive unscathed. Although a long name of a variable can describe its role more accurately, a long name may decay the readability of code [6]. Kernighan and Pike [10] said that it is overdone to use a long and descriptive name for a variable if its scope is narrow. .", "metadata": {"source": "data/16_ktashima_2018_faultprone.pdf", "page_number": 2, "chunk": 3, "title": ""}}, "33": {"idx": 33, "content": "Binkley et al. [8] compared the impact of difference in the naming style\u2014the camel case and the snake case, e.g., \u201cmaxValue\u201d vs. \u201cmax value\u201d\u2014on the program comprehension. As the result, they reported that the camel case is better for programming beginners in terms of the comprehension while there is no signi\ufb01cant difference for experts. Bulter et al. [9] proposed 12 naming rules and showed that programs whose identi\ufb01ers are against their rules are fault-prone. As the above studies reported, a name of variable (identi\ufb01er) is a noteworthy feature in the program comprehension and quality management. To the best of our knowledge, however, the previous work focused on the \u201cindividual\u201d name; a \u201crelationship between names\u201d has not been well-discussed in the past. When a method has two or more local variables, the similarity among them would be yet another interesting point of view. This is our research motivation in this paper. III. CONFUSING NAMES OF LOCAL VARIABLES As we mentioned above, we focus on a relationship between local variables\u2019 names in this paper. Notice that our analysis is limited to local variables although there are also other types of identi\ufb01ers including class names, class \ufb01eld (class or instance variable) names and method names. Since classes, \ufb01elds and methods might be speci\ufb01ed in the design phase, programmers would not be able to decide their names freely.", "metadata": {"source": "data/16_ktashima_2018_faultprone.pdf", "page_number": 3, "chunk": 0, "title": ""}}, "34": {"idx": 34, "content": "On the other hand, names of local variables are almost always given by programmers at their own discretion during the programming phase. In order to support the programming activity from the perspective of naming, we decided to analyze local variables in this paper; a further analysis involving other types of identi\ufb01ers is our future work. A. Pair of Highly-Similar (Confusing) Names In general, a local variable should have an proper name to express its role. However, when two or more local variables are declared in a method, we may need to take care of their similarities as well. If there is a pair of highly-similar names, one variable is confusable with another variable. Such a confusing pair might cause a misuse of variable which is a fault creation, or an overlooking of fault. Figure 1 presents an example of code fragment. In the \ufb01gure, the program logic is simple and each local variable has a well-described name. However, it does not look a \u201csimple\u201d code; there is a pair of local variables with long and similar (confusing) names\u2014\u201cdistanceBetweenAbscissae\u201d and \u201cdistanceBetweenOrdinates.\u201d We might make a mistake when we copy and paste some statements or when we use ..... distanceBetweenAbscissae = \ufb01rstAbscissa \u2013 secondAbscissa; distanceBetweenOrdinates = \ufb01rstOrdinate \u2013 secondOrdinate; if ( distanceBetweenAbscissae \u2217distanceBetweenAbscissae > distanceBetweenOrdinates \u2217distanceBetweenOrdinates ){ ..... Fig. 1.", "metadata": {"source": "data/16_ktashima_2018_faultprone.pdf", "page_number": 3, "chunk": 1, "title": ""}}, "35": {"idx": 35, "content": "A code fragment having a pair of local variables with confusing names. the code completion function on an integrated development environment such as Eclipse. Now we de\ufb01ne highly-similar names as \u201cconfusing\u201d names. Hereinafter, we will call a pair of local variables with confusing names as a pair of \u201cconfusing local variables\u201d or simply \u201cconfusing variables.\u201d B. Levenshtein Distance To evaluate the degree to which two names are confusing, we will leverage the Levenshtein distance between them in this paper. Basically, the notion of confusing names has two different points of view: the string similarity and the semantic similarity. For example, \u201clevelOfStrength\u201d and \u201clevelOfStrangeness\u201d are similar strings, but their meanings are not close. We also tried evaluating the semantic similarity by using the Word2Vec [11]. However, we faced a dif\ufb01culty to appropriately handle abbreviated words in the semantic analysis. For example, we have to appropriately expand \u201cidx\u201d to \u201cindex\u201d so that the Word2Vec can process it. Hence, we decided to start tackling the string similarity in this paper; we would like to perform a further analysis focusing on the semantic similarity as our important future work.", "metadata": {"source": "data/16_ktashima_2018_faultprone.pdf", "page_number": 3, "chunk": 2, "title": ""}}, "36": {"idx": 36, "content": "The Levenshtein distance between two strings is the minimum number of the character editing operations which are required to change one string to another string, where a character editing operation is one of the following operations: (1) a single character deletion, (2) a single character insertion, and (3) a single character substitution. For example, the Levenshtein distance between \u201c\ufb01rst\u201d and \u201clast\u201d is 3 because \u201c\ufb01rst\u201d f\u2192l \u2212\u2212\u2192\u201clirst\u201d i\u2192a \u2212\u2212\u2192\u201clarst\u201d delete r \u2212\u2212\u2212\u2212\u2212\u2192\u201clast.\u201d C. Normalized Levenshtein Distance Although the Levenshtein distance can be a measure of string similarity, it has an unsuitable feature to evaluate the degree to which two names are confusing. Let us take the following two pairs for example: 1) \u201cget\u201d and \u201cset\u201d , 2) \u201cdXAxisTitleThickness\u201d and \u201cdYAxisTitleThickness\u201d . Both pairs have the same Levenshtein distance, 1. However, pair 2) seems to be more confusing than pair 1). Since such an abnormality is from the difference of their string lengths, we propose the following normalized Levenshtein distance (NLD) between two strings (local variables\u2019 names) s1 and s2 as our measure of how confusing these two variables are: NLD(s1, s2) = LD(s1, s2) max{ \u03bb(s1), \u03bb(s2) } , (1) where LD(s1, s2) is the (original) Levenshtein distance, and \u03bb(\u00b7) signi\ufb01es the length of the corresponding string. We get the following evaluations: NLD(\u201cget\u201d, \u201cset\u201d) = 1/3 and NLD(\u201cdXAxisTitleThickness\u201d, \u201cdYAxisTitleThickness\u201d) = 1/20.", "metadata": {"source": "data/16_ktashima_2018_faultprone.pdf", "page_number": 3, "chunk": 3, "title": ""}}, "37": {"idx": 37, "content": "That is to say, the latter pair is evaluated as more similar than the former pair. .", "metadata": {"source": "data/16_ktashima_2018_faultprone.pdf", "page_number": 3, "chunk": 4, "title": ""}}, "38": {"idx": 38, "content": "IV. EMPIRICAL STUDY To examine the impact of confusing local variables on the fault-proneness of methods, we conduct an empirical study. A. Dataset We analyze \ufb01ve major open source projects shown in Table I. In the table, \u201c# \ufb01les\u201d signi\ufb01es the number of source \ufb01les surveyed, excluding test programs and documents1. The main reasons why we targeted these projects are as follows. 1) Their source \ufb01les are written in Java; 2) Their source \ufb01les are maintained by using the Git; 3) Their fault data are available. The requirements 1) and 2) are from our data collection tools. The requirement 3) is essential to the fault-prone method analysis. In this study, we leverage the fault data from the teraPROMISE repository2. B. Procedure We conduct our data collection and analysis in the following procedure. 1) Extract methods and their local variables from Java source \ufb01les. By analyzing Java source \ufb01les using Eclipse JDT, we extract methods and their local variables from source \ufb01les. In this study, we consider method parameters to be local variables as well. Notice that we exclude the following methods since we cannot compute NLD for variable pairs: (a) a method having no local variable, (b) a method having only one local variable, and (c) an abstract method. 2) Get NLD for each method. In each method, compute NLD for each pair of local variables.", "metadata": {"source": "data/16_ktashima_2018_faultprone.pdf", "page_number": 4, "chunk": 0, "title": ""}}, "39": {"idx": 39, "content": "Since our main focus is on whether the presence of confusing variable pair is related to the fault-proneness of the method or not, we adopt the minimum NLD (the highest similarity) as the measure of the method when there are two or more variable pairs. 3) Divide the method set into subsets by NLD, and compare the fault-proneness among subsets. To compare methods according to their NLD, we divide the set of methods into four subsets G1, G2, G3 and G4, based on the quartile of NLD distribution, where \u2022 G1: NLD \u226425 percentile; \u2022 G2: 25 percentile < NLD \u2264median; \u2022 G3: median < NLD \u226475 percentile; \u2022 G4: 75 percentile < NLD. Then, we compute the rate of faulty methods in each subset (fault rate: FR) and compare FR values among Gi (for i = 1, 2, 3, 4). In this paper, we use FR as our measure of the fault-proneness of method. Notice that the fault data of the surveyed projects from the tera-PROMISE repository is at \ufb01le-level. To detect faulty methods, we checked which methods are changed through fault \ufb01xes. 1We omitted source \ufb01les whose paths contain \u201ctest\u201d or \u201cdocumentation.\u201d 2http://openscience.us/repo/ TABLE I NUMBERS OF SURVEYED FILES, METHODS AND LOCAL VARIABLES. project # \ufb01les # methods # local variables Apache Tomcat 1, 726 7, 051 32, 139 BIRT 8, 232 34, 027 156, 786 Eclipse JDT UI 10, 452 18, 571 87, 568 Eclipse Platform UI 4, 272 15, 937 65, 160 SWT 1, 731 11, 157 64, 770 total 26, 413 86, 743 406, 423 C.", "metadata": {"source": "data/16_ktashima_2018_faultprone.pdf", "page_number": 4, "chunk": 1, "title": ""}}, "40": {"idx": 40, "content": "Results Figure 2 and Table II present the FRs in G1 \u2013 G4 for each project. From Fig. 2, we see monotonically decreasing trends of FR from G1 to G4 in four out of \ufb01ve projects except for SWT: FR becomes lower as NLD gets larger (from G1 to G4). A method category with larger NLD is the set of methods having only pairs of local variables whose names are less similar each other. Thus, a method having only non-confusing local variable pairs is less fault-prone. In other words, a method having a pair of more confusing local variables is more likely to be faulty. FR values change from G1 (with the most confusing pairs) to G4 (with the least confusing pairs) as follows. \u2022 Apache Tomcat: FR in G1 is about 2.6 times higher than that in G4; \u2022 BIRT: about 1.4 times higher; \u2022 Eclipse JDT UI: about 1.6 times higher; \u2022 Eclipse Platform UI: about 1.5 times higher. SWT has a different trend: FR in G2 is higher than that in G1. But it shows a monotonic decrease from G2 to G4 as well, and moreover, FR in G1 is about 1.1 times higher than that in G4. Thus, it still seems to be that a method having a confusing local variable pair tends to be more fault-prone. D. Discussions In order to examine the relationship between the presence of confusing local variables in a Java method and the faultproneness of the methods, we divided the method set into four subsets G1 \u2013 G4 by NLD, and compared their FRs.", "metadata": {"source": "data/16_ktashima_2018_faultprone.pdf", "page_number": 4, "chunk": 2, "title": ""}}, "41": {"idx": 41, "content": "G1 is the set of methods having the most confusing local variable pairs, 0.0 0.1 0.2 0.3 0.4 G1 G2 G3 G4 method category fault rate project Apache Tomcat BIRT Eclipse JDT UI Eclipse Platform UI SWT Fig. 2. Fault rates (FRs) in method categories. .", "metadata": {"source": "data/16_ktashima_2018_faultprone.pdf", "page_number": 4, "chunk": 3, "title": ""}}, "42": {"idx": 42, "content": "TABLE II FAULT RATES (FRS) IN METHOD CATEGORIES. project G1 G2 G3 G4 0.1815 0.1387 0.0981 0.0694 Apache Tomcat ( 399 2198 ) ( 208 1500 ) ( 162 1652 ) ( 118 1701 ) 0.2035 0.2006 0.1565 0.1409 BIRT ( 1827 8977 ) ( 1635 8148 ) ( 1400 8942 ) ( 1122 7960 ) 0.3319 0.2845 0.2499 0.2144 Eclipse JDT UI ( 1825 5499 ) ( 1208 4246 ) ( 1151 4605 ) ( 905 4221 ) 0.3258 0.2906 0.2294 0.2193 Eclipse Platform UI ( 1303 4000 ) ( 1266 4357 ) ( 831 3622 ) ( 868 3958 ) 0.2439 0.2857 0.2430 0.2183 SWT ( 714 2928 ) ( 760 2660 ) ( 761 3132 ) ( 532 2437 ) and the confusing level decreases in the order of G1 to G4. As shown in Fig. 2, the FR tends to get lower as the confusing level decreases from G1 to G4: four out of \ufb01ve projects except for SWT show monotonic decreases. While SWT presents a different tendency, it also has a monotonically decreasing trend from G2 to G4, and the FR in G1 is still higher than G4. By comparing FR values, we see that methods in G1 are about 1.1 to 2.6 times more fault-prone than ones in G4. To check the difference in FR values between G1 and G4, we additionally performed \u03c72 test for each project data. As the results, we con\ufb01rmed that their differences in all projects are statistically signi\ufb01cant at a 5% signi\ufb01cance level3. Thus, a method having a pair of confusing variables is more likely to be faulty than a method having only non-confusing pairs.", "metadata": {"source": "data/16_ktashima_2018_faultprone.pdf", "page_number": 5, "chunk": 0, "title": ""}}, "43": {"idx": 43, "content": "If there is a pair of confusing (highly-similar) local variables in a method, one variable may be confusable with another variable. Such a confusable state may be related to a lack of clearness or readability in the program, and causes a fault creation or an overlooking of fault. However, the appearance of such a confusing variable pair may be related to the size or complexity of the program as well. That is to say, as programs become larger or more complex, programmers tend to use more variables, so the possibility of appearing such a confusing variable pair increases. Thus, we also compared NLD with the conventional size metric (lines of code: LOC) and the code complexity metric (cyclomatic complexity [12]: CC)\u2014we built a fault-prone method prediction model using NLD together with LOC and CC for each project. If NLD is worthless, it cannot contribute to the prediction because LOC and CC work dominantly. Here we used the random forest as our prediction model [13] since it is one of the most promising models for predicting fault-prone programs [14]. In a random forest, we can evaluate the importance of each variable (metric) by Breiman\u2019s method [13]. Table III shows the importance values computed for three metrics.", "metadata": {"source": "data/16_ktashima_2018_faultprone.pdf", "page_number": 5, "chunk": 1, "title": ""}}, "44": {"idx": 44, "content": "As the results, while NLD does not have the highest 3For each project, we performed a \u03c72 test on the null hypothesis that the FR in G1 is equal to the FR in G4.The results are: (Apache Tomcat) \u03c72 = 103.90, df = 1, p < 2.2 \u00d7 10\u221216; (BIRT) \u03c72 = 114.41, df = 1, p < 2.2\u00d710\u221216; (Eclipse JDT UI) \u03c72 = 162.58, df = 1, p < 2.2\u00d710\u221216; (Eclipse Platform UI) \u03c72 = 113.09, df = 1, p < 2.2 \u00d7 10\u221216; (SWT) \u03c72 = 4.73, df = 1, p = 0.02968. TABLE III IMPORTANCE VALUES OF METRICS IN EACH PROJECT\u2019S RANDOM FOREST. metric project NLD LOC CC Apache Tomcat 217.7 278.4 171.9 BIRT 722.2 1065.5 535.4 Eclipse JDT UI 546.1 882.3 395.0 Eclipse Platform UI 479.9 786.7 352.3 SWT 411.0 580.9 448.8 importance, it has higher value than CC except for SWT. Although the importance of NLD in SWT is less than that of CC, they seem to be at almost the same level. Therefore, NLD would be one of useful metrics for predicting fault-prone methods, and it is equivalent to or better than CC in predicting fault-prone methods. Since metric LOC showed the highest importance in the random forest models, we can say again that a larger method tends to be more fault-prone, which has been supported by the previous work (e.g., [15]). This result may be related to the number of local variables as well. That is to say, a larger method is likely to have more local variables within the body, and then the importance of non-confusable naming may get higher.", "metadata": {"source": "data/16_ktashima_2018_faultprone.pdf", "page_number": 5, "chunk": 2, "title": ""}}, "45": {"idx": 45, "content": "To examine the relationship between NLD and LOC, and to analyze the impact of confusing variable names on the method\u2019s fault-proneness in more depth, we plan to perform a further analysis focusing on the details of local variables\u2019 properties such as their dependent relationships in the future. E. Threats to Validity Since our data analysis is limited to \ufb01ve open source projects, it may be a threat to validity regarding the generality of our results. However, the granularity of our data analysis is at method-level and local variable-level, so we had a lot of samples\u201486, 743 methods and 406, 423 local variables as shown in Table I. Moreover, to mitigate the threat, we selected popular projects in which many developers are involved. To assure a higher generality of our \ufb01ndings, we will collect more data and analyze them in the future. Our data are also limited to Java programs. This limitation is from our data collection tool which we developed to extract local variable\u2019s features (names and scopes). Since the notion of local variable is common to modern programming languages, the difference in the programming language may not be a serious threat to validity. To clarify the impact of language difference, we plan to analyze programs written in other languages as our future work. Our metric NLD focuses only on the highest similarity between two local variables\u2019 names within a Java method.", "metadata": {"source": "data/16_ktashima_2018_faultprone.pdf", "page_number": 5, "chunk": 3, "title": ""}}, "46": {"idx": 46, "content": "Thus, other properties in regard to local variables are not taken into account: the number of local variables, their dependent relationships, etc. Such other properties might also affect the fault-proneness of Java methods. Although we analyzed the effect of NLD together with LOC (method size) anc CC (method complexity) using the random forest, we need to .", "metadata": {"source": "data/16_ktashima_2018_faultprone.pdf", "page_number": 5, "chunk": 4, "title": ""}}, "47": {"idx": 47, "content": "perform a further analysis using not only these three metrics but also other local-variable-related metrics in the future. V. CONCLUSION We focused on confusing (highly-similar) names of local variables in a Java method, and proposed to quantify the confusing level by the normalized Levenshtein distance (NLD). Then, we conducted an empirical study with \ufb01ve major open source software products, and proved that a Java method having a pair of confusing local variables is about 1.1 to 2.6 times more fault-prone than a method having only non-confusing ones. Moreover, we compared a power of NLD with popular code metrics in terms of the fault-prone method prediction through the random forest, and showed that NLD can probably outperform the cyclomatic complexity. Therefore, focusing on pairs of confusing local variables would be one of bene\ufb01cial points of view for enhancing the code quality management. Our future work includes: 1) a further analysis focusing on not only the string similarity but also the meaning closeness between local variables\u2019 names; 2) a further investigation on why the appearance of highly-similar variables relates with a fault creation or an overlooking of fault. ACKNOWLEDGMENT This work was supported by JSPS KAKENHI #16K00099 and #18K11246. The authors would like to thank the anonymous reviewers for their helpful comments to an earlier version of this paper. REFERENCES [1] F. Deissenboeck and M. Pizka, \u201cConcise and consistent naming,\u201d Softw. Quality J., vol.", "metadata": {"source": "data/16_ktashima_2018_faultprone.pdf", "page_number": 6, "chunk": 0, "title": ""}}, "48": {"idx": 48, "content": "14, no. 3, pp. 261\u2013282, Sep. 2006. [2] D. Lawrie, H. Feild, and D. Binkley, \u201cQuantifying identi\ufb01er quality: an analysis of trends,\u201d Empir. Softw. Eng., vol. 12, no. 4, pp. 359\u2013388, Aug. 2007. [3] D. Lawrie, C. Morrell, H. Feild, and D. Binkley, \u201cWhat\u2019s in a name? a study of identi\ufb01ers,\u201d in Proc. 14th Int\u2019l Conf. Program Comprehension, Jun. 2006, pp. 3\u201312. [4] G. Scanniello, M. Risi, P. Tramontana, and S. Romano, \u201cFixing faults in c and java source code: Abbreviated vs. full-word identi\ufb01er names,\u201d ACM Trans. Softw. Eng. Methodol., vol. 26, no. 2, pp. 6:1\u20136:43, Jul. 2017. [5] K. Kawamoto and O. Mizuno, \u201cPredicting fault-prone modules using the length of identi\ufb01ers,\u201d in Proc. 4th Int\u2019l Workshop Empir. Softw. Eng. in Practice, Oct. 2012, pp. 30\u201334. [6] D. Binkley, D. Lawrie, S. Maex, and C. Morrell, \u201cIdenti\ufb01er length and limited programmer memory,\u201d Sc. Comp. Prog., vol. 74, no. 7, pp. 430 \u2013 445, May. 2009. [7] H. Aman, S. Amasaki, T. Sasaki, and M. Kawahara, \u201cEmpirical analysis of change-proneness in methods having local variables with long names and comments,\u201d in Proc. 9th Int\u2019l Symp. Empir. Softw. Eng. & Measurement, Oct. 2015, pp. 50\u201353. [8] D. Binkley, M. Davis, D. Lawrie, J. I. Maletic, C. Morrell, and B. Sharif, \u201cThe impact of identi\ufb01er style on effort and comprehension,\u201d Empir. Softw. Eng., vol. 18, no. 2, pp. 219\u2013276, Apr. 2013. [9] S. Butler, M. Wermelinger, Y. Yu, and H. Sharp, \u201cRelating identi\ufb01er naming \ufb02aws and code quality: An empirical study,\u201d in Proc.", "metadata": {"source": "data/16_ktashima_2018_faultprone.pdf", "page_number": 6, "chunk": 1, "title": ""}}, "49": {"idx": 49, "content": "16th Working Conf. Reverse Eng., Oct. 2009, pp. 31\u201335. [10] B. Kernighan and R. Pike, The Practice of Programming, AddisonWesley, Boston, MA, 1999. [11] T. Mikolov, K. Chen, G. Corrado, and J. Dean, \u201cEf\ufb01cient estimation of word representations in vector space,\u201d Computing Research Repository, vol. abs/1301.3781, pp. 1\u201312, June 2013. [12] T. J. McCabe, \u201cA complexity measure,\u201d IEEE Trans. Softw. Eng., vol. SE-2, no. 4, pp. 308\u2013320, Dec. 1976. [13] L. Breiman, \u201cRandom forests,\u201d Machine Learning, vol. 45, no. 1, pp. 5\u201332, Oct. 2001. [14] S. Lessmann, B. Baesens, C. Mues, and S. Pietsch, \u201cBenchmarking classi\ufb01cation models for software defect prediction: A proposed framework and novel \ufb01ndings,\u201d IEEE Trans. Softw. Eng., vol. 34, no. 4, pp. 485\u2013 496, Jul. 2008. [15] N. E. Fenton and N. Ohlsson, \u201cQuantitative analysis of faults and failures in a complex software system,\u201d IEEE Trans. Softw. Eng., vol. 26, no. 8, pp. 797\u2013814, Aug. 2000. View publication stats .", "metadata": {"source": "data/16_ktashima_2018_faultprone.pdf", "page_number": 6, "chunk": 2, "title": ""}}, "50": {"idx": 50, "content": "See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/282495770 CERMINE: automatic extraction of structured metadata from scienti\ufb01c literature Article\u00a0\u00a0in\u00a0\u00a0International Journal on Document Analysis and Recognition (IJDAR) \u00b7 July 2015 DOI: 10.1007/s10032-015-0249-8 CITATIONS 208 READS 868 5 authors, including: Dominika Tkaczyk University of Warsaw 25 PUBLICATIONS\u00a0\u00a0\u00a0522 CITATIONS\u00a0\u00a0\u00a0 SEE PROFILE Mateusz Fedoryszak University of Warsaw 10 PUBLICATIONS\u00a0\u00a0\u00a0296 CITATIONS\u00a0\u00a0\u00a0 SEE PROFILE Piotr Jan Dendek University of Warsaw 11 PUBLICATIONS\u00a0\u00a0\u00a0286 CITATIONS\u00a0\u00a0\u00a0 SEE PROFILE \u0141ukasz Bolikowski University of Warsaw 42 PUBLICATIONS\u00a0\u00a0\u00a0515 CITATIONS\u00a0\u00a0\u00a0 SEE PROFILE All content following this page was uploaded by Dominika Tkaczyk on 18 February 2018. The user has requested enhancement of the downloaded file. .", "metadata": {"source": "data/24_tkaczyk_2015_cermine.pdf", "page_number": 1, "chunk": 0, "title": "CERMINE: automatic extraction of structured metadata from scientific literature"}}, "51": {"idx": 51, "content": "IJDAR (2015) 18:317\u2013335 DOI 10.1007/s10032-015-0249-8 ORIGINAL PAPER CERMINE: automatic extraction of structured metadata from scienti\ufb01c literature Dominika Tkaczyk1 \u00b7 Pawe\u0142 Szostek1 \u00b7 Mateusz Fedoryszak1 \u00b7 Piotr Jan Dendek1 \u00b7 \u0141ukasz Bolikowski1 Received: 10 December 2014 / Revised: 4 May 2015 / Accepted: 19 June 2015 / Published online: 3 July 2015 \u00a9 The Author(s) 2015. This article is published with open access at Springerlink.com Abstract CERMINE is a comprehensive open-source system for extracting structured metadata from scienti\ufb01c articles in a born-digital form. The system is based on a modular work\ufb02ow, whose loosely coupled architecture allows for individual component evaluation and adjustment, enables effortless improvements and replacements of independent parts of the algorithm and facilitates future architecture expanding. The implementations of most steps are based on supervised and unsupervised machine learning techniques, which simpli\ufb01es the procedure of adapting the system to new document layouts and styles. The evaluation of the extraction work\ufb02ow carried out with the use of a large dataset showed good performance for most metadata types, with the average F score of 77.5%. CERMINE system is available under an open-source licence and can be accessed at http:// cermine.ceon.pl. In this paper, we outline the overall work\ufb02ow architecture and provide details about individual steps implementations.", "metadata": {"source": "data/24_tkaczyk_2015_cermine.pdf", "page_number": 2, "chunk": 0, "title": "CERMINE: automatic extraction of structured metadata from scientific literature"}}, "52": {"idx": 52, "content": "We also thoroughly compare CERMINE to similar solutions, describe evaluation methodology and \ufb01nally report its results. B Dominika Tkaczyk d.tkaczyk@icm.edu.pl Pawe\u0142 Szostek pawel.szostek@gmail.com Mateusz Fedoryszak m.fedoryszak@icm.edu.pl Piotr Jan Dendek p.dendek@icm.edu.pl \u0141ukasz Bolikowski l.bolikowski@icm.edu.pl 1 Interdisciplinary Centre for Mathematical and Computational Modelling, University of Warsaw, ul. Prosta 69, 00-838 Warsaw, Poland Keywords Metadata extraction \u00b7 Bibliography extraction \u00b7 Content classi\ufb01cation \u00b7 Reference parsing \u00b7 Scienti\ufb01c literature analysis 1 Introduction Academic literature is a very important communication channel in the scienti\ufb01c world. Keeping track of the latest scienti\ufb01c \ufb01ndings and achievements, typically published in journals or conference proceedings, is a crucial aspect of the research work. Ignoring this task can result in de\ufb01ciencies in the knowledge related to the latest discoveries and trends, which in turn can lower the quality of the research, make results assessment much harder and signi\ufb01cantly limit the possibility to \ufb01nd new interesting research areas and challenges. Unfortunately, studying scienti\ufb01c literature, and in particular being up-to-date with the latest positions, is dif\ufb01cult and extremely time-consuming. The main reason for this is huge and constantly growing volume of scienti\ufb01c literature, and also the fact that publications are mostly available in the form of unstructured text.", "metadata": {"source": "data/24_tkaczyk_2015_cermine.pdf", "page_number": 2, "chunk": 1, "title": "CERMINE: automatic extraction of structured metadata from scientific literature"}}, "53": {"idx": 53, "content": "Modern digital libraries support the process of studying the literature by providing intelligent search tools, proposing similar and related documents, building citation and author networks,andsoon.Inordertoprovidesuchhigh-qualityservices, the library requires an access not only to the sources of stored documents, but also to their metadata including information such as title, authors, keywords, abstract or bibliographic references. Unfortunately, in practice good quality metadata is not always available, sometimes it is missing, full of errors or fragmentary. In such cases, the library needs a reliable automatic method to extract metadata and references from documents at hand. Even limited to analysing scienti\ufb01c literature only, the problem of extracting the document\u2019s metadata remains 123 .", "metadata": {"source": "data/24_tkaczyk_2015_cermine.pdf", "page_number": 2, "chunk": 2, "title": "CERMINE: automatic extraction of structured metadata from scientific literature"}}, "54": {"idx": 54, "content": "318 D. Tkaczyk et al. dif\ufb01cult and challenging, mainly due to the vast diversity of possible layouts and styles used in articles. In different documents, the same type of information can be displayed in different places using a variety of formatting styles and fonts. For instance, a random subset of 125,000 documents from PubMed Central [1] contains publications from nearly 500 different publishers, many of which use original layouts and styles in their articles. What is more, PDF format, which is currently the most popular format for storing source documents, does not preserve the information related to the document\u2019s structure, such as words and paragraphs, lists and enumerations, the structure of tables, the hierarchy of sections, or the reading order of the text. This information has to be reverse engineered based on the text content and the way the text is displayed in the source \ufb01le. These problems are addressed by CERMINE\u2014a comprehensive tool for automatic metadata extraction from born-digital scienti\ufb01c literature. The extraction algorithm proposed by CERMINE performs a thorough analysis of the input scienti\ufb01c publication in PDF format and extracts: \u2013 a rich set of document\u2019s metadata, \u2013 alistofbibliographicreferencesalongwiththeirmetadata, \u2013 structured full text with sections and subsections (currently in experimental phase). CERMINE is based on a modular work\ufb02ow composed of three paths and a number of steps with carefully de\ufb01ned input and output.", "metadata": {"source": "data/24_tkaczyk_2015_cermine.pdf", "page_number": 3, "chunk": 0, "title": "CERMINE: automatic extraction of structured metadata from scientific literature"}}, "55": {"idx": 55, "content": "By virtue of such work\ufb02ow architecture, individual steps can be maintained separately. As a result, it is easy to perform evaluation or training, improve or replace one step implementation without changing other parts of the work\ufb02ow. Designed as a universal solution, CERMINE is able to handle a vast variety of publication layouts reasonably well, instead of being perfect in processing a limited number of document layouts only. We achieved this by employing supervised and unsupervised machine learning algorithms trained on large diverse datasets. This decision also resulted in increased maintainability of the system, as well as its ability to adapt to new, previously unseen document layouts. The evaluation we conducted showed good performance of the key process steps and the entire metadata extraction process, with the overall F score of 77.5% (the details are provided in Sect. 5.5). The comparison to other similar systems showed CERMINE performs better for most metadata types. CERMINE web service, as well as the source code, can be accessed online [2]. This article is an extended version of the conference paper describing CERMINE system [3].", "metadata": {"source": "data/24_tkaczyk_2015_cermine.pdf", "page_number": 3, "chunk": 1, "title": "CERMINE: automatic extraction of structured metadata from scientific literature"}}, "56": {"idx": 56, "content": "In contrast to the previous version, the article contains: \u2013 detailed descriptions of all the extraction algorithm components, \u2013 the details related to feature selection for zone classi\ufb01ers, \u2013 new evaluation results for algorithms trained on GROTOAP2 dataset [4], \u2013 the evaluation of the bibliography extraction work\ufb02ow, \u2013 the comparison to other similar systems. In the following sections, we describe the state of the art, provide the details about the overall work\ufb02ow architecture and individual implementations and \ufb01nally report the evaluation methodology and its results. 2 State of the art Extracting metadata from articles and other documents is a well-studied problem. Older approaches expected scanned documents on the input and were prepared for executing full digitization from bitmap images. Nowadays, we have to deal with growing amount of born-digital documents, which do not require individual character recognition. The approaches to the problem differ in the scope of the solution, supported \ufb01le formats and methods and algorithms used. Most approaches focus on extracting the article\u2019s metadata only and often do not process the entire input document. Proposed solutions are usually based on rules and heuristics or machine learning techniques. For example, Giuffrida et al. [5] extract the content from PostScript \ufb01les usingatool basedon pstotext, whilebasic document metadata is extracted by a set of rules and features computed for extracted text chunks.", "metadata": {"source": "data/24_tkaczyk_2015_cermine.pdf", "page_number": 3, "chunk": 2, "title": "CERMINE: automatic extraction of structured metadata from scientific literature"}}, "57": {"idx": 57, "content": "Another example of a rule-based system is PDFX described by Constatin et al. [6]. PDFX can be used for converting scholarly articles in PDF format to their XML representation by annotating fragments of the input documents and extracts basic metadata, structured full text and unparsed reference strings. Pdf-extract [7] is an open-source tool for identifying and extracting semantically signi\ufb01cant regions of scholarly articles in PDF format. It uses a combination of visual cues and content traits to perform structural analysis in order to determine columns, headers, footers and sections, detect references sections and \ufb01nally extract individual references. Machine learning-based approaches are far more popular. They differ in classi\ufb01cation algorithms, document fragments that undergo the classi\ufb01cation (text chunks, lines or blocks) and extracted features. For example, Han et al. [8] extract metadata from the headers of scienti\ufb01c papers by two-stage classi\ufb01cation of text lines with the use of support vector machines and text-related features. Another example of SVM-based approach is metadata extractor used in CRIS systems proposed by Kovacevic et al. [9]. The tool classi\ufb01es the lines of text using both geometric and text-related 123 .", "metadata": {"source": "data/24_tkaczyk_2015_cermine.pdf", "page_number": 3, "chunk": 3, "title": "CERMINE: automatic extraction of structured metadata from scientific literature"}}, "58": {"idx": 58, "content": "CERMINE: automatic extraction of structured metadata from scienti\ufb01c literature 319 features in order to extract the document\u2019s metadata from PDFs. Lu et al. [10] analyse scanned scienti\ufb01c journals in order to obtain volume level, issue level and article level metadata. In their approach, the pages are \ufb01rst OCRed, rulebased pattern matching is used for volume and issue title pages, while article metadata is extracted using SVM and both geometric and textual features of text lines. Other classi\ufb01cation techniques include for example hidden Markov models, neural classi\ufb01ers, maximum entropy and conditional random \ufb01elds. Marinai [11] extracts characters from PDF documents using JPedal package, performs rule-based page segmentation, and \ufb01nally employs neural classi\ufb01erforzoneclassi\ufb01cation.CuiandChen[12]useHMM classi\ufb01ertoextractmetadatafromPDFdocuments,whiletext extraction and page segmentation are done by pdftohtml, a third-party open-source tool. The system based on TeamBeam algorithm proposed by Kern et al. [13] is able to extract a basic set of metadata from PDF documents using an enhanced Maximum Entropy classi\ufb01er. Lopez [14] proposes GROBID system for analysing scienti\ufb01c texts in PDF format. GROBID uses CRF in order to extract document\u2019s metadata, full text and a list of parsed bibliographic references. ParsCit, described by Luong et al.", "metadata": {"source": "data/24_tkaczyk_2015_cermine.pdf", "page_number": 4, "chunk": 0, "title": "CERMINE: automatic extraction of structured metadata from scientific literature"}}, "59": {"idx": 59, "content": "[15] also uses CRF for extracting the logical structure of scienti\ufb01c articles, including the document\u2019s metadata, structured full text and parsed bibliography. ParsCitanalysesdocumentsintextformat,andthereforedoes not use geometric hints present in the PDF \ufb01les. Reference sections are typically located in the documents using heuristics [6,7,16,17] or machine learning [14,18]. Citation parsing, that is extracting metadata from citation strings, is usually performed using regular expressions and knowledge-based approaches [19,20], or more popular machine learning techniques, such as CRF [16\u201318,21], SVM [22] or HMM [23]. A number of systems mentioned above are available online: PDFX [24] (the tool is closed source, available only as a web service), GROBID [25], ParsCit [26] and Pdfextract [7]. In Sect. 5.6, we report the results of comparing the performance of these tools with CERMINE. Table 1 shows the scope of the information various metadata extraction systems are able to extract. The most important features differentiating CERMINE from other approaches are: \u2013 CERMINE is able to extract bibliographic information related to the document, such as journal name, volume, issue or pages range. \u2013 The algorithms use not only the text content of the document, but also its geometric features related to the way the text is displayed in the source PDF \ufb01le. \u2013 Our solution is based mostly on machine learning, which increases its ability to conform to different article layouts.", "metadata": {"source": "data/24_tkaczyk_2015_cermine.pdf", "page_number": 4, "chunk": 1, "title": "CERMINE: automatic extraction of structured metadata from scientific literature"}}, "60": {"idx": 60, "content": "\u2013 The \ufb02exibility of the system implementation is granted by its modular architecture. \u2013 For most metadata types, the solution is very effective. \u2013 The source code is open and the web service is available online [2]. Table 1 The comparison of the scope of the information extracted by various metadata extraction systems CERMINE PDFX GROBID ParsCit Pdf-extract Title \u2713 \u2713 \u2713 \u2713 \u2713 Author \u2713 \u2713 \u2713 \u2713 \u00d7 Af\ufb01liation \u2713 \u00d7 \u2713 \u2713 \u00d7 Af\ufb01liation\u2019s metadata \u2713 \u00d7 \u2713 \u00d7 \u00d7 Author\u2013af\ufb01liation \u2713 \u00d7 \u2713 \u00d7 \u00d7 Email address \u2713 \u2713 \u2713 \u2713 \u00d7 Author\u2013email \u2713 \u00d7 \u2713 \u00d7 \u00d7 Abstract \u2713 \u2713 \u2713 \u2713 \u00d7 Keywords \u2713 \u00d7 \u2713 \u2713 \u00d7 Journal \u2713 \u00d7 \u00d7 \u00d7 \u00d7 Volume \u2713 \u00d7 \u00d7 \u00d7 \u00d7 Issue \u2713 \u00d7 \u00d7 \u00d7 \u00d7 Pages range \u2713 \u00d7 \u00d7 \u00d7 \u00d7 Year \u2713 \u00d7 \u2713 \u00d7 \u00d7 DOI \u2713 \u00d7 \u2713 \u00d7 \u00d7 Reference \u2713 \u2713 \u2713 \u2713 \u2713 Reference\u2019s metadata \u2713 \u00d7 \u2713 \u2713 \u00d7 The table shows simple metadata types (e.g. title, author, abstract or bibliographic references), relations between them (author\u2013af\ufb01liation, author\u2013email address), and also metadata in the structured form (references and af\ufb01liations along with their metadata) 123 .", "metadata": {"source": "data/24_tkaczyk_2015_cermine.pdf", "page_number": 4, "chunk": 2, "title": "CERMINE: automatic extraction of structured metadata from scientific literature"}}, "61": {"idx": 61, "content": "320 D. Tkaczyk et al. 3 System architecture CERMINE accepts a scienti\ufb01c publication in PDF format on the input. The extraction algorithm inspects the entire content of the document and produces two kinds of output: the document\u2019s metadata and bibliography. CERMINE\u2019s extraction work\ufb02ow is composed of three paths (Fig. 1): (A) Basic structure extraction path takes a PDF \ufb01le on the input and produces its geometric hierarchical representation, which stores the entire text content of the input document and the geometric features related to the way the text is displayed in the PDF \ufb01le. More precisely, the structure is composed of pages, zones, lines, words and characters, along with their coordinates and dimensions. Additionally, the reading order of all elements is set and every zone is labelled with one of four general categories: metadata, references, body or other. (B) Metadata extraction path analyses metadata parts of the geometric hierarchical structure and extracts a rich set of document\u2019s metadata from them. (C) Bibliography extraction path analyses parts of the structure labelled as references. The result is a list of document\u2019s parsed bibliographic references. Table 2 shows the decomposition of the extraction work\ufb02ow into paths and steps and provides basic information about tools and algorithms used for every step. 3.1 Models and formats CERMINE\u2019s input document format is PDF, currently the most popular format for storing the sources of scienti\ufb01c publications.", "metadata": {"source": "data/24_tkaczyk_2015_cermine.pdf", "page_number": 5, "chunk": 0, "title": "CERMINE: automatic extraction of structured metadata from scientific literature"}}, "62": {"idx": 62, "content": "A PDF \ufb01le contains by design the text of the document in the form of a list of chunks of various length specifying the position, size and other geometric features of the text as well as the information related to the fonts and graphics. PDF documents look the same no matter what software or hardware is used for viewing them. Unfortunately, the format does not preserve any information related to the logical structure of the text, such as words, lines, paragraphs, enumerations, sections, section titles or even the reading order of text chunks. This information has to be deduced from the geometric features of the text. Currently, the extraction work\ufb02ow does not include any OCR phase, it analyses only the PDF text stream found in the input document. As a result, PDF documents containing scanned pages in the form of images will not be properly processed. We plan to provide this functionality in the future. Thanks to the \ufb02exible architecture of the work\ufb02ow, the only required change is adding an alternative implementation of the character extraction step, able to perform optical character recognition on scanned pages and extract characters along with dimensions and positions. Other parts of the work\ufb02ow will remain the same.", "metadata": {"source": "data/24_tkaczyk_2015_cermine.pdf", "page_number": 5, "chunk": 1, "title": "CERMINE: automatic extraction of structured metadata from scientific literature"}}, "63": {"idx": 63, "content": "CERMINE\u2019s intermediate model of the document constructedduringthe\ufb01rstprocesspathisahierarchicalstructure that holds the entire text content of the article, while also preserving the information related to the way elements are displayed in the corresponding PDF \ufb01le. In this representation, an article is a list of pages, each page contains a list of zones, each zone contains a list of lines, each line contains a list of words, and \ufb01nally each word contains a list of characters. Each structure element can be described by its text content and bounding box (a rectangle enclosing the element). The structure contains also the natural reading order for the elements on each level. Additionally, labels describing the role in the document are assigned to zones. The smallest elements in the structure are individual characters. A word is a continuous sequence of characters placed in one line with no spaces between them. Punctuation marks and typographical symbols can be separate words or parts of adjacent words, depending on the presence ofspaces. Fig. 1 CERMINE\u2019s extraction work\ufb02ow architecture. At the beginning, the basic structure is extracted from the PDF \ufb01le. Then, metadata and bibliography are extracted in two parallel paths 123 .", "metadata": {"source": "data/24_tkaczyk_2015_cermine.pdf", "page_number": 5, "chunk": 2, "title": "CERMINE: automatic extraction of structured metadata from scientific literature"}}, "64": {"idx": 64, "content": "CERMINE: automatic extraction of structured metadata from scienti\ufb01c literature 321 Table 2 The decomposition of CERMINE\u2019s extraction work\ufb02ow into independent processing paths and steps Path Step Goal Implementation A. Basic structure extraction A1. Character extraction Extracting individual characters along with their page coordinates and dimensions from the input PDF \ufb01le iText library A2. Page segmentation Constructing the document\u2019s geometric hierarchical structure containing (from the top level) pages, zones, lines, words and characters, along with their page coordinates and dimensions Enhanced Docstrum A3. Reading order resolving Determining the reading order for all structure elements Bottom-up heuristic-based A4. Initial zone classi\ufb01cation Classifying the document\u2019s zones into four main categories: metadata, body, references and other SVM B. Metadata extraction B1. Metadata zone classi\ufb01cation Classifying the document\u2019s zones into speci\ufb01c metadata classes SVM B2. Metadata extraction Extracting atomic metadata information from labelled zones Simple rule-based C. Bibliography extraction C1. Reference strings extraction Dividing the content of references zones into individual reference strings K-means clustering C2. Reference parsing Extracting metadata information from references strings CRF Hyphenated words that are divided into two lines appear in the structure as two separate words that belong to different lines.", "metadata": {"source": "data/24_tkaczyk_2015_cermine.pdf", "page_number": 6, "chunk": 0, "title": "CERMINE: automatic extraction of structured metadata from scientific literature"}}, "65": {"idx": 65, "content": "A line is a sequence of words that forms a consistent fragment of the document\u2019s text. Words placed geometrically in the same line of the page, that are parts of neighbouring columns, in the structure do not belong to the same line. A zone is a consistent fragment of the document\u2019s text, geometrically separated from surrounding fragments and not divided into paragraphs or columns. All bounding boxes are rectangles with edges parallel to the page\u2019s edges. A bounding box is de\ufb01ned by two points: left upper corner and right lower corner of the rectangle. The coordinates are given in typographic points (1 typographic point equals to 1/72 of an inch). The origin of the coordinate system is the left upper corner of the page. The model can be serialized using XML TrueViz format [27]. The listing below shows a fragment of an example TrueViz \ufb01le. Repeated fragments or fragments that are not used by the system have been omitted.", "metadata": {"source": "data/24_tkaczyk_2015_cermine.pdf", "page_number": 6, "chunk": 1, "title": "CERMINE: automatic extraction of structured metadata from scientific literature"}}, "66": {"idx": 66, "content": "<?xml version=\"1.0\" encoding=\"UTF-8\"?> <!DOCTYPE Document SYSTEM \"Trueviz.dtd\"> <Document> [...] <Page> <PageID Value=\"0\"/> [...] <PageNext Value=\"1\"/> <Zone> <ZoneID Value=\"0\"/> <ZoneCorners> <Vertex x=\"55.4\" y=\"34.3\"/> <Vertex x=\"250.5\" y=\"58.3\"/> </ZoneCorners> <ZoneNext Value=\"1\"/> <Classification> <Category Value=\"BIB_INFO\"/> <Type Value=\"\"/> </Classification> <Line> <LineID Value=\"0\"/> <LineCorners> <Vertex x=\"55.4\" y=\"34.3\"/> <Vertex x=\"250.5\" y=\"58.3\"/> </LineCorners> <LineNext Value=\"1\"/> <LineNumChars Value=\"\"/> <Word> <WordID Value=\"0\"/> <WordCorners> <Vertex x=\"55.4\" y=\"34.3\"/> <Vertex x=\"115.3\" y=\"58.3\"/> </WordCorners> <WordNext Value=\"1\"/> <WordNumChars Value=\"\"/> <Character> <CharacterID Value=\"0\"/> <CharacterCorners> <Vertex x=\"55.4\" y=\"34.3\"/> <Vertex x=\"74.1\" y=\"58.3\"/> </CharacterCorners> <CharacterNext Value=\"1\"/> <GT_Text Value=\"B\"/> </Character> <Character> [...] </Word> [...] </Line> [...] </Zone> </Page> </Document> 123 .", "metadata": {"source": "data/24_tkaczyk_2015_cermine.pdf", "page_number": 6, "chunk": 2, "title": "CERMINE: automatic extraction of structured metadata from scientific literature"}}, "67": {"idx": 67, "content": "322 D. Tkaczyk et al. The output format of the extraction work\ufb02ow is NLM JATS [28]. JATS (Journal Article Tag Suite) de\ufb01nes a rich set of XML elements and attributes for describing scienti\ufb01c publications and is an application of NISO Z39.96-2012 standard [29]. Documents in JATS format can store a wide range of structured metadata of the document (title, authors, af\ufb01liations, abstract, journal name, identi\ufb01ers, etc.), the full text (the hierarchy of sections, headers and paragraphs, structured tables, equations, etc.), the document\u2019s bibliography in the form of a list of references along with their identi\ufb01ers and metadata, and also the information related to the text formatting. 4 Extraction work\ufb02ow implementation In this section, we describe in detail the approaches and algorithms used to implement all the individual work\ufb02ow steps. 4.1 Layout analysis Layout analysis is the initial phase of the entire work\ufb02ow. Its goal is to create a hierarchical structure of the document preserving the entire text content of the input document and featuresrelatedtothewaythetextisdisplayedinthePDF\ufb01le. Layout analysis is composed of the following steps: 1. Character extraction (A1)\u2014extracting individual characters from a PDF document. 2. Page segmentation (A2)\u2014joining characters into words, lines and zones. 3. Reading order determination (A3)\u2014calculating the reading order for all the structure levels.", "metadata": {"source": "data/24_tkaczyk_2015_cermine.pdf", "page_number": 7, "chunk": 0, "title": "CERMINE: automatic extraction of structured metadata from scientific literature"}}, "68": {"idx": 68, "content": "4.1.1 Character extraction The purpose of the character extraction step is to extract individual characters from the PDF stream along with their positions on the page, widths and heights. These geometric parameters play important role in further steps, in particular page segmentation and content classi\ufb01cation. The implementation of character extraction is based on open-source iText [30] library. We use iText to iterate over PDF\u2019s text-showing operators. During the iteration, we extract text strings along with their size and position on the page. Next, extracted strings are split into individual characters and their individual widths and positions are calculated. The result is an initial \ufb02at structure of the document, which consists only of pages and characters. The widths and heights computed for individual characters are approximate and can slightly differ from the exact values depending on the font, styleandcharactersused.Fortunately,thoseapproximatevalues are suf\ufb01cient for further steps. 4.1.2 Page segmentation The goal of page segmentation step is to create a geometric hierarchical structure storing the document\u2019s content. As a result the document is represented by a list of pages, each page contains a set of zones, each zone contains a set of text lines, each line contains a set of words, and \ufb01nally each word contains a set of individual characters. Each object in the structure has its content, position and dimensions.", "metadata": {"source": "data/24_tkaczyk_2015_cermine.pdf", "page_number": 7, "chunk": 1, "title": "CERMINE: automatic extraction of structured metadata from scientific literature"}}, "69": {"idx": 69, "content": "The structure is heavily used in further steps, especially zone classi\ufb01cation and bibliography extraction. Page segmentation is implemented with the use of a bottom-up Docstrum algorithm [31]: 1. The algorithm is based to a great extent on the analysis of the nearest-neighbour pairs of individual characters. In the \ufb01rst step, \ufb01ve nearest components for every character are identi\ufb01ed (red lines in Fig. 2). 2. In order to calculate the text orientation (the skew angle), we analyse the histogram of the angles between the elements of all nearest-neighbour pairs. The peak value is assumed to be the angle of the text. Since in the case of born-digital documents, the skew is almost always horizontal, and this step is mostly useful for documents containing scanned pages. 3. Next, within-line spacing is estimated by detecting the peak of the histogram of distances between the nearest neighbours. For this histogram, we use only those pairs, in which the angle between components is similar to the estimated text orientation angle (blue lines in Fig. 2). All the histograms used in Docstrum are smoothed to avoid detecting local abnormalities. An example of a smoothed distance histogram is shown in Fig. 3. 4. Similarly, between-line spacing is also estimated with the use of a histogram of the distances between the nearestneighbour pairs. In this case, we include only those pairs that are placed approximately in the line perpendicular to the text line orientation (green lines in Fig. 2). 5.", "metadata": {"source": "data/24_tkaczyk_2015_cermine.pdf", "page_number": 7, "chunk": 2, "title": "CERMINE: automatic extraction of structured metadata from scientific literature"}}, "70": {"idx": 70, "content": "Next, line segments are found by performing a transitive closure on within-line nearest-neighbour pairs. Fig. 2 An example fragment of a text zone in a scienti\ufb01c article. The \ufb01gure shows \ufb01ve nearest neighbours of a given character (red lines), neighbours placed in the same line used to determine in-line spacing (blue lines), and neighbours placed approximately in the line perpendicular to the text line orientation used to determine between-line spacing (green lines) (color \ufb01gure online) 123 .", "metadata": {"source": "data/24_tkaczyk_2015_cermine.pdf", "page_number": 7, "chunk": 3, "title": "CERMINE: automatic extraction of structured metadata from scientific literature"}}, "71": {"idx": 71, "content": "CERMINE: automatic extraction of structured metadata from scienti\ufb01c literature 323 0 250 500 750 1000 1250 0 5 10 15 20 Distance Count original smoothed Fig. 3 An example of a nearest-neighbour distance histogram. The \ufb01gure shows both original and smoothed versions of the histogram. The peak distance chosen based on the original data would be the global maximum, even though the histogram contains two close peaks of similarly high frequency. Thanks to smoothing both local peaks are taken into account, shifting the resulting peak slightly to the left and yielding more reliable results To prevent joining line segments belonging to different columns, the components are connected only if the distance between them is suf\ufb01ciently small. 6. The zones are then constructed by grouping the line segments on the basis of heuristics related to spatial and geometric characteristics: parallelness, distance and overlap. 7. The segments belonging to the same zone and placed in one line horizontally are merged into \ufb01nal text lines. 8. Finally, we divide the content of each text line into words based on within-line spacing.", "metadata": {"source": "data/24_tkaczyk_2015_cermine.pdf", "page_number": 8, "chunk": 0, "title": "CERMINE: automatic extraction of structured metadata from scientific literature"}}, "72": {"idx": 72, "content": "A few improvements were added to the Docstrum-based implementation of page segmentation: \u2013 the distance between connected components, which is used for grouping components into lines, has been split into horizontal and vertical distance (based on estimated text orientation angle), \u2013 \ufb01xed maximum distance between lines that belong to the same zone has been replaced with a value scaled relatively to the line height, \u2013 merging of lines belonging to the same zone has been added, \u2013 rectangular smoothing window has been replaced with Gaussian smoothing window, \u2013 merging of highly overlapping zones has been added, \u2013 words determination based on within-line spacing has been added. 4.1.3 Reading order resolving A PDF \ufb01le contains by design a stream of strings that undergoes extraction and segmentation process. As a result, we obtain pages containing characters grouped into zones, lines and words, all of which have a form of unsorted bag of items. The aim of setting the reading order is to determine the right sequence in which all the structure elements should be read. This information is used in zone classi\ufb01ers and also allows to extract the full text of the document in the right order. An example document page with a reading order of the zones is shown in Fig. 4. Readingorderresolvingalgorithmisbasedonabottom-up strategy: \ufb01rst characters are sorted within words and words within lines horizontally, then lines are sorted vertically within zones, and \ufb01nally we sort zones.", "metadata": {"source": "data/24_tkaczyk_2015_cermine.pdf", "page_number": 8, "chunk": 1, "title": "CERMINE: automatic extraction of structured metadata from scientific literature"}}, "73": {"idx": 73, "content": "The fundamental principle for sorting zones was taken from [32]. We make use of an observation that the natural reading order in most modern languages descends from top to bottom, if successive zones are aligned vertically, otherwise it traverses from left to right. There are few exceptions to this rule, for example, Arabic script, and such cases would not be handled properly by the algorithm. This observation is re\ufb02ected in the distances counted for all zone pairs: the distance is calculated using the angle of the slope of the vector connecting Fig. 4 An example page from a scienti\ufb01c publication. The image shows the zones and their reading order 123 .", "metadata": {"source": "data/24_tkaczyk_2015_cermine.pdf", "page_number": 8, "chunk": 2, "title": "CERMINE: automatic extraction of structured metadata from scientific literature"}}, "74": {"idx": 74, "content": "324 D. Tkaczyk et al. zones. As a result, zones aligned vertically are in general closer than those aligned horizontally. Then, using an algorithm similar to hierarchical clustering methods, we build a binary tree by repeatedly joining the closest zones and groups of zones. After that, for every node its children are swapped, if needed. Finally, an in order tree traversal gives the desired zones order. 4.2 Content classi\ufb01cation The goal of content classi\ufb01cation is to determine the role played by every zone in the document. This is done in two steps: initial zone classi\ufb01cation (A4) and metadata zone classi\ufb01cation (B1). The goal of initial classi\ufb01cation is to label each zone with one of four general classes: metadata (document\u2019s metadata, e.g. title, authors, abstract, keywords, and so on), references (the bibliography section), body (publication\u2019s text, sections, section titles, equations, \ufb01gures and tables, captions) or other (acknowledgments, con\ufb02icts of interests statements, page numbers, etc.).", "metadata": {"source": "data/24_tkaczyk_2015_cermine.pdf", "page_number": 9, "chunk": 0, "title": "CERMINE: automatic extraction of structured metadata from scientific literature"}}, "75": {"idx": 75, "content": "The goal of metadata zone classi\ufb01cation is to classify all metadata zones into speci\ufb01c metadata classes: title (the title of the document), author (the names of the authors), af\ufb01liation (authors\u2019 af\ufb01liations), editor (the names of the editors), correspondence (addresses and emails), type (the type speci\ufb01ed in the document, such as \u201cresearch article\u201d, \u201ceditorial\u201d or \u201ccase study\u201d, abstract (document\u2019s abstract), keywords (keywords listed in the document), bib_info (for zones containing bibliographic information, such as journal name, volume, issue, DOI, etc.), dates (the dates related to the process of publishing the article). The classi\ufb01ers are implemented in a similar way. They both employ support vector machines, and the implementation is based on LibSVM library [33]. They differ in target zone labels, extracted features and SVM parameters used. The features, as well as SVM parameters were selected using the same procedure, described in Sects. 4.2.1 and 4.2.2. Support vector machines is a very powerful classi\ufb01cation technique able to handle a large variety of input and work effectively even with training data of a small size. The algorithm is based on \ufb01nding the optimal separation hyperplane and is little prone to over\ufb01tting. It does not require a lot of parameters and can deal with highly dimensional data. SVM is widely used for content classi\ufb01cation and achieves very good results in practice.", "metadata": {"source": "data/24_tkaczyk_2015_cermine.pdf", "page_number": 9, "chunk": 1, "title": "CERMINE: automatic extraction of structured metadata from scientific literature"}}, "76": {"idx": 76, "content": "The decision of splitting content classi\ufb01cation into two separate classi\ufb01cation steps, as opposed to implementing onlyonezoneclassi\ufb01cationstep,wasbasedmostlyonaspects related to the work\ufb02ow architecture and maintenance. In fact both tasks have different characteristics and needs. The goal of the initial classi\ufb01er is to divide the article\u2019s content into three general areas of interest, which can be then analysed independently in parallel, while metadata classi\ufb01er performs far more detailed analysis of only a small subset of all zones. The implementation of the initial classi\ufb01er is more stable: the target label set does not change, and once trained on a reasonably large and diverse dataset, the classi\ufb01er performs well on other layouts as well. On the other hand, metadata zones have much more variable characteristics across different layouts, and from time to time there is a need to tune the classi\ufb01er or retrain it using a wider document set. What is more, sometimes the classi\ufb01er has to be extended to be able to capture new labels, not considered before (for example a special label for zones containing both author and af\ufb01liation, a separate label for categories or general terms). For these reasons, we decided to implement content classi\ufb01cation in two separate steps. As a result, we can maintain them independently, and for example adding another metadata label to the system does not change the performance of recognizing the bibliography sections.", "metadata": {"source": "data/24_tkaczyk_2015_cermine.pdf", "page_number": 9, "chunk": 2, "title": "CERMINE: automatic extraction of structured metadata from scientific literature"}}, "77": {"idx": 77, "content": "It is also possible that in the future the metadata classi\ufb01er will be reimplemented using a different technique, allowing to add new training cases incrementally, for example using a form of online learning. For completeness, we compared the performance of a single zone classi\ufb01er assigning all needed labels in one step to the classi\ufb01er containing two separate classi\ufb01ers executed in a sequence (our current solution). The results can be found in Sect. 5.3. 4.2.1 Feature selection The features used by the classi\ufb01ers were selected with the use of the zone validation dataset (all the datasets used for experiments are described in Sect. 5.1). For each classi\ufb01er, we analysed 97 features in total.", "metadata": {"source": "data/24_tkaczyk_2015_cermine.pdf", "page_number": 9, "chunk": 3, "title": "CERMINE: automatic extraction of structured metadata from scientific literature"}}, "78": {"idx": 78, "content": "The features capture various aspects of the content and surroundings of the zones and can be divided into the following categories: \u2013 geometric\u2014basedongeometricattributes,someexamples include: zone\u2019s height and width, height to width ratio, zone\u2019s horizontal and vertical position, the distance to the nearest zone, empty space below and above the zone, mean line height, whether the zone is placed at the top, bottom, left or right side of the page; \u2013 lexical\u2014based upon keywords characteristic for different parts of narration, such as: af\ufb01liations, acknowledgments, abstract, keywords, dates, references, or article type; these features typically check, whether the text of the zone contains any of the characteristic keywords; \u2013 sequential\u2014based on sequence-related information, some examples include the label of the previous zone (according to the reading order) and the presence of the 123 .", "metadata": {"source": "data/24_tkaczyk_2015_cermine.pdf", "page_number": 9, "chunk": 4, "title": "CERMINE: automatic extraction of structured metadata from scientific literature"}}, "79": {"idx": 79, "content": "CERMINE: automatic extraction of structured metadata from scienti\ufb01c literature 325 same text blocks on the surrounding pages, whether the zone is placed in the \ufb01rst/last page of the document; \u2013 formatting\u2014related to text formatting in the zone, examples include font size in the current and adjacent zones, the amount of blank space inside zones, mean indentation of text lines in the zone; \u2013 heuristics\u2014based on heuristics of various nature, such as the count and percentage of lines, words, uppercase words, characters, letters, upper/lowercase letters, digits, whitespaces, punctuation, brackets, commas, dots, etc; also whether each line starts with enumeration-like tokens, or whether the zone contains only digits. In general, feature selection was performed by analysing the correlations between the features and between features and expected labels. For simplicity, we treat all the features as numericalvariables;thevaluesofbinaryfeaturesaredecoded as 0 or 1. The labels, on the other hand, are an unordered categorical variable. Let L be a set of zone labels for a given classi\ufb01er, n the number of the observations (zones) in the validation dataset and k = 97 the initial number of analysed features. For ith feature, where 0 \u2264i < k, we can de\ufb01ne fi \u2208Rn, a vector of the values of the feature ith for subsequent observations. Let also l \u2208Ln be the corresponding vector of zone labels. In the \ufb01rst step, we removed redundant features, highly correlated with other features.", "metadata": {"source": "data/24_tkaczyk_2015_cermine.pdf", "page_number": 10, "chunk": 0, "title": "CERMINE: automatic extraction of structured metadata from scientific literature"}}, "80": {"idx": 80, "content": "For each pair of feature vectors, we calculated the Pearson\u2019s correlation score and identi\ufb01ed all the pairs fi, f j \u2208Rn, such that |corr( fi, f j)| > 0.9 Next, for every feature from highly correlated pairs, we calculated the mean absolute correlation: meanCorr( fi) = 1 k k\u22121 \u0002 j=0 corr( fi, f j) and from each highly correlated pair, the feature with higher meanCorr was eliminated. This left us with 78 and 75 features for initial and metadata classi\ufb01ers, respectively. Let\u2019s denote the number of remaining features as k\u2032. After eliminating features using correlations between them, we analysed the features using their associations with the expected zone labels vector l. To calculate the correlation between a single feature vector fi (numeric) and label vector l (unordered categorical), we employed Goodman and Kruskal\u2019s \u03c4 (tau) measure [34]. Let\u2019s denote it as \u03c4( fi,l). Let f0, f1, . . . fk\u2032\u22121 be the sequence of the feature vectors ordered by non-decreasing \u03c4 measure, that is \u03c4( f0,l) \u2264\u03c4( f1,l) \u2264\u00b7 \u00b7 \u00b7 \u2264\u03c4( fk\u2032\u22121,l) The features were then added to the classi\ufb01er one by one, starting from the best one (the mostly correlated with the labelsvector, fk\u2032\u22121),andattheendtheclassi\ufb01ercontainedthe entire feature set. At each step, we performed a \ufb01vefold crossvalidation on the validation dataset and calculated the overall F score as an average for individual labels. For completeness, we also repeated the same process with reversed order of the features, starting with less useful features.", "metadata": {"source": "data/24_tkaczyk_2015_cermine.pdf", "page_number": 10, "chunk": 1, "title": "CERMINE: automatic extraction of structured metadata from scientific literature"}}, "81": {"idx": 81, "content": "The results for initial and metadata classi\ufb01er are shown in Figs. 5 and 6, respectively. Using these results, we eliminated a number of the least useful features f0, f1, . . . ft, such that the performance of the classi\ufb01er with the remaining features was similar to the performance of the classi\ufb01er trained on the entire feature set. Final feature sets contain 53 and 51 features for initial and metadata classi\ufb01er, respectively. 0.00 0.25 0.50 0.75 1.00 0 10 20 30 40 50 60 70 80 Number of features Average F1 score increasing tau decreasing tau Fig. 5 Average F score for initial classi\ufb01er for \ufb01vefold crossvalidation for various number of features. Blue line shows the change in F score, while adding features from the most to the least useful one, and the red line shows the increase with the reversed order. The vertical line marks the feature set chosen for the \ufb01nal classi\ufb01er (color \ufb01gure online) 0.00 0.25 0.50 0.75 1.00 0 10 20 30 40 50 60 70 Number of features Average F1 score increasing tau decreasing tau Fig. 6 Average F score for metadata classi\ufb01er for \ufb01vefold crossvalidation for various number of features. Blue line shows the increase in F score while adding features from the most to the least useful one, and the red line shows the increase with the reversed order. The vertical line marks the feature set chosen for the \ufb01nal classi\ufb01er (color \ufb01gure online) 123 .", "metadata": {"source": "data/24_tkaczyk_2015_cermine.pdf", "page_number": 10, "chunk": 2, "title": "CERMINE: automatic extraction of structured metadata from scientific literature"}}, "82": {"idx": 82, "content": "326 D. Tkaczyk et al. 4.2.2 SVM parameters adjustment SVM parameters were also estimated using the zone validation dataset. The feature vectors were scaled linearly to interval [0, 1] according to the bounds found in the learning samples. In order to \ufb01nd the best parameters for the classi\ufb01ers we performed a grid search over a three-dimensional space \u27e8K, \u0393, C\u27e9, where K is a set of kernel function types (linear, fourth degree polynomial, radial-basis and sigmoid), \u0393 = {2i|i \u2208[\u221215, 3]} is a set of possible values of the kernel coef\ufb01cient \u03b3 , and C = {2i|i \u2208[\u22125, 15]} is a set of possible values of the penalty parameter. For every combination of the parameters, we performed a \ufb01vefold cross-validation. Finally, we chose those parameters, for which we obtained the highest mean F score (calculated as an average for individual classes). We also used classes weights based on the number of their training samples to set larger penalty for less represented classes. Parameters for the best obtained results are presented in Tables 3 and 4. In both cases, we chose radial-basis kernel function, and chosen values of C and \u03b3 parameters are 25 and 2\u22123 in the case of initial classi\ufb01er and 29 and 2\u22123 in the case of metadata classi\ufb01er.", "metadata": {"source": "data/24_tkaczyk_2015_cermine.pdf", "page_number": 11, "chunk": 0, "title": "CERMINE: automatic extraction of structured metadata from scientific literature"}}, "83": {"idx": 83, "content": "4.3 Metadata extraction The purpose of this phase is to analyse zones labelled as metadata and extract a rich set of document\u2019s metadata information, including: title, authors, af\ufb01liations, relations author\u2013af\ufb01liation, email addresses, relations author\u2013email, Table 3 The results of SVM parameters searching for initial classi\ufb01cation Initial classi\ufb01cation Kernel Linear 4th poly. RBF Sigmoid log2(C), log2(\u03b3 ) 7, 1 9, \u22125 5, \u22123 15, \u221213 Mean F1 (%) 90.7 93.5 93.9 90.1 The table shows the mean F score values for all kernel function types obtained during \ufb01vefold cross-validation, as well as related values of C and \u03b3 parameters Table 4 The results of SVM parameters searching for metadata classi\ufb01cation Metadata classi\ufb01cation Kernel Linear 4th poly. RBF Sigmoid log2(C), log2(\u03b3 ) 4, \u22129 7, \u22124 9, \u22123 11, \u22127 Mean F1 (%) 85.0 87.5 88.6 81.0 The table shows the mean F score values for all kernel function types obtained during \ufb01vefold cross-validation, as well as related values of C and \u03b3 parameters abstract, keywords, journal, volume, issue, pages range, year and DOI. The phase contains two steps: 1. Metadata zone classi\ufb01cation (B1)\u2014assigning speci\ufb01c metadata classes to metadata zones, described in detail in Sect. 4.2. 2. Metadata extraction (B2)\u2014extracting atomic information from labelled zones.", "metadata": {"source": "data/24_tkaczyk_2015_cermine.pdf", "page_number": 11, "chunk": 1, "title": "CERMINE: automatic extraction of structured metadata from scientific literature"}}, "84": {"idx": 84, "content": "During the last step (B2), a set of simple heuristic-based rules is used to perform the following operations: \u2013 zones labelled as abstract are concatenated, \u2013 as type is often speci\ufb01ed just above the title, it is removed from the title zone if needed (based on a dictionary of types), \u2013 authors, af\ufb01liations and keywords lists are split with the use of a list of separators, \u2013 af\ufb01liations are associated with authors based on indexes and distances, \u2013 email addresses are extracted from correspondence and af\ufb01liation zones using regular expressions, \u2013 email addresses are associated with authors based on author names, \u2013 pages ranges placed directly in bib_info zones are parsed using regular expressions, \u2013 ifthereisnopagesrangegivenexplicitlyinthedocument, we also try to retrieve it from the pages numbers on each page, \u2013 dates are parsed using regular expressions, \u2013 journal, volume, issue and DOI are extracted from bib_info zones based on regular expressions. 4.4 Bibliography extraction The goal of bibliography extraction is to extract a list of bibliographic references with their metadata (including author, title, source, volume, issue, pages and year) from zones labelled as references. Bibliography extraction path contains two steps: 1. Reference strings extraction (C1)\u2014dividing the content of references zones into individual reference strings. 2. Reference parsing (C2)\u2014extracting metadata from reference strings.", "metadata": {"source": "data/24_tkaczyk_2015_cermine.pdf", "page_number": 11, "chunk": 2, "title": "CERMINE: automatic extraction of structured metadata from scientific literature"}}, "85": {"idx": 85, "content": "4.4.1 Extracting reference strings References zones contain a list of reference strings, each of which can span over one or more text lines. The goal of reference strings extraction is to split the content of those zones 123 .", "metadata": {"source": "data/24_tkaczyk_2015_cermine.pdf", "page_number": 11, "chunk": 3, "title": "CERMINE: automatic extraction of structured metadata from scientific literature"}}, "86": {"idx": 86, "content": "CERMINE: automatic extraction of structured metadata from scienti\ufb01c literature 327 into individual reference strings. This step utilizes unsupervised machine learning techniques, which allows to omit time-consuming training set preparation and learning phases, while achieving very good extraction results. Every bibliographic reference is displayed in the PDF document as a sequence of one or more text lines. Each text line in a reference zone belongs to exactly one reference string, some of them are \ufb01rst lines of their reference, others are inner or last ones. The sequence of all text lines belonging to bibliography section can be represented by the following regular expression: ( <first line of a reference> ( <inner line of a reference>* <last line of a reference> )? )* In order to group text lines into consecutive references, \ufb01rst we determine which lines are \ufb01rst lines of their references. A set of such lines is presented in Fig. 7. To achieve this, we transform all lines to feature vectors and cluster them into two sets (\ufb01rst lines and all the rest). We make use of a simple observation that the \ufb01rst line from all references blocks is also the \ufb01rst line of its reference. Thus, the cluster containing this \ufb01rst line is assumed to contain all \ufb01rst lines. After recognizing all \ufb01rst lines, it is easy to concatenate lines to form consecutive reference strings. For clustering lines, we use KMeans algorithm with Euclidean distance metric.", "metadata": {"source": "data/24_tkaczyk_2015_cermine.pdf", "page_number": 12, "chunk": 0, "title": "CERMINE: automatic extraction of structured metadata from scientific literature"}}, "87": {"idx": 87, "content": "In this case K = 2, since the line set is clustered into two subsets. As initial centroids, we set the \ufb01rst line\u2019s feature vector and the vector with the largest distance to the \ufb01rst one. We use \ufb01ve features based on line relative length, line indentation, space between the line and the previous one, and the text content of the line (if the line starts with an enumeration pattern, if the previous line ends with a dot). 4.4.2 Reference strings parsing Reference strings extracted from references zones contain important reference metadata. In this step, metadata is extracted from reference strings and the result is the list of document\u2019sparsedbibliographicreferences.Theinformation we extract from the strings include: author, title, source, volume, issue, pages and year. An example of a parsed reference is shown in Fig. 8. First a reference string is tokenized. The tokens are then transformed into vectors of features and classi\ufb01ed by a supervised classi\ufb01er. Finally, the neighbouring tokens with the same label are concatenated, the labels are mapped into \ufb01nal metadata classes and the resulting reference metadata record is formed. The heart of the implementation is a classi\ufb01er that assigns labels to reference tokens.", "metadata": {"source": "data/24_tkaczyk_2015_cermine.pdf", "page_number": 12, "chunk": 1, "title": "CERMINE: automatic extraction of structured metadata from scientific literature"}}, "88": {"idx": 88, "content": "For better performance, the classi\ufb01er uses slightly more detailed labels than the target ones: \ufb01rst_name (author\u2019s \ufb01rst name or initial), surname (author\u2019s surname), title, source (journal or conference name), volume, issue, page_\ufb01rst (the lower bound of pages range), page_last (the upper bound of pages range), year and text (for separators and other tokens without a speci\ufb01c label). The token classi\ufb01er employs conditional random \ufb01elds and is built on top of GRMM and MALLET packages [35]. Fig. 7 A fragment of the references section of an article. Marked lines are the \ufb01rst lines of their references. After detecting these lines, the references section content can be easily split to form consecutive references strings Fig. 8 An example of a bibliographic reference with various metadata information highlighted using different colors, and these are in order: author, title, journal, volume, issue, pages and year (color \ufb01gure online) 123 .", "metadata": {"source": "data/24_tkaczyk_2015_cermine.pdf", "page_number": 12, "chunk": 2, "title": "CERMINE: automatic extraction of structured metadata from scientific literature"}}, "89": {"idx": 89, "content": "328 D. Tkaczyk et al. CRF classi\ufb01ers are a state-of-the-art technique for citation parsing. They achieve very good results for classifying instances that form a sequence, especially when the label of one instance depends on the labels of previous instances. The basic features are the tokens themselves. We use 42 additional features to describe the tokens: \u2013 Some of them are based on the presence of a particular character class, e.g. digits or lowercase/uppercase letters. \u2013 Others check whether the token is a particular character (e.g. a dot, a square bracket, a comma or a dash), or a particular word. \u2013 Finally, we use features checking if the token is contained by the dictionary built from the dataset, e.g. a dictionary of cities or words commonly appearing in the journal title. It is worth to notice that the token\u2019s label depends not only on its feature vector, but also on the features of the surrounding tokens. To re\ufb02ect this in the classi\ufb01er, the token\u2019s feature vector contains not only features of the token itself, but also features of two preceding and two following tokens. Aftertokenclassi\ufb01cation,fragmentslabelledas\ufb01rst_name and surname are joined together based on their order to form consecutive author names, and similarly fragments labelled as page_\ufb01rst and page_last are joined together to form pages range. Additionally, in the case of title or source labels, the neighbouring tokens with the same label are concatenated.", "metadata": {"source": "data/24_tkaczyk_2015_cermine.pdf", "page_number": 13, "chunk": 0, "title": "CERMINE: automatic extraction of structured metadata from scientific literature"}}, "90": {"idx": 90, "content": "Theresultofbibliographyextractionisalistofdocument\u2019s bibliographic references in a structured form, each of which contains the raw text as well as additional metadata. 5 Evaluation We performed the evaluation of the key steps of the algorithm and the entire extraction process as well. The ground truth data used for the evaluation is based mainly on the resources of PubMed Central Open Access Subset [1]. Evaluated steps include: page segmentation (Sect. 5.2), initial and metadata zone classi\ufb01cation (Sect. 5.3) and reference parsing (Sect. 5.4). Other steps were not directly evaluated, mainly due to the fact that creating ground truth datasets for them would be dif\ufb01cult and time-consuming. Since all the steps affect the \ufb01nal extraction result, they were all evaluated indirectly by the assessment of the performance of the entire CERMINE system (Sect. 5.5) and the comparison with similar tools as well (Sect. 5.6). 5.1 Datasets preparation Table 5 provides details about all the datasets used for the experiments. In general, we use three types of data. Subsets of PubMed Central were used directly to evaluate the entire extraction work\ufb02ow (metadata test set) and compare the performance of CERMINE with similar systems (comparison test set). Additionally, PMC resources served as a base for constructing GROTOAP and GROTOAP2 datasets, which were used for the experiments related to page segmentation (segmentation test set) and zone classi\ufb01cation (zone validation set and zone test set).", "metadata": {"source": "data/24_tkaczyk_2015_cermine.pdf", "page_number": 13, "chunk": 1, "title": "CERMINE: automatic extraction of structured metadata from scientific literature"}}, "91": {"idx": 91, "content": "A set used for citation parser evaluation was build using PMC and also CiteSeer [36] and Cora-ref [37] (citation test set). PubMed Central Open Access Subset [1] contains life sciences publications in PDF format, and their corresponding metadata in the form of NLM JATS \ufb01les. NLM \ufb01les contain a rich set of document\u2019s metadata (title, authors, af\ufb01liations, abstract, journal name, etc.), full text (sections, section titles, paragraphs, tables, equations) and also document\u2019s bibliography. Subsets of PMC were used to: (1) evaluate the entire metadata and references extraction work\ufb02ow (metadata test set) and (2) compare the system performance with other tools (comparison test set). Unfortunately, the quality of data in ground truth NLM JATS \ufb01les varies from perfectly labelled documents to documents containing no valuable information at all. In some cases, NLM \ufb01les lack the entire sections of the document (usually the bibliography and/or the body). Such \ufb01les were \ufb01ltered out in both sets, and for evaluation we used only documents, whose metadata \ufb01les contained all three important sections: front matter, body and bibliography. What is more, ground truth \ufb01les from PMC contain only the annotated text of the document and do not preserve geometric features related to the way the text is displayed in PDF \ufb01les. As a result, PMC could not be directly used for training and evaluation of the individual steps, such as page segmentation and zone classi\ufb01cation.", "metadata": {"source": "data/24_tkaczyk_2015_cermine.pdf", "page_number": 13, "chunk": 2, "title": "CERMINE: automatic extraction of structured metadata from scientific literature"}}, "92": {"idx": 92, "content": "For these tasks, we built GROTOAP [38] and GROTOAP2 [4] datasets. GROTOAP is a dataset of 113 documents in TrueViz format preserving not only the text content, but also geometric features of the text and zone labels. GROTOAP was built semi-automatically from PMC resources. First PDF documents were processed by automatic tools in order to extract the geometric structure along with zone labels, and the results were corrected manually by human experts. Since the task of correcting the geometric structure and zone labelling of the entire document is time-consuming, we were able to produce only a small set of documents. GROTOAP was used to evaluate page segmentation (segmentation test set). GROTOAP2 is a successor of GROTOAP. GROTOAP2 is a much larger and diverse dataset, also containing information related to the document\u2019s text, geometric features and zone labels. The label set in GROTOAP2 is a union of all labels used in both zone classi\ufb01ers. GROTOAP2 was created semi-automatically using PMC resources (Fig. 9). Our goal was to create a fairly large dataset, useful for machine learning algorithms. Unfortu123 .", "metadata": {"source": "data/24_tkaczyk_2015_cermine.pdf", "page_number": 13, "chunk": 3, "title": "CERMINE: automatic extraction of structured metadata from scientific literature"}}, "93": {"idx": 93, "content": "CERMINE: automatic extraction of structured metadata from scienti\ufb01c literature 329 Table 5 The summary of all the datasets used in the experiments Name Source Format Content Purpose Segmentation test set GROTOAP TrueViz 113 documents The evaluation of page segmentation (Sect. 5.2) Zone validation set GROTOAP2 TrueViz 100 documents containing 14,000 labelled zones, 2743 of which are metadata zones Zone classi\ufb01ers feature selection (Sect. 4.2.1) and SVM parameters determination (Sect. 4.2.2) Zone test set GROTOAP2 TrueViz 2551 documents containing 355,779 zones, 68,557 of which are metadata zones Zone classi\ufb01ers evaluation (Sect. 5.3) and \ufb01nal classi\ufb01ers training Citation test set CiteSeer, Cora-ref and PMC NLM JATS 4000 parsed citations (2000 from CiteSeer and Cora-ref, 2000 from 1991 different PMC documents) The evaluation of the references parser (Sect. 5.4) Metadata test set PubMed Central PDF + NLM JATS 47,983 PDF documents with corresponding metadata records The evaluation of the entire metadata and bibliography extraction work\ufb02ow (Sect. 5.5) Comparison test set PubMed Central PDF + NLM JATS 1943 PDF documents with corresponding metadata records The comparison of CERMINE\u2019s performance with the performance of other similar tools (Sect. 5.6) nately, an approach used for GROTOAP would not allow to create a large dataset, due to the manual correction of every document.", "metadata": {"source": "data/24_tkaczyk_2015_cermine.pdf", "page_number": 14, "chunk": 0, "title": "CERMINE: automatic extraction of structured metadata from scientific literature"}}, "94": {"idx": 94, "content": "Instead, we decided to make use of the text labelling already present in the PMC\u2019s NLM JATS \ufb01les to assign labels to zones automatically, while the zones themselves were constructed using CERMINE tools. More precisely, GROTOAP2 was created with the following steps: 1. First, PDF \ufb01les from PMC were processed automatically by CERMINE in order to extract the hierarchical geometric structure and the reading order. 2. The text content of every zone was then compared to labelled text from NLM \ufb01les with the use of Smith\u2013 Waterman sequence alignment algorithm [39]. This allowed to assign labels to zones. 3. Files with a lot of zones labelled as \u201cunknown\u201d, that is zones, for which the labelling process was unable to \ufb01nd a concrete label, were \ufb01ltered out. 4. A small sample of the remaining \ufb01les was inspected manually. This resulted in identifying a number of repeated problems and errors in the dataset. 5. Based on the results of the analysis, we developed a set of heuristic-based rules and applied them to the dataset in order to increase the labelling accuracy. More details about GROTOAP2 dataset and its creation process can be found in [4]. Since GROTOAP\u2019s creation process did not contain manual correction of every document, the dataset contains errors, caused by both segmentation and labelling steps. Segmentation errors were comparatively rare. According to the evaluation we performed on a random sample of 50 documents, the accuracy of zone labelling is 93%. Despite this Fig.", "metadata": {"source": "data/24_tkaczyk_2015_cermine.pdf", "page_number": 14, "chunk": 1, "title": "CERMINE: automatic extraction of structured metadata from scientific literature"}}, "95": {"idx": 95, "content": "9 Semi-automatic method of creating GROTOAP2 dataset. First automatic tools extracted the hierarchical geometric structure and the reading order of a document. Next, we automatically assigned labels to zones by matching their text to labelled fragments from NLM \ufb01les. Finally, additional rules were developed manually and applied to the dataset in order to increase the labelling accuracy. It should be noted that since CERMINE was not involved in the process of assigning labels, the dataset can be used to evaluate the performance of zone classi\ufb01cation drawback, the lack of manual correction of every document guaranteed the scalability of the method, which allowed to create much larger dataset than in the case of more traditional approaches. Since CERMINE was not involved in the process of assigning labels, subsets of GROTOAP2 could be used for the experiments with zone classi\ufb01cation: feature selection and SVM parameters adjustment (zone validation set), and \ufb01nal zone classi\ufb01ers evaluation and training (zone test set). 123 .", "metadata": {"source": "data/24_tkaczyk_2015_cermine.pdf", "page_number": 14, "chunk": 2, "title": "CERMINE: automatic extraction of structured metadata from scientific literature"}}, "96": {"idx": 96, "content": "330 D. Tkaczyk et al. For reference parser evaluation, we used CiteSeer [36], Cora-ref [37] and PubMed Central resources combined together into a single set (citation test set). CiteSeer and Cora-ref already contain parsed references. Unfortunately, due to some differences in the labels used, labels mapping had to be performed. Labels from original datasets were mapped in the following way: title and year remained the same; journal, booktitle, tech and type were mapped to source; date was mapped to year. Labels author and pages were split, respectively, into givenname and surname, page_\ufb01rst and page_last using regular expressions. All remaining tokens were labelled as text. NLM \ufb01les from PMC also contain parsed references. Unfortunately, in most cases, they do not preserve the entire reference strings from the original PDF \ufb01le, and separators and punctuation are often omitted. For this reason, the reference set was built using a similar technique as in the case of GROTOAP2. We extracted reference strings from PDF \ufb01les using CERMINE tools and labelled them using annotated data from NLM \ufb01les. 5.2 Page segmentation Page segmenter was evaluated using the entire GROTOAP dataset. For each structure type (zone, line, word), we calculated the overall accuracy over all documents that is the percentage of elements correctly constructed by the algorithm. An item is considered constructed correctly if it contains exactly the same set of characters as the original element.", "metadata": {"source": "data/24_tkaczyk_2015_cermine.pdf", "page_number": 15, "chunk": 0, "title": "CERMINE: automatic extraction of structured metadata from scientific literature"}}, "97": {"idx": 97, "content": "Since in our ground truth dataset every table and \ufb01gure is placed in one zone, and Docstrum usually divides these (often sparse) areas into more zones, these regions were excluded from the evaluation. We performed the evaluation of two versions of the segmentation algorithm: the original Docstrum and the algorithm with the modi\ufb01cations listed in Sect. 4.1.2. The results are shown in Fig. 10. For all structure types, the modi\ufb01cations resulted in increased extraction accuracy. 5.3 Zone classi\ufb01cation Both zone classi\ufb01ers were evaluated by a \ufb01vefold crossvalidation using zone test set (described in Sect. 5.1). The Tables 6 and 7 show the confusion matrices as well as precision and recall values for individual classes for initial and metadata classi\ufb01cation, respectively. For a class C, precision and recall were calculated in the following way: PrecisionC = |SC| |CC|, RecallC = |SC| |GC| where SC is a set of zones correctly recognized as C by the classi\ufb01er, CC is a set of zones labelled as C by the classi\ufb01er 0 20 40 60 80 100 zones lines words accuracy Docstrum original modified Fig. 10 The results of page segmentation evaluation. The plot shows the accuracy of extracting zones, lines and words for the original Docstrum algorithm and Docstrum with modi\ufb01cations proposed in Sect. 4.1.2 and GC is a set of zones labelled as C in the ground truth data.", "metadata": {"source": "data/24_tkaczyk_2015_cermine.pdf", "page_number": 15, "chunk": 1, "title": "CERMINE: automatic extraction of structured metadata from scientific literature"}}, "98": {"idx": 98, "content": "Initial classi\ufb01er achieved the following results calculated as mean values for individual classes: precision 97.2%, recall 95.4%, F score 96.3%. The results achieved by metadata classi\ufb01er were as follows: precision 95.4%, recall 95.1%, F score 95.3%. We also compared the performance of the classi\ufb01cation obtained from our two classi\ufb01ers executed in sequence with one combined classi\ufb01er, which assigns both general categories and speci\ufb01c metadata classes (more details about the two approaches and the decision to use two classi\ufb01cation steps instead of one can be found in Sect. 4.2). The combined classi\ufb01er achieved 95.1% accuracy and 85.2% mean F score, while two separate classi\ufb01ers working together (the current solution) achieved 95.3% accuracy and 85.9% F score. The performance of these approaches is thus very similar to each other. 5.4 Reference parsing Bibliographic reference parser was evaluated with the use of a \ufb01vefold cross-validation on the citation test set (described in Sect. 5.1). For every metadata class, we computed precision and recall in a similar way as in the case of zone classi\ufb01cation. This time the objects in SC, CC and GC sets were not individual tokens, but entire reference substrings. As a consequence, a token correctly labelled with a class C contributes to the overall success rate only if the entire token sequence of class C containing the given token is correctly labelled. 123 .", "metadata": {"source": "data/24_tkaczyk_2015_cermine.pdf", "page_number": 15, "chunk": 2, "title": "CERMINE: automatic extraction of structured metadata from scientific literature"}}, "99": {"idx": 99, "content": "CERMINE: automatic extraction of structured metadata from scienti\ufb01c literature 331 Table 6 Confusion matrix for initial classi\ufb01cation for \ufb01vefold cross-validation Metadata Body References Other Precision (%) Recall (%) Metadata 66,042 2181 75 259 96.6 96.3 Body 1551 232,464 177 934 97.9 98.9 References 47 806 17,489 67 98.2 95.0 Other 733 2118 65 30,771 96.1 91.3 Rows and columns represent the desired and obtained classi\ufb01cation result, respectively Bold values on the main matrix diagonal are the numbers of correctly classi\ufb01ed zones of respective classes Figure 11 shows precision and recall values for individual metadata classes. The parser achieved the following scores calculated as mean values for individual classes: precision 92.9%, recall 93.8%, F score 93.3%. 5.5 Metadata extraction evaluation The evaluation of the entire work\ufb02ow was performed with the use of metadata test set (described in Sect. 5.1). The PDF \ufb01les were processed by CERMINE and the resulting metadata (the \u201ctested\u201d documents) was compared to metadata stored in NLM \ufb01les (the \u201cground truth\u201d documents). For each type of metadata, we used different measures of correctness. In general, we deal with two types of metadata \ufb01elds: those that appear at most once per document (these are: title, abstract, journal, volume, issue, pages range, year and DOI) and those present as lists (authors, af\ufb01liations, email addresses, keywords and bibliographic references).", "metadata": {"source": "data/24_tkaczyk_2015_cermine.pdf", "page_number": 16, "chunk": 0, "title": "CERMINE: automatic extraction of structured metadata from scientific literature"}}, "100": {"idx": 100, "content": "In the \ufb01rst case, for every document, a single string from NLM \ufb01le was compared to the extracted string, which gives a binary output: information extracted correctly or not. The overall precision and recall scores for a metadata class C are calculated in the following way: PrecisionC = |SC| |CC|, RecallC = |SC| |GC| where SC is a set of documents from which the non-empty information of a class C was correctly extracted, CC is a set of tested documents with non-empty \ufb01eld of class C, and \ufb01nally GC is a set of ground truth documents with non-empty \ufb01eld of class C. Some information types from this group, such as article\u2019s volume, issue, DOI, dates and pages, were considered correct only if exactly equal to NLM data. As the journal name is often abbreviated, we marked it as correct if it was a subsequence of the ground truth journal name. Article\u2019s title and abstract were tokenized and compared with the use of Smith\u2013Waterman sequence alignment algorithm [39]. In the case of list metadata types, for every document the elements of tested and ground truth lists were compared using cosine distance. This resulted in individual precision and recall for every document. The overall precision and recall were computed as mean values over all documents. In the case of bibliographic references, only their full text was compared, and the detailed metadata was ignored. The evaluation results are shown in Fig. 12.", "metadata": {"source": "data/24_tkaczyk_2015_cermine.pdf", "page_number": 16, "chunk": 1, "title": "CERMINE: automatic extraction of structured metadata from scientific literature"}}, "101": {"idx": 101, "content": "CERMINE achieved the following results calculated as mean values for individual metadata classes: precision 81.0%, recall 74.7%, F score 77.5%. 5.6 Comparison evaluation Comparison test set (described in Sect. 5.1) was used to compare the performance of CERMINE with similar extraction systems. The results are shown in Table 8. The evaluation methodology was the same as before, with the exception of ParsCit system. Since ParsCit analyses only the text content of a document, PDF \ufb01les were \ufb01rst transformed to text using pdftotext tool. What is more, the output of ParsCit can contain multiple titles or abstracts; thus, for this system, all metadata classes were treated as list types. For most metadata classes, CERMINE performs the best. The worst values were obtained in the case of ParsCit system, which was probably caused by the fact that the algorithm inspects only the text content of a documents, ignoring hints related to the way the text is displayed in the PDF \ufb01le. 5.7 Error analysis The errors made by the extraction work\ufb02ow can be divided into two groups: metadata was not extracted or the extracted information is incorrect. The majority of errors happen in the following situations: \u2013 When two (or more) zones with different roles in the document are placed close to each other, they are often merged together by the segmenter. In this case, the classi\ufb01cation is more dif\ufb01cult and by design only one label is assigned to such a hybrid zone.", "metadata": {"source": "data/24_tkaczyk_2015_cermine.pdf", "page_number": 16, "chunk": 2, "title": "CERMINE: automatic extraction of structured metadata from scientific literature"}}, "102": {"idx": 102, "content": "A potential solution would be to introduce additional labels for pairs of labels that often appear close to each other, for example title_author or author_af\ufb01liation, and split the content of such zones later in the work\ufb02ow. \u2013 The segmenter introduces other errors as well, such as incorrectly attaching an upper index to the line above the current line, or merging text written in two columns. These 123 .", "metadata": {"source": "data/24_tkaczyk_2015_cermine.pdf", "page_number": 16, "chunk": 3, "title": "CERMINE: automatic extraction of structured metadata from scientific literature"}}, "103": {"idx": 103, "content": "332 D. Tkaczyk et al. Table 7 Confusion matrix for metadata classi\ufb01cation for \ufb01vefold cross-validation Abstract Af\ufb01liation Author Bib_info Correspondence Dates Editor Keywords Title Type Copyright Precision (%) Recall (%) Abstract 6866 8 7 62 8 1 1 23 7 5 10 97.7 98.1 Af\ufb01liation 11 3532 16 31 62 5 8 3 1 6 6 95.5 96.0 Author 4 14 2684 42 18 0 3 1 6 6 4 96.9 96.5 Bib_info 75 22 14 40, 982 25 119 1 41 16 115 100 98.7 98.9 Corresp. 9 107 15 32 1616 2 0 3 1 1 3 92.8 90.3 Dates 5 1 4 136 3 2835 0 1 0 2 13 94.7 94.5 Editor 0 2 1 0 0 0 473 0 0 0 0 96.9 99.4 Keywords 28 8 5 86 1 6 1 896 5 7 1 91.5 85.8 Title 9 0 13 26 0 0 0 3 2574 6 2 98.3 97.8 Type 4 0 4 88 0 2 1 6 6 1497 2 91.0 93.0 Copyright 14 5 7 45 8 23 0 2 3 0 2927 95.4 96.5 Rows and columns represent the desired and obtained classi\ufb01cation result, respectively Bold values on the main matrix diagonal are the numbers of correctly classi\ufb01ed zones of respective classes Fig. 11 Bibliographic reference parser evaluation. The \ufb01gure shows precision and recall values for extracting reference fragments belonging to individual metadata classes. A given fragment is considered correctly extracted, if it is identical to the ground truth data Fig. 12 The evaluation results of CERMINE\u2019s extraction process on metadata test set. The \ufb01gure shows precision and recall values for individual metadata classes errors can be corrected by further improvement of the page segmenter.", "metadata": {"source": "data/24_tkaczyk_2015_cermine.pdf", "page_number": 17, "chunk": 0, "title": "CERMINE: automatic extraction of structured metadata from scientific literature"}}, "104": {"idx": 104, "content": "\u2013 Zone classi\ufb01cation errors are also responsible for a lot of extraction errors. These errors can be improved by adding training instances to the training set and improving the labelling accuracy in GROTOAP2. \u2013 Sometimes the metadata, usually keywords, volume, issue or pages, is not explicitly given in the input PDF \ufb01le. Since CERMINE analyses the PDF \ufb01le only, such information cannot be extracted. This is in fact not an extraction error. Unfortunately, since ground truth NLM data in PMC usually contains such information, whether it is written in the PDF or not, these situations also contribute to the overall error rates (equally for all evaluated systems). 123 .", "metadata": {"source": "data/24_tkaczyk_2015_cermine.pdf", "page_number": 17, "chunk": 1, "title": "CERMINE: automatic extraction of structured metadata from scientific literature"}}, "105": {"idx": 105, "content": "CERMINE: automatic extraction of structured metadata from scienti\ufb01c literature 333 Table 8 The results of comparing the performance of various metadata extraction systems CERMINE PDFX GROBID ParsCit Pdf-extract Title 95.5 85.7 82.5 34.1 49.4 93.4 84.7 77.4 39.6 49.4 94.5 85.2 79.8 36.6 49.4 Authors 90.2 71.2 85.9 57.9 \u2013 89.0 71.5 90.5 48.6 \u2013 89.6 71.3 88.1 52.8 \u2013 Af\ufb01liations 88.2 \u2013 90.8 72.2 \u2013 83.1 \u2013 51.8 44.3 \u2013 85.6 \u2013 66.0 54.9 \u2013 Email addresses 51.7 53.0 26.9 28.8 \u2013 42.6 73.6 7.8 36.2 \u2013 46.7 61.6 12.1 32.1 \u2013 Abstract 82.8 71.1 70.4 47.7 \u2013 79.9 66.7 67.7 61.3 \u2013 81.3 68.8 69.0 53.7 \u2013 Keywords 89.9 \u2013 94.2 15.6 \u2013 63.5 \u2013 44.2 3.0 \u2013 74.4 \u2013 60.2 5.1 \u2013 Journal 80.3 \u2013 \u2013 \u2013 \u2013 73.2 \u2013 \u2013 \u2013 \u2013 76.6 \u2013 \u2013 \u2013 \u2013 Volume 93.3 \u2013 \u2013 \u2013 \u2013 83.0 \u2013 \u2013 \u2013 \u2013 87.8 \u2013 \u2013 \u2013 \u2013 Issue 53.7 \u2013 \u2013 \u2013 \u2013 28.4 \u2013 \u2013 \u2013 \u2013 37.1 \u2013 \u2013 \u2013 \u2013 Pages 87.0 \u2013 \u2013 \u2013 \u2013 80.4 \u2013 \u2013 \u2013 \u2013 83.5 \u2013 \u2013 \u2013 \u2013 Year 96.3 \u2013 95.7 \u2013 \u2013 95.0 \u2013 40.4 \u2013 \u2013 95.6 \u2013 56.8 \u2013 \u2013 DOI 98.2 \u2013 99.1 \u2013 \u2013 75.0 \u2013 65.4 \u2013 \u2013 85.1 \u2013 78.8 \u2013 \u2013 References 96.1 91.3 79.7 81.2 80.4 89.8 88.9 66.7 71.8 57.5 92.8 90.1 72.6 76.2 67.0 In every cell, the precision, recall and F score values are shown. The best results in every category are bolded The most common extraction errors include: \u2013 Title merged with other parts of the document, when title zone is placed close to another region. \u2013 Title not recognized, for example when it appears on the second page of the PDF \ufb01le. \u2013 Title zone split by the segmenter into a few zones, and only a subset of them is correctly classi\ufb01ed.", "metadata": {"source": "data/24_tkaczyk_2015_cermine.pdf", "page_number": 18, "chunk": 0, "title": "CERMINE: automatic extraction of structured metadata from scientific literature"}}, "106": {"idx": 106, "content": "\u2013 Authors zone not labelled, in that case the authors are missing. \u2013 Authors zone merged with other fragments, such as af\ufb01liations or research group name, in such cases additional fragments appear in the authors list. 123 .", "metadata": {"source": "data/24_tkaczyk_2015_cermine.pdf", "page_number": 18, "chunk": 1, "title": "CERMINE: automatic extraction of structured metadata from scientific literature"}}, "107": {"idx": 107, "content": "334 D. Tkaczyk et al. 0 20 40 0 10 20 30 40 Number of pages Time [s] Fig. 13 The plot shows CERMINE\u2019s processing time (in seconds) as a function of the number of pages of a document for a subset of 1238 documents from PMC \u2013 Af\ufb01liation zone not properly recognized by the classi\ufb01er, for example when it not visually separated from other zones, or placed at the end of the document. Af\ufb01liations are missing in that case. \u2013 The entire abstract or a part of it recognized as body by the classi\ufb01er, as a result the abstract or a part of it is missing. \u2013 The\ufb01rstbodyparagraphrecognizedincorrectlyasabstract, as a result the extracted abstract contains a fragment of the document\u2019s proper text. \u2013 Bibliographic information missing from a PDF \ufb01le or not recognized by the classi\ufb01ers, as a result journal name, volume, issue and/or pages range are not extracted. \u2013 Keywords missing because the zone was not recognized or not included in the PDF \ufb01le. \u2013 A few of the references zones classi\ufb01ed as body, and in such cases some or all of the references are missing. 5.8 Processing time The processing time of a document depends mainly on its number of pages. The most time-consuming steps are page segmentation and initial zone classi\ufb01cation. Figure 13 shows the processing time as a function of the number of document\u2019s pages for 1238 random documents. The average processing time for this subset was 9.4s.", "metadata": {"source": "data/24_tkaczyk_2015_cermine.pdf", "page_number": 19, "chunk": 0, "title": "CERMINE: automatic extraction of structured metadata from scientific literature"}}, "108": {"idx": 108, "content": "6 Conclusions and future work The article presents CERMINE\u2014a system for extracting both metadata and bibliography from scienti\ufb01c articles in a born-digital form. CERMINE is very useful for digital librariesandsimilarenvironmentswhenevertheyhavetodeal with documents with metadata information missing, fragmentary or not reliable. Automatic extraction tools provided by CERMINE support a number of tasks such as intelligent searching, \ufb01nding similar and related documents, building citation and author networks, and so on. The system is open source and available online at http:// cermine.ceon.pl. The modular architecture and the use of supervised and unsupervised machine learning techniques make CERMINE \ufb02exible and easy to adapt to new document layouts. The evaluation against a large and diverse dataset shows good results for the key individual steps and the entire extraction work\ufb02ow. For most metadata types, the results are better than in the case of other similar extraction systems. Our future plans include: \u2013 extending the work\ufb02ow, so that the system is able to process documents in the form of scanned pages as well, \u2013 expanding the work\ufb02ow architecture by adding a process path for extracting structured full text containing sections and subsections, headers and paragraphs, \u2013 adding af\ufb01liation parsing step, the goal of which is to extract af\ufb01liation metadata: institution name, address and country, \u2013 making the citation dataset used for parser evaluation publicly available.", "metadata": {"source": "data/24_tkaczyk_2015_cermine.pdf", "page_number": 19, "chunk": 1, "title": "CERMINE: automatic extraction of structured metadata from scientific literature"}}, "109": {"idx": 109, "content": "Acknowledgments This work has been supported by the European Commission as part of the FP7 OpenAIREplus (Grant No. 283595) and OCEAN projects. Open Access This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (http://creativecomm ons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. References 1. PubMed. http://www.ncbi.nlm.nih.gov/pubmed 2. CERMINE. http://cermine.ceon.pl 3. Tkaczyk, D., Szostek, P., Dendek, P.J., Fedoryszak, M., Bolikowski, L.: CERMINE\u2014automatic extraction of metadata and references from scienti\ufb01c literature. In: 11th IAPR International Workshop on Document Analysis Systems, pp. 217\u2013221 (2014) 4. Tkaczyk, D., Szostek, P., Bolikowski, L.: GROTOAP2\u2014the methodology of creating a large ground truth dataset of scienti\ufb01c articles. D-Lib Magazine (2014) 5. Giuffrida, G., Shek, E.C., Yang, J.: Knowledge-based metadata extraction from postscript \ufb01les. In: ACM DL, pp. 77\u201384 (2000) 6. Constantin, A., Pettifer, S., Voronkov, A.: PDFX: fully-automated pdf-to-xml conversion of scienti\ufb01c literature. In: ACM Symposium on Document Engineering, pp. 177\u2013180 (2013) 7. Pdf-extract. http://labs.crossref.org/pdfextract/ 8.", "metadata": {"source": "data/24_tkaczyk_2015_cermine.pdf", "page_number": 19, "chunk": 2, "title": "CERMINE: automatic extraction of structured metadata from scientific literature"}}, "110": {"idx": 110, "content": "Han, H., Giles, C.L., Manavoglu, E., Zha, H., Zhang, Z., Fox, E.A.: Automatic document metadata extraction using support vector machines. In: ACM/IEEE 2003 Joint Conference on Digital Libraries, pp. 37\u201348 (2003) 123 .", "metadata": {"source": "data/24_tkaczyk_2015_cermine.pdf", "page_number": 19, "chunk": 3, "title": "CERMINE: automatic extraction of structured metadata from scientific literature"}}, "111": {"idx": 111, "content": "CERMINE: automatic extraction of structured metadata from scienti\ufb01c literature 335 9. Kovacevic, A., Ivanovic, D., Milosavljevic, B., Konjovic, Z., Surla, D.: Automatic extraction of metadata from scienti\ufb01c publications for CRIS systems. Program 45(4), 376\u2013396 (2011) 10. Lu, X., Kahle, B., Wang, J.Z., Giles, C.L.: A metadata generation system for scanned scienti\ufb01c volumes. In: ACM/IEEE Joint Conference on Digital Libraries, JCDL 2008, Pittsburgh, PA, USA, 16\u201320 June 2008, pp. 167\u2013176 (2008) 11. Marinai,S.:MetadataextractionfromPDFpapersfordigitallibrary ingest. In: 10th International Conference on Document Analysis and Recognition, pp. 251\u2013255 (2009) 12. Cui, B., Chen, X.: An improved hidden Markov model for literature metadata extraction. In: Advanced Intelligent Computing Theories and Applications, 6th International Conference on Intelligent Computing, pp. 205\u2013212 (2010) 13. Kern, R., Jack, K., Hristakeva, M., Granitzer, M.: Teambeam\u2014 meta-data extraction from scienti\ufb01c literature. D Lib Mag. 18(7/8), 1 (2012) 14. Lopez, P.: GROBID: combining automatic bibliographic data recognition and term extraction for scholarship publications. In: Research and Advanced Technology for Digital Libraries, 13th European Conference, pp. 473\u2013474 (2009) 15. Luong, M., Nguyen, T.D., Kan, M.: Logical structure recovery in scholarly articles with rich document features. IJDLS 1(4), 1\u201323 (2010) 16. Councill, I.G., Giles, C.L., Kan, M.: Parscit: an open-source CRF reference string parsing package.", "metadata": {"source": "data/24_tkaczyk_2015_cermine.pdf", "page_number": 20, "chunk": 0, "title": "CERMINE: automatic extraction of structured metadata from scientific literature"}}, "112": {"idx": 112, "content": "In: Proceedings of the International Conference on Language Resources and Evaluation, LREC 2008, 26 May\u20131 June 2008, Marrakech, Morocco (2008) 17. Gao, L., Tang, Z., Lin, X.: CEBBIP: a parser of bibliographic information in chinese electronic books. In: Proceedings of the 2009 Joint International Conference on Digital Libraries, pp. 73\u2013 76 (2009) 18. Zou, J., Le, D.X., Thoma, G.R.: Locating and parsing bibliographic references in HTML medical articles. IJDAR 13(2), 107\u2013119 (2010) 19. Day, M., Tsai, R.T., Sung, C., Hsieh, C., Lee, C., Wu, S., Wu, K., Ong, C., Hsu, W.: Reference metadata extraction using a hierarchical knowledge representation framework. Decis. Support Syst. 43(1), 152\u2013167 (2007) 20. Vilarinho,E.C.C.,daSilva,A.S.,Gon\u00e7alves,M.A.,deS\u00e1Mesquita, F., de Moura, E.S.: FLUX-CIM: \ufb02exible unsupervised extraction of citation metadata. In: ACM/IEEE Joint Conference on Digital Libraries, pp. 215\u2013224 (2007) 21. Zhang, Q., Cao, Y., Yu, H.: Parsing citations in biomedical articles using conditional random \ufb01elds. Comput. Biol. Med. 41(4), 190\u2013 194 (2011) 22. Zhang, X., Zou, J., Le, D.X., Thoma, G.R.: A structural SVM approach for reference parsing. BMC Bioinform. 12(S\u20133), S7 (2011) 23. Hetzner, E.: A simple method for citation metadata extraction using hidden markov models. In: ACM/IEEE Joint Conference on Digital Libraries, pp. 280\u2013284 (2008) 24. PDFX. http://pdfx.cs.man.ac.uk/ 25. Grobid. https://github.com/grobid/grobid 26. ParsCit. http://aye.comp.nus.edu.sg/parsCit/ 27.", "metadata": {"source": "data/24_tkaczyk_2015_cermine.pdf", "page_number": 20, "chunk": 1, "title": "CERMINE: automatic extraction of structured metadata from scientific literature"}}, "113": {"idx": 113, "content": "Lee, C.H., Kanungo, T.: The architecture of trueviz: a groundtruth/metadata editing and visualizing toolkit. Pattern Recognit. 36(3), 811\u2013825 (2003) 28. NLM, JATS. http://dtd.nlm.nih.gov/archiving/ 29. NISO Z39.96-2012. http://www.niso.org/apps/group_public/ document.php?document_id=10591 30. iText. http://itextpdf.com/ 31. O\u2019Gorman, L.: The document spectrum for page layout analysis. IEEE Trans. Pattern Anal. Mach. Intell. 15(11), 1162\u20131173 (1993) 32. PdfMiner. http://www.unixuser.org/euske/python/pdfminer/ 33. Chang, C., Lin, C.: LIBSVM: a library for support vector machines. ACM TIST 2(3), 27 (2011) 34. Goodman, L.A., Kruskal, W.H.: Measures of association for cross classi\ufb01cations iii: approximate sampling theory. J. Am. Stat. Assoc. 58, 310\u2013364 (1963) 35. McCallum, A.K.: MALLET: a machine learning for language toolkit (2002) 36. Giles, C.L., Bollacker, K.D., Lawrence, S.: Citeseer: an automatic citation indexing system. In: Proceedings of the 3rd ACM International Conference on Digital Libraries, pp. 89\u201398 (1998) 37. McCallum, A., Nigam, K., Rennie, J.: Automating the construction of internet portals with machine learning. Inf. Retr. 3, 127\u2013163 (2000) 38. Tkaczyk, D., Czeczko, A., Rusek, K., Bolikowski, L., Bogacewicz, R.: GROTOAP: ground truth for open access publications. In: Proceedings of the 12th ACM/IEEE-CS Joint Conference on Digital Libraries, pp. 381\u2013382 (2012) 39. Smith, T., Waterman, M.: Identi\ufb01cation of common molecular subsequences. J. Mol. Biol.", "metadata": {"source": "data/24_tkaczyk_2015_cermine.pdf", "page_number": 20, "chunk": 2, "title": "CERMINE: automatic extraction of structured metadata from scientific literature"}}, "114": {"idx": 114, "content": "147(1), 195\u2013197 (1981) 123 View publication stats .", "metadata": {"source": "data/24_tkaczyk_2015_cermine.pdf", "page_number": 20, "chunk": 3, "title": "CERMINE: automatic extraction of structured metadata from scientific literature"}}, "115": {"idx": 115, "content": "Machine Learning vs. Rules and Out-of-the-Box vs. Retrained: An  Evaluation of Open-Source Bibliographic Reference and Citation  Parsers  Dominika Tkaczyk  ADAPT Centre, School of Computer  Science and Statistics  Trinity College Dublin, Ireland  Dominika.Tkaczyk@adaptcentre.ie  Andrew Collins  ADAPT Centre, School of Computer  Science and Statistics  Trinity College Dublin, Ireland  Andrew.Collins@adaptcentre.ie  Paraic Sheridan  ADAPT Centre, School of Computer  Science and Statistics  Trinity College Dublin, Ireland  Paraic.Sheridan@adaptcentre.ie    Joeran Beel  ADAPT Centre, School of Computer  Science and Statistics  Trinity College Dublin, Ireland  Joeran.Beel@adaptcentre.ie    ABSTRACT  Bibliographic reference parsing refers to extracting machinereadable metadata, such as the names of the authors, the title, or  journal name, from bibliographic reference strings. Many  approaches to this problem have been proposed so far, including  regular expressions, knowledge bases and supervised machine  learning. Many open source reference parsers based on various  algorithms are also available. In this paper, we apply, evaluate and  compare ten reference parsing tools in a specific business use  case. The tools are Anystyle-Parser, Biblio, CERMINE, Citation,  Citation-Parser, GROBID, ParsCit, PDFSSA4MET, Reference  Tagger and Science Parse, and we compare them in both their outof-the-box versions and versions tuned to the project-specific  data.", "metadata": {"source": "data/12_dominika_2018_machinelearningvsrulebased.pdf", "page_number": 1, "chunk": 0, "title": "Machine Learning vs. Rules and Out-of-the-Box vs. Retrained: An Evaluation of Open-Source Bibliographic Reference and Citation Parsers"}}, "116": {"idx": 116, "content": "According to our evaluation, the best performing out-of-thebox tool is GROBID (F1 0.89), followed by CERMINE (F1 0.83)  and ParsCit (F1 0.75).  We also found that even though machine  learning-based tools and tools based on rules or regular  expressions achieve on average similar precision (0.77 for MLbased tools vs. 0.76 for non-ML-based tools), applying machine  learning-based tools results in a recall three times higher than in  the case of non-ML-based tools (0.66 vs. 0.22). Our study also  confirms that tuning the models to the task-specific data results in  the increase in the quality. The retrained versions of reference  parsers are in all cases better than their out-of-the-box  counterparts; for GROBID F1 increased by 3% (0.92 vs. 0.89), for  CERMINE by 11% (0.92 vs. 0.83), and for ParsCit by 16% (0.87  vs. 0.75).  CCS CONCEPTS  \u2022 Information systems \u2192 Information systems applications;  Digital libraries and archives    KEYWORDS  bibliographic reference parsing, citation parsing, machine  learning, sequence tagging  ACM Reference format:  D. Tkaczyk, A. Collins, P. Sheridan, J. Beel. 2018. Machine Learning vs.  Rules and Out-of-the-Box vs. Retrained: An Evaluation of Open-Source  Bibliographic Reference and Citation Parsers. In Proceedings of ACM  JCDL '18, June 3\u20137, 2018, Fort Worth, TX, USA, 10 pages.  DOI: 10.1145/3197026.3197048  1 INTRODUCTION  Within the past decades there has been exponential increase in the  volume of available scientific literature [1].", "metadata": {"source": "data/12_dominika_2018_machinelearningvsrulebased.pdf", "page_number": 1, "chunk": 1, "title": "Machine Learning vs. Rules and Out-of-the-Box vs. Retrained: An Evaluation of Open-Source Bibliographic Reference and Citation Parsers"}}, "117": {"idx": 117, "content": "This has resulted in a  scientific information overload problem, which refers to  challenges related to consuming enormous amount of information  by interested readers. Scientific information systems and digital  libraries help researchers to tackle the scientific information  overload problem by providing intelligent information retrieval  and recommendation services. These services need machinereadable, rich bibliographic metadata of stored documents to  function correctly, but this requirement is not always met in  practice. As a consequence, there is a huge demand for automated  methods and tools able to extract high-quality machine-readable  bibliographic metadata information directly from scientific  unstructured data.  Reference parsing is one important task in this research area.  In reference parsing, the input is a single reference string, usually  formatted in a specific bibliography style (Figure 1). The output is  a machine-readable representation of the input string, typically  called a parsed reference (Figure 2). Such parsed representation is  a collection of metadata fields, each of which is composed of a  field type (e.g. \u201cvolume\u201d or \u201cjournal\u201d) and value (e.g. \u201c12\u201d or  \u201cNature\u201d).  .", "metadata": {"source": "data/12_dominika_2018_machinelearningvsrulebased.pdf", "page_number": 1, "chunk": 2, "title": "Machine Learning vs. Rules and Out-of-the-Box vs. Retrained: An Evaluation of Open-Source Bibliographic Reference and Citation Parsers"}}, "118": {"idx": 118, "content": "JCDL\u201918, June 2018, Fort Worth, Texas USA  D. Tkaczyk et al.    2        Figure 1: An example bibliographic reference string on the  input of the reference parsing algorithm.    Figure 2: An example output of the reference parsing task,  which is a machine-readable representation of the reference  string from Figure 1. This representation is a collection of  metadata fields, each composed of a field type and value. For  this reference, the following metadata field types were  extracted: author, title, journal, volume, issue, first page, last  page, year and doi.  Bibliographic reference parsing is important for tasks such as  matching citations to cited documents [2], assessing the impact of  researchers [3, 4], journals [5, 6] and research institutions [7, 8],  and calculating document similarity [9, 10], in the context of  academic search engines [11, 12] and recommender systems [13,  14].  Reference parsing can be viewed as reversing the process of  formatting a bibliography record into a string. During formatting  some information is lost, and thus the reversed process is not a  trivial task and usually introduces errors.  There are a few challenges related to reference parsing. First,  the type of the referenced object (a journal article, a conference  publication, a patent, etc.) is typically not known, so we do not  know which metadata fields can be extracted.", "metadata": {"source": "data/12_dominika_2018_machinelearningvsrulebased.pdf", "page_number": 2, "chunk": 0, "title": "Machine Learning vs. Rules and Out-of-the-Box vs. Retrained: An Evaluation of Open-Source Bibliographic Reference and Citation Parsers"}}, "119": {"idx": 119, "content": "Second, the  reference style is unknown, thus we do not know where in the  string specific metadata fields are present. Finally, it is common  for a reference string to contain errors, introduced either by  humans while adding the references to the paper, or by the process  of extracting the string itself from the scientific publication. These  errors include for example OCR errors, unexpected spaces inside  words, missing spaces, typos and errors in style-specific  punctuation.   The most popular approaches to reference parsing include  regular expressions, template matching, knowledge bases and  supervised machine learning. There also exist a number of open  source reference parsers ready to use. It is unknown, however,  which approaches and which open source parsers give the best  results for given metadata field types. What is more, some of the  existing parsers can be tuned to the data of interest. In theory, this  process should increase the quality of the results, but it is also  time consuming and requires training data, which is typically  expensive to obtain. An important issue is then how high an  increase in the quality should be expected after retraining. These  aspects are important for researchers and programmers developing  larger information extraction systems for scientific data, as well as  digital library practitioners wishing to use existing bibliographic  reference parsers within their infrastructures.", "metadata": {"source": "data/12_dominika_2018_machinelearningvsrulebased.pdf", "page_number": 2, "chunk": 1, "title": "Machine Learning vs. Rules and Out-of-the-Box vs. Retrained: An Evaluation of Open-Source Bibliographic Reference and Citation Parsers"}}, "120": {"idx": 120, "content": "In this study we apply, evaluate and compare a number of  existing reference parsing tools, both their out-of-the-box and  retrained versions, in the context of a real business project  involving data from chemical domains. Specifically, we are  interested in the following questions:   1.  How good are reference parsing tools for our use  case?   2.  How do the results of machine learning-based  approaches compare to the results of more static,  non-trainable  approaches,  such  as  regular  expressions or rules?  3.  How much impact does retraining the machine  learning models using project-specific data have on  the parsing results?  In the following sections, we describe the state of the art, give  the larger context of the business case, list the tools we evaluated,  describe our evaluation setup and report the results. Finally, we  discuss the findings and present conclusions.  2 RELATED WORK   Reference parsing is a well-known research problem, and many  techniques have been proposed for solving it over the years,  including regular expressions, template matching, knowledge  bases and supervised machine learning.  Regular expressions are a simple way of approaching the task  of reference parsing. This approach is typically based on a set of  manually developed regular expressions able to capture single or  multiple metadata fields in different reference styles.", "metadata": {"source": "data/12_dominika_2018_machinelearningvsrulebased.pdf", "page_number": 2, "chunk": 2, "title": "Machine Learning vs. Rules and Out-of-the-Box vs. Retrained: An Evaluation of Open-Source Bibliographic Reference and Citation Parsers"}}, "121": {"idx": 121, "content": "Such a  strategy works best if the reference styles to process are known in  advance and if the data contains little noise. In practice, it can be  challenging to maintain a regular expressions-based system,  constantly adapting the set of used regular expressions to  changing data.  Regular expressions are often combined with other techniques,  such as hand-crafted rules or knowledge bases. In knowledgebased approaches, at the beginning the system is populated with  knowledge extracted from available data and/or existing external  sources, such as digital libraries. During the actual parsing,  fragments of the input reference string are matched against the  information in the knowledge base. This approach works best in  .", "metadata": {"source": "data/12_dominika_2018_machinelearningvsrulebased.pdf", "page_number": 2, "chunk": 3, "title": "Machine Learning vs. Rules and Out-of-the-Box vs. Retrained: An Evaluation of Open-Source Bibliographic Reference and Citation Parsers"}}, "122": {"idx": 122, "content": "Machine Learning vs. Rules and Out-of-the-Box vs. Retrained\u2026  JCDL\u201918, June 2018, Fort Worth, Texas USA      3 the case of fields which values tend to form closed sets, such as  journal titles or last names.  Gupta et al. [15] propose a combination of regular-expression  based heuristics and knowledge-based systems for reference  parsing. In addition, their approach is able to match inline  citations to their corresponding bibliographic references.  Constantin et al. [16] describe a rule- and regular expressionsbased system called PDFX. PDFX is in fact a large system able to  extract the logical structure of scholarly articles in PDF form,  including parsed bibliography.  Day et al. [17] employ a hierarchical knowledge representation  framework called INFOMAP for extracting metadata from  reference strings. They report 92.39% accuracy for extracting  author, title, journal, volume, issue, year, and page from  references formatted with six major reference styles.  Finally, Cortez et al. [18] present FLUX-CiM, a method for  reference parsing based on a knowledge base automatically  constructed from an existing set of sample metadata records,  obtained from public data repositories. According to their results,  FLUX-CiM achieves precision and recall above 94% for a wide  set of metadata fields.  In template matching approaches, references are first matched  against a database of templates and then template-specific rules or  regular expressions are used.", "metadata": {"source": "data/12_dominika_2018_machinelearningvsrulebased.pdf", "page_number": 3, "chunk": 0, "title": "Machine Learning vs. Rules and Out-of-the-Box vs. Retrained: An Evaluation of Open-Source Bibliographic Reference and Citation Parsers"}}, "123": {"idx": 123, "content": "For example, Hsieh et al. [19] propose a reference parsing  algorithm, in which the matching is based on sequence alignment.  They report a 70% decrease in the average field error rate (2.24%  vs. 7.54%) in comparison to a widely used machine learningbased approach.  Chen et al. [20] describe a tool called BibPro, which is able to  extract metadata from reference strings using a gene sequence  alignment tool (Basic Local Alignment Search Tool).  The most popular approach to reference parsing is supervised  machine learning. In this approach training data is used to learn a  so-called model, which is used during actual parsing to extract  metadata from the input string. Such an approach requires little  expert knowledge, as patterns are learned directly from the  training data. Maintainability is also an important concern in a  machine learning-based approach, however, it is comparatively  easy to make sure the models are up to date by repeatedly  retraining them on newer data.  In a supervised machine learning-based approach, reference  parsing is usually formally defined as a sequence tagging  problem. In a sequence tagging problem, on the input there is a  sequence of objects represented by features, and the goal is to  assign a corresponding sequence of labels, taking into account not  only the features themselves, but also the dependencies between  direct and indirect neighboring labels in the sequence.", "metadata": {"source": "data/12_dominika_2018_machinelearningvsrulebased.pdf", "page_number": 3, "chunk": 1, "title": "Machine Learning vs. Rules and Out-of-the-Box vs. Retrained: An Evaluation of Open-Source Bibliographic Reference and Citation Parsers"}}, "124": {"idx": 124, "content": "For a sequence tagger to be useful for a reference parsing task,  first the input reference string has to be transformed into a  sequence of smaller  fragments, typically called tokens.  Tokenization can be performed in many different ways, for  example it can be based on punctuation characters, or spaces.  After tokenization, each token is assigned a label by a supervised  sequence tagger. The labels usually correspond to the sought  metadata field types, and a special label \u201cother\u201d is used for tokens  that are not a part of any metadata field. Sometimes separate  labels are used for the first token of a metadata field. After  assigning labels to tokens, neighboring tokens with the same label  are concatenated to form the final metadata fields.  It is important to note that in order to train a supervised  sequence tagger for reference parsing, a specific representation of  a reference string, composed of labeled tokens is required (Figure  3). In practice, training data is usually stored in an XML-based  format, which can be easily transformed to the sequence of  labelled tokens (Figure 4).    Figure 3: An example of a reference string represented as a  sequence of labelled tokens. In this case a single token can be  either a sequence of letters, a sequence of digits, or a single  other character. The labels are given in square brackets.    Figure 4: An example of a reference string, represented in  XML-based format.", "metadata": {"source": "data/12_dominika_2018_machinelearningvsrulebased.pdf", "page_number": 3, "chunk": 2, "title": "Machine Learning vs. Rules and Out-of-the-Box vs. Retrained: An Evaluation of Open-Source Bibliographic Reference and Citation Parsers"}}, "125": {"idx": 125, "content": "Given the tokenization strategy, there is a  1-1 mapping between this representation and the sequence of  labelled tokens.  Many machine learning algorithms have been applied to  problem of reference parsing, including Support Vector Machines  (SVM) [21, 22], Hidden Markov Models (HMM) [23, 24, 25], and  Conditional Random Fields (CRF) [21, 22, 26, 27, 28, 29, 30].  SVM is a general-purpose classification technique, while both  HMM and CRF can be directly employed as sequence taggers.  Hetzner [23] proposes a simple HMM-based solution for  extracting metadata fields from references. Yin et al. [24] employ  a modification of a traditional HMM called a bigram HMM,  which considers words\u2019 bigram sequential relation and position  information. Finally, Ojokoh et al. [25] explore a trigram version  of HMM, reporting overall accuracy, precision, recall and F1  measure of over 95%.  .", "metadata": {"source": "data/12_dominika_2018_machinelearningvsrulebased.pdf", "page_number": 3, "chunk": 3, "title": "Machine Learning vs. Rules and Out-of-the-Box vs. Retrained: An Evaluation of Open-Source Bibliographic Reference and Citation Parsers"}}, "126": {"idx": 126, "content": "JCDL\u201918, June 2018, Fort Worth, Texas USA  D. Tkaczyk et al.    4    By far the most popular machine learning algorithm for  reference parsing is Conditional Random Fields. Councill et al.   [26] describe ParsCit, one of the best known, widely used open  source CRF-based systems for extracting metadata from  references.  GROBID, created by Lopez [27], is another example of a  CRF-based system able to parse bibliographic references.  GROBID is also a larger tool, able to extract the metadata and  logical structure from scientific papers in PDF. The author reports  metadata field-level accuracy of 95,7%.  CERMINE, proposed by Tkaczyk et al. [28], is also a large  system able to extract metadata and structure, including parsed  bibliography, from scientific papers in PDF format. CERMINE\u2019s  reference parsing functionality is also based on CRF technique. In  2015 CERMINE won Semantic Publishing Challenge [31, 32],  which included tasks requiring accurate extraction of title and  year information from bibliographic references.  Matsouka et al. [33] also propose a CRF-based reference  parsing method, which uses lexical features as well as lexicons.  Finally, Zhang et al. [30] applied CRF algorithm for the task  of extracting author, title, journal and year information from  reference strings, reporting an overall 97.95% F1 on PubMed  Central data.  Some researchers also compare various approaches to  bibliographic reference parsing. For example, Zou et al.", "metadata": {"source": "data/12_dominika_2018_machinelearningvsrulebased.pdf", "page_number": 4, "chunk": 0, "title": "Machine Learning vs. Rules and Out-of-the-Box vs. Retrained: An Evaluation of Open-Source Bibliographic Reference and Citation Parsers"}}, "127": {"idx": 127, "content": "[21]  compare CRF and SVM, achieving very similar overall accuracies  for both approaches: above 99% accuracy at the token level, and  over 97% accuracy at the metadata field level.  Zhang et al. [22] propose structural SVM with contextual  features, and compare it to conventional SVM and CRF. They  also report similar accuracies for all three approaches: above 98%  token classification accuracy and above 95% for field extraction.  Finally, Kim et al. [29] describe a system called BILBO and  compare it to other popular reference parsing tools (ParsCit,  Biblio, free_cite and GROBID), using previously unseen data.  According to their study, the best results were obtained by BILBO  (F1 0.64), followed closely by GROBID (F1 0.63).  A number of reference parsers are also available as open  source tools. They can be divided into two categories: tools that  are solely reference parsers, and tools with wider functionality.", "metadata": {"source": "data/12_dominika_2018_machinelearningvsrulebased.pdf", "page_number": 4, "chunk": 1, "title": "Machine Learning vs. Rules and Out-of-the-Box vs. Retrained: An Evaluation of Open-Source Bibliographic Reference and Citation Parsers"}}, "128": {"idx": 128, "content": "Pure reference parsers include:  \u2022  Anystyle-Parser 1  (a CRF-based tool written in  Ruby)  \u2022  Biblio2 (a Perl library based on regular expressions)  \u2022  BibPro3 [20] (based on sequence alignment)  \u2022  Citation 4 (a parser written in Ruby, uses regular  expressions and additional rules)  \u2022  Citation-Parser 5  (a rule-based parser written in  Python)  \u2022  Free_cite6 (a CRF-based parser written in Ruby)                                                                    1 https://github.com/inukshuk/anystyle-parser  2 http://search.cpan.org/~mjewell/Biblio-Citation-Parser-1.10/  3 https://github.com/ice91/BibPro  4 https://github.com/nishimuuu/citation  5 https://github.com/manishbisht/Citation-Parser  \u2022  Neural Parscit7 (a parser based on LSTM, the only  deep learning-based tool we found)  \u2022  Reference Tagger8 (a CRF-based parser written in  Python)  Apart  from  tools  providing  only  reference  parsing  functionality, there exist a few larger systems able to extract much  more information from scientific documents. It is possible,  however, to employ them only for the task of reference parsing.  These are:  \u2022  CERMINE9 [28]  \u2022  GROBID10 [27]  \u2022  ParsCit11 [26]  \u2022  PDFSSA4MET12  \u2022  Science Parse13   Table 1 summarizes the techniques employed by each parser  and gives details about the extracted metadata fields.  3 BUSINESS CASE   Some details related to the business case are left out on purpose,  as we are not allowed to publish them.", "metadata": {"source": "data/12_dominika_2018_machinelearningvsrulebased.pdf", "page_number": 4, "chunk": 2, "title": "Machine Learning vs. Rules and Out-of-the-Box vs. Retrained: An Evaluation of Open-Source Bibliographic Reference and Citation Parsers"}}, "129": {"idx": 129, "content": "In the business project, the input is a collection of 506,540  scientific documents in PDF format, mostly from chemical  domains. The goal of the project is to extract machine-readable  bibliographies from the input documents in order to identify all  documents cited by each paper. More specifically, for each input  document we require all bibliographic items (journal papers,  conference proceedings, web pages, etc.) listed in the document.  Every extracted bibliographic item should be in the form of a  parsed bibliographic reference.  The input documents vary in quality. Some of them are native  PDF files, with all characters correctly present in the PDF content  stream, while others contain the results of a separate OCR  process, with typical OCR errors.  The following metadata fields are required by the client of the  project as output from the reference parsing process:  \u2022  author: the first author of the referenced document,  formatted as \u201cLastname, Initial_of_the_first_name\u201d  (e.g. \u201cTkaczyk, D\u201d),  \u2022  source: the source of the referenced document, this  can be the name of the journal or the conference,  URL or identifier such as ArXiv id or DOI,  \u2022  year,  \u2022  volume,  \u2022  issue,  \u2022  page: the first page of the pages range,  \u2022  organization: the organization, which is an author of  the referenced document, the \u201ccorporate author\u201d.", "metadata": {"source": "data/12_dominika_2018_machinelearningvsrulebased.pdf", "page_number": 4, "chunk": 3, "title": "Machine Learning vs. Rules and Out-of-the-Box vs. Retrained: An Evaluation of Open-Source Bibliographic Reference and Citation Parsers"}}, "130": {"idx": 130, "content": "6 https://github.com/miriam/free_cite  7 https://github.com/opensourceware/Neural-ParsCit  8 https://github.com/rmcgibbo/reftagger  9 https://github.com/CeON/CERMINE  10 https://github.com/kermitt2/grobid  11 https://github.com/knmnyn/ParsCit  12 https://github.com/eliask/pdfssa4met  13 https://github.com/allenai/science-parse  .", "metadata": {"source": "data/12_dominika_2018_machinelearningvsrulebased.pdf", "page_number": 4, "chunk": 4, "title": "Machine Learning vs. Rules and Out-of-the-Box vs. Retrained: An Evaluation of Open-Source Bibliographic Reference and Citation Parsers"}}, "131": {"idx": 131, "content": "Machine Learning vs. Rules and Out-of-the-Box vs. Retrained\u2026  JCDL\u201918, June 2018, Fort Worth, Texas USA      5 Table 1: Summary of extracted metadata fields and  techniques employed by each open source reference parser.", "metadata": {"source": "data/12_dominika_2018_machinelearningvsrulebased.pdf", "page_number": 5, "chunk": 0, "title": "Machine Learning vs. Rules and Out-of-the-Box vs. Retrained: An Evaluation of Open-Source Bibliographic Reference and Citation Parsers"}}, "132": {"idx": 132, "content": "Tool  Approach  Extracted fields  Anystyle-Parser  CRF  authors, booktitle, date, DOI,  edition, editor, genre, ISBN,  journal, location, pages,  publisher, title, URL, volume    Biblio  regular  expressions  authors, date, editor, genre,  issue, pages, publisher, title,  volume, year    BibPro  template  matching  authors, editor, institution,  issue, journal, pages, volume,  year    CERMINE  CRF  authors, DOI, issue, pages,  title, volume, year    citation  regular  expressions  + rules    authors, title, URL, year  Citation-Parser  rules  authors, booktitle, issue,  journal, pages, publisher, title,  volume, year    free_cite  CRF  authors, booktitle, date, editor,  institution, journal, location,  pages, publisher, title, volume    GROBID  CRF  authors, date, editor, issue,  journal, organization, pages,  title, volume    Neural ParsCit  LSTM  authors, booktitle, date, editor,  institution, journal, issue,  location, pages, publisher,  volume    ParsCit  CRF  authors, booktitle, date, editor,  institution, journal, issue,  location, pages, publisher,  volume    PDFSSA4MET  regular  expressions    pages, title, volume, year  Reference  Tagger  CRF  authors, issue, journal, pages,  title, volume, year    Science Parse  CRF  author title, volume, year,  journal          Unlike the typical reference parsing task, the title of the  referenced document is not required by our client.", "metadata": {"source": "data/12_dominika_2018_machinelearningvsrulebased.pdf", "page_number": 5, "chunk": 1, "title": "Machine Learning vs. Rules and Out-of-the-Box vs. Retrained: An Evaluation of Open-Source Bibliographic Reference and Citation Parsers"}}, "133": {"idx": 133, "content": "In chemistry,  information about the title is often missing from the reference  string, as the information about the authors, source and numbers  (volume, issue, pages) are sufficient to identify a cited paper.  For the task of extracting machine-readable bibliography  metadata from a scientific paper, we employ a workflow  composed of three stages (Figure 5):  1.  First, the PDF file is parsed and the regions containing  bibliography are recognized.  2.  Next, the content of these bibliography regions is split  into a list of individual reference strings.  3.  Finally, we perform reference parsing for each reference  string separately.      Figure 5: The workflow of extracting machine-readable  bibliography metadata from a document in PDF format. The  workflow is composed of three stages: 1) recognizing the  bibliography regions in the document, 2) splitting the  bibliography into individual references, and 3) parsing each  reference string in isolation.  For the first two stages we employed the open source tool  GROBID [27]. It uses supervised machine learning to find  bibliography regions within a document and split their contents  into a list of reference strings.  The third stage of the workflow is in fact reference parsing  task. Since there are a lot of open source bibliographic reference  parsers available (including the GROBID system itself), we  decided to perform a comparative study to find out which parsers  perform the best.", "metadata": {"source": "data/12_dominika_2018_machinelearningvsrulebased.pdf", "page_number": 5, "chunk": 2, "title": "Machine Learning vs. Rules and Out-of-the-Box vs. Retrained: An Evaluation of Open-Source Bibliographic Reference and Citation Parsers"}}, "134": {"idx": 134, "content": "This paper focuses solely on the third, final  stage of the workflow.  4 METHODOLOGY  4.1 Evaluated Tools  In our study, we include only open source reference parsers:  Anystyle-Parser, Biblio, BibPro, CERMINE, Citation, CitationParser,  Free_cite,  GROBID,  Neural  Parscit,  ParsCit,  PDFSSA4MET, Reference Tagger and Science Parse.  We were not able to evaluate three tools, due to installation  errors or missing resources: BibPro, Free_cite and Neural ParsCit.  As mentioned before, not all evaluated tools extract all needed  metadata fields. Also, in some cases the tools extract only a subset  of a metadata field (for example, Anystyle-Parser extracts journal  name, but not URL or DOI, which constitutes only part of the  \u201csource\u201d field). Table 2 shows the matching between the fields  extracted by all evaluated tools and the desired metadata fields.      .", "metadata": {"source": "data/12_dominika_2018_machinelearningvsrulebased.pdf", "page_number": 5, "chunk": 3, "title": "Machine Learning vs. Rules and Out-of-the-Box vs. Retrained: An Evaluation of Open-Source Bibliographic Reference and Citation Parsers"}}, "135": {"idx": 135, "content": "JCDL\u201918, June 2018, Fort Worth, Texas USA  D. Tkaczyk et al.    6    Table 2: The matching between the output of all evaluated  tools and the metadata fields required in our project.  Tool  author  source  year  vol  issue  page  Org  Anystyle-  Parser    + 14  + 15  +  +  -  +  -    Biblio  + 16  +  +  +  +  +  -    CER-  MINE  +  + 17  +  +  +  +  -    citation  -  + 18  +  -  -  -  -    Citation-  Parser    +  + 19  +  +  +  +  -  GROBID  +  +  +  +  +  +  -    ParsCit  + 14  + 19  +  +  +  +  -    PDFSSA-  4MET  -  -  +  +  -  +  -    Reference  Tagger    +  + 15  +  +  +  +  -    Science  Parse  + 16  + 15  +  +  -  +  -  4.2 Data  We had access to a collection of 9,491 pairs: PDF document + a  list of parsed references, provided by our client. The collection  contained 371,656 parsed references and 1,886,174 metadata  fields in total. The data was manually curated and contains  occasional minor errors (e.g. typos). For the purpose of the study  we assume it is 100% correct.  The data was divided in the following way: roughly 67% of  the dataset (6,306 documents) were used for manual exploratory  analyses and training the tools, and the remaining 33% (3,185  documents) were used for testing and comparing the tools, both  their out-of-the-box and retrained versions. The test set contains  64,495 references in total, which is large enough for a fair  comparison.  To be useful for the evaluation and training, the data needed  additional preprocessing.", "metadata": {"source": "data/12_dominika_2018_machinelearningvsrulebased.pdf", "page_number": 6, "chunk": 0, "title": "Machine Learning vs. Rules and Out-of-the-Box vs. Retrained: An Evaluation of Open-Source Bibliographic Reference and Citation Parsers"}}, "136": {"idx": 136, "content": "For the evaluation we needed pairs: reference string + parsed  reference. One problematic issue was that the client did not  provide reference strings directly, but they were buried in PDF  files. To obtain them, we processed the PDFs automatically using  the implementation of the first two steps of our workflow (Figure  5). Unfortunately, this process is not error-free and in some cases  results in strings missing or incorrect strings present on the output.                                                                    14 Entire author list only  15 Journal name only  16 Author fullnames only  17 Journal name and DOI only  18 URL only  19 Journal name and book title only  As a result, the number of extracted reference strings does not  even have to be equal to the number of ground truth parsed  references provided by the client, and we cannot simply use the  order of the lists to decide which string corresponds to which  ground truth reference. For example, the fifth reference string  might correspond to the seventh parsed reference, because the first  two strings are missing. To solve this problem, we used a separate  process based on dynamic programming and fuzzy term matching  to automatically infer correspondence between extracted strings  and parsed references. This resulted in generating pairs needed for  evaluation: reference string, parsed reference.", "metadata": {"source": "data/12_dominika_2018_machinelearningvsrulebased.pdf", "page_number": 6, "chunk": 1, "title": "Machine Learning vs. Rules and Out-of-the-Box vs. Retrained: An Evaluation of Open-Source Bibliographic Reference and Citation Parsers"}}, "137": {"idx": 137, "content": "For training we needed the references in a format preserving  both reference string and token tags, as explained in Section 2  (Figure 3 and Figure 4). To obtain such a representation, we  matched the ground truth field values against the extracted strings,  which allowed us to find substrings corresponding to the metadata  fields. In some cases, this process failed to find a suitable  substring (for example if the string was extracted erroneously or if  it contains noise). Such references were discarded and not used  for training.  4.3 Comparison Procedure  For a given tool and a given reference, the ground truth  metadata fields are compared with the fields extracted by the tool  from the string. The field values are subject to simple  normalization and cleaning steps (transformation to lowercase,  normalization of hyphen-like characters, cleaning fragments like  \u201c&apos;\u201d and \u201c&amp;\u201d). After cleaning, every extracted metadata  field is marked as correct or incorrect. A correct field is a field  with both type and value equal to one of the fields in the ground  truth parsed reference.  For a given metadata field type, we calculate precision, recall  and F1 measure. Precision is the ratio of the number of correctly  extracted fields (over the entire reference set) to the number of all  extracted fields. Recall is the fraction of correctly extracted fields  to the number of expected fields (fields in the ground truth data).", "metadata": {"source": "data/12_dominika_2018_machinelearningvsrulebased.pdf", "page_number": 6, "chunk": 2, "title": "Machine Learning vs. Rules and Out-of-the-Box vs. Retrained: An Evaluation of Open-Source Bibliographic Reference and Citation Parsers"}}, "138": {"idx": 138, "content": "F1 measure is the harmonic mean of precision and recall.  In practice, the tools vary in the field types and their meaning,  and in each case careful mapping from the tool\u2019s output to our  desired collection of fields was needed. For example, URL, DOI  and journal name are usually present as three separate metadata  field types, while in our project they are treated as one field  \u201csource\u201d. The tools also differ with respect to how the authors are  extracted. Some tools (e.g. Anystyle-Parser) extract the entire  author list as one field, while others split the author names. Some  tools (e.g. Biblio, ParsCit, Science Parse) extract the entire author  fullname as one string, while others mark additionally firstname,  middlename and/or surname. In our case, the surname and first  name of the first author was needed. For the systems which do not  include this information, we employ additional simple heuristics  on top of their output.  4.4 Training Procedure  Some of the tools, in particular those based on machine learning,  are trainable, which means that they are able to automatically  .", "metadata": {"source": "data/12_dominika_2018_machinelearningvsrulebased.pdf", "page_number": 6, "chunk": 3, "title": "Machine Learning vs. Rules and Out-of-the-Box vs. Retrained: An Evaluation of Open-Source Bibliographic Reference and Citation Parsers"}}, "139": {"idx": 139, "content": "Machine Learning vs. Rules and Out-of-the-Box vs. Retrained\u2026  JCDL\u201918, June 2018, Fort Worth, Texas USA      7 learn custom parsing \u201crules\u201d from the training data. Their out-ofthe-box versions already contain trained models, which are used  by default for parsing. However, we do not know whether the  default models were trained on similar data as used in our project,  or whether reference styles typical for chemical domains even  appeared in the training sets used by the authors of the tools. As a  result, we cannot be sure that the default models contain all  needed information useful for parsing chemical references. For  these reasons we decided to investigate whether retraining the  parsers on the data specific for our project improves the parsing  results.  We retrained the three most promising tools, that is, tools with  the best average results obtained by their out-of-the-box versions:  GROBID, CERMINE and ParsCit. For the training we used  10,000 references randomly chosen from the documents in the  training set. We did not use more training data for performance  reasons.  Even though all machine learning-based tools are trainable, it  is important to note that they vary a lot in how easy it is to train  them. For example, Anystyle-Parser, CERMINE, GROBID and  ParsCit contain specific training procedures and instructions,  while in other cases retraining is more difficult due to a lack of  documentation.", "metadata": {"source": "data/12_dominika_2018_machinelearningvsrulebased.pdf", "page_number": 7, "chunk": 0, "title": "Machine Learning vs. Rules and Out-of-the-Box vs. Retrained: An Evaluation of Open-Source Bibliographic Reference and Citation Parsers"}}, "140": {"idx": 140, "content": "Figure 6: The overall precision, recall and F1 values for outof-the-box systems.  5 RESULTS  Figure 6 presents the overall results of the comparison of the outof-the-box systems and Table 3 presents the evaluation results  broken down by metadata field type. Each cell in the table gives  precision, recall and F1 values, respectively. For each  combination (metadata type, metric) the best result is bolded. We  do not give the results for organization, as none of the systems is  able to extract this field.  Measured with F1, the best performing out-of-the-box tools  are: GROBID (F1 0.89), followed by CERMINE (F1 0.83) and  ParsCit (F1 0.75). All these tools implement CRF-based reference  parsers. The worst performing systems are: Citation-Parser (F1  0.27), Citation (F1 0.32) and PDFSSA4MET (F1 0.32). All those  tools are based on rules and/or regular expressions.   Table 3: Results of the evaluation of out-of-the-box tools. Each  cell gives precision, recall and F1 values, respectively. For  each category the best result is bolded.", "metadata": {"source": "data/12_dominika_2018_machinelearningvsrulebased.pdf", "page_number": 7, "chunk": 1, "title": "Machine Learning vs. Rules and Out-of-the-Box vs. Retrained: An Evaluation of Open-Source Bibliographic Reference and Citation Parsers"}}, "141": {"idx": 141, "content": "Tool  author  source  year  vol  issue  page  AnystyleParser  .62  .74  .58  .69  .44  .54  .74  .54  .62  .29  .27  .28  -  .76  .69  .73    Biblio  .74  .65  .69  .29  .11  .16  .91  .51  .65  .81  .14  .24  .11  .14  .12  .99  .18  .30    CERMINE  .81  .81  .81  .75  .61  .67  .96  .96  .96  .91  .92  .92  .51  .83  .63  .93  .82  .87    citation  -  .47  .01  .03  .99  .95  .97    -  -  -  CitationParser  .48  .49  .48  .09  .06  .07  .92  .34  .50  .45  .07  .11  .77  .10  .18  .67  .03  .06    GROBID  .86  .82  .84  .88  .82  .85  .97  .95  .96  .96  .93  .94  .91  .82  .86  .90  .89  .90    ParsCit  .86  .84  .85  75  55  63  .98  .84  .91  .92  .65  .76  .91  .42  .57  .67  .62  .65    PDFSSA4 MET  -  -  .95  .75  .84  .96  .05  .10  -  .99  .20  .34    Reference  Tagger  .69  .57  .62  .63  .53  .67  .97  .63  .76  .51  .47  .49  .58  .52  .55  .76  .66  .71    Science  Parse  .86  .62  .72  .62  .52  .57  .98  .98  .98  .43  .28  .34  -  .59  .45  .51    Measured by recall, results are the same. GROBID (0.87),  CERMINE (0.82), and ParsCit (0.69) perform best. Citation  (0.19), PDFSSA4MET (0.19), and Citation-Parser (0.20) perform  worst.   However, measured with precision, this ranking changes.  Citation (0.97), PDFSSA4MET (0.96), and GROBID (0.91)  perform best, while Citation-Parser (0.43), Anystyle-Parser (0.62)  and Biblio (0.66) perform worst.", "metadata": {"source": "data/12_dominika_2018_machinelearningvsrulebased.pdf", "page_number": 7, "chunk": 2, "title": "Machine Learning vs. Rules and Out-of-the-Box vs. Retrained: An Evaluation of Open-Source Bibliographic Reference and Citation Parsers"}}, "142": {"idx": 142, "content": "In general, for all tools precision is higher than recall, with the  difference ranging from 0.03 (CERMINE, 0.82 and 0.85) to 0.78  .", "metadata": {"source": "data/12_dominika_2018_machinelearningvsrulebased.pdf", "page_number": 7, "chunk": 3, "title": "Machine Learning vs. Rules and Out-of-the-Box vs. Retrained: An Evaluation of Open-Source Bibliographic Reference and Citation Parsers"}}, "143": {"idx": 143, "content": "JCDL\u201918, June 2018, Fort Worth, Texas USA  D. Tkaczyk et al.    8    (Citation, 0.19 and 0.97). Interestingly, the difference between  precision and recall is smaller in the case of machine learningbased tools (average difference 0.11) than in the case of regular  expressions- or rule-based systems (average difference 0.53).  The following three systems were retrained: GROBID,  CERMINE and ParsCit. These are the systems achieving the best  results in the previous experiment. Figure 7 and Table 4 show the  results.    Figure 7: The overall precision, recall and F1 of the retrained  tools.  Table 4: Evaluation results for retrained tools, broken down  by metadata field types. Each cell contains precision, recall  and F1, respectively.  Tool  author  source  year  vol  issue  page  org  CERMINE  .91  .91  .91  .84  .83  .84  .98  .97  .98  .96  .95  .96  .94  .87  .90  .96  .96  .96  .55  .31  .39    GROBID  .93  .92  .92  .89  .85  .87  .99  .98  .98  .97  .95  .96  .92  .87  .90  .91  .89  .90  .54  .52  .53    ParsCit  .74  .71  .73  .88  .75  .81  .99  .97  .98  .97  .89  .93  .96  .78  .86  .97  .92  .72  -  -  -    Both retrained CERMINE and GROBID achieved the same F1  of 0.92, and ParsCit was a bit worse with F1 of 0.87. The results  of CERMINE and GROBID broken down by metadata types  (Table 4) are similar with the exception of source (CERMINE:  0.84, GROBID: 0.87), page (CERMINE: 0.96, GROBID: 0.90)  and organization (CERMINE: 0.39, GROBID: 0.53).", "metadata": {"source": "data/12_dominika_2018_machinelearningvsrulebased.pdf", "page_number": 8, "chunk": 0, "title": "Machine Learning vs. Rules and Out-of-the-Box vs. Retrained: An Evaluation of Open-Source Bibliographic Reference and Citation Parsers"}}, "144": {"idx": 144, "content": "All three  systems achieved very similar high results for year. ParsCit did  not extract organization at all, which suggests the training process  did not pick it up from the training data.    6 DISCUSSION  At the beginning, we stated the following questions:  1.  How good are the results of existing reference  parsing tools for previously unseen data?  2.  How do the results of machine learning-based  approaches compare to the results of more static,  non-trainable  approaches,  such  as  regular  expressions or rules?  3.  How does retraining the machine learning models  using project-specific data affect the results?  Question 1. The evaluated systems vary greatly in the quality  of the results. The out-of-the-box tool achieving the best F1 is  GROBID with F1 of 0.89, followed by CERMINE (F1 0.83) and  ParsCit (F1 0.75). The tools with the worst F1 are: Citation-Parser  (F1 0.27), Citation (F1 0.32) and PDFSSA4MET (F1 0.32). Table  5 shows the final ranking of out-of-the-box systems, ordered by  decreasing F1.  Table 5: Final ranking of out-of-the-box tools, ordered by F1.  Tool  F1  precision  recall  GROBID  .89  .91  .87  CERMINE  .83  .85  .82  ParsCit  .75  .84  .69  Science Parse  .63  .72  .55  Reference Tagger  .62  .70  .57  Anystyle-Parser  .54  .62  .48  Biblio  .42  .31  .66  Citation  .32  .97  .19  PDFSSA4MET  .32  .96  .19  Citation-Parser  .27  .43  .20    Question 2.", "metadata": {"source": "data/12_dominika_2018_machinelearningvsrulebased.pdf", "page_number": 8, "chunk": 1, "title": "Machine Learning vs. Rules and Out-of-the-Box vs. Retrained: An Evaluation of Open-Source Bibliographic Reference and Citation Parsers"}}, "145": {"idx": 145, "content": "Machine learning-based systems achieve on  average better results (precision: 0.77, recall: 0.66, F1: 0.71) than  regular expressions- or rule-based tools (precision: 0.76, recall:  0.22, F1: 0.33) (Figure 8). What is more, the worst ML-based  tool, Anystyle-Parser (F1 0.54) outperforms the best non-MLbased tool, Biblio (F1 0.42).  The main cause of this difference is recall (Figure 8). The  average recall for ML-based tools (0.66) is three times as high as  non-ML-based tools (0.22). At the same time, the difference in  average precisions is small (0.77 for ML-based tools vs. 0.76 for  non-ML-based tools). The reason for this might be that it is  relatively easy to achieve good precision of manually developed  rules and regular expression, but it is difficult to have a high  enough number of rules, covering all possible reference styles.  Question 3. For all three retrained systems (CERMINE,  GROBID, ParsCit), retrained versions are better than out-of-thebox versions. The relative increase in F1 vary: GROBID 3%  (increase from 0.89 to 0.92), CERMINE 11% (increase from 0.83  to 0.92), ParsCit 16% (increase from 0.75 to 0.87). Figure 9  compares the F1 before and after retraining for each system. In  addition, in Table 6 we present the exact values for all metrics.      .", "metadata": {"source": "data/12_dominika_2018_machinelearningvsrulebased.pdf", "page_number": 8, "chunk": 2, "title": "Machine Learning vs. Rules and Out-of-the-Box vs. Retrained: An Evaluation of Open-Source Bibliographic Reference and Citation Parsers"}}, "146": {"idx": 146, "content": "Machine Learning vs. Rules and Out-of-the-Box vs. Retrained\u2026  JCDL\u201918, June 2018, Fort Worth, Texas USA      9   Figure 8: Overall precision, recall and F1 aggregated by  system type (ML, non-ML and retrained). The values are  averaged over all tools in the respective category.    Figure 9: Comparison of F1 for the out-of-the-box, and  retrained versions of systems: CERMINE, GROBID, ParsCit.  This effect is not surprising. We expect the retrained models to  perform better, since they had an opportunity to analyze the  training data and find specific \u201crules\u201d, as well as typical terms,  useful for parsing chemical references.  We obtained the highest increase in the results in the case of  ParsCit, which was the weakest system (of the three retrained)  before retraining. On the other hand, in the case of GROBID the  increase was the smallest. After retraining, the results of the three  systems were much more similar to each other than before.  In general, our results suggest that if the pretrained version of a  ML-based tool performs poorly (e.g. ParsCit), we can gain a lot  by retraining the system. On the other hand, if a system already  performs well (GROBID), we should still expect increase in the  quality, but the magnitude of the increase might be lower.  7 SUMMARY AND FUTURE WORK  In this paper we study the problem of reference parsing in the  context of a real business use case.", "metadata": {"source": "data/12_dominika_2018_machinelearningvsrulebased.pdf", "page_number": 9, "chunk": 0, "title": "Machine Learning vs. Rules and Out-of-the-Box vs. Retrained: An Evaluation of Open-Source Bibliographic Reference and Citation Parsers"}}, "147": {"idx": 147, "content": "We applied and compared ten  reference parsing tools: Anystyle-Parser, Biblio, Citation,  Citation-Parser,  Reference  Tagger,  CERMINE,  GROBID,  ParsCit, PDFSSA4MET and Science Parse. We investigated the  differences between tools that use rules or regular expressions and  machine learning-based tools. We also checked, how important  training machine learning-based tools is and how it affects the  results.  Table 6: Comparison of precision, recall and F1 between outof-the-box versions of tools and retrained versions.  tool  metric  out-ofthe-box  retrained  relative  difference  GROBID    Precision  .91  .93  2%  Recall  .87  .91  5%  F1  .89  .92  3%  CERMINE  Precision  .85  .93  9%  Recall  .82  .92  13%  F1  .83  .92  11%  ParsCit  Precision  .84  .91  9%  Recall  .69  .84  23%  F1  .75  .87  16%    According to our results, the best performing out-of-the-box  tool is GROBID with F1 of 0.89, followed by CERMINE (F1  0.83) and ParsCit (F1 0.75). On average, machine learning-based  systems achieve better results than rule-based systems (F1 0.71  vs. 0.33). While ML-based and non-ML-based tools achieve  similar precisions (0.77 and 0.76, respectively), ML-based tools  have three times higher recall than non-ML-based tools (0.66 vs.  0.22).  Our study also confirms that it is worth retraining the models  using task-specific data, especially if initial results appear low.", "metadata": {"source": "data/12_dominika_2018_machinelearningvsrulebased.pdf", "page_number": 9, "chunk": 1, "title": "Machine Learning vs. Rules and Out-of-the-Box vs. Retrained: An Evaluation of Open-Source Bibliographic Reference and Citation Parsers"}}, "148": {"idx": 148, "content": "For all three retrained systems (CERMINE, GROBID, ParsCit),  retrained versions are better than out-of-the-box versions, with the  relative differences in F1 varying from 3% (GROBID, increase  from 0.89 to 0.92), through 11% (CERMINE, increase from 0.83  to 0.92), to 16% (ParsCit, increase from 0.75 to 0.87).  It is important to note some limitations of our study. First, in  our business project a very specific metadata type set was required  by the client and only those types were present in the ground truth  data. As a result, we did not evaluate the extraction of important  metadata such as the title of the referenced document or the names  of all the authors. Second, we limited our study to reference  parsers fully implemented and made available on the Internet,  rejecting also three systems which we could not install and use  due to errors. As a result, the list of evaluated parsers does not  include, for example, tools that use template matching. Finally,  only the three best systems were retrained.  In the future, we plan to retrain all the available ML-based  tools, and perform a similar study using other available datasets  and including more metadata field types, in particular the title of  the referenced paper and the names of all the authors. We also  plan to experiment with building intelligent reference parsing  ensembles.  .", "metadata": {"source": "data/12_dominika_2018_machinelearningvsrulebased.pdf", "page_number": 9, "chunk": 2, "title": "Machine Learning vs. Rules and Out-of-the-Box vs. Retrained: An Evaluation of Open-Source Bibliographic Reference and Citation Parsers"}}, "149": {"idx": 149, "content": "JCDL\u201918, June 2018, Fort Worth, Texas USA  D. Tkaczyk et al.    10    ACKNOWLEDGMENTS  This publication has emanated from research conducted with the  financial support of Science Foundation Ireland (SFI) under Grant  Number 13/RC/2016. The project has also received funding from  the European Union\u2019s Horizon 2020 research and innovation  programme under the Marie Sk\u0142odowska-Curie grant agreement  No 713567.  REFERENCES    [1]  M. Khabsa and C. L. Giles, \"The Number of Scholarly Documents on the  Public Web,\" PLoS ONE, 2014.   [2]  M. Fedoryszak, D. Tkaczyk and L. Bolikowski, \"Large Scale Citation Matching  Using Apache Hadoop,\" in International Conference on Theory and Practice of  Digital Libraries (TPDL), 2013.   [3]  J. E. Hirsch, \"An index to quantify an individual's scientific research output that  takes into account the effect of multiple coauthorship,\" Scientometrics, vol. 85,  no. 3, pp. 741-754, 2010.   [4]  S. Alonso, F. J. Cabrerizo, E. Herrera-Viedma and F. Herrera, \"h-Index: A  review focused in its variants, computation and standardization for different  scientific fields,\" J. Informetrics, vol. 3, no. 4, pp. 273-289, 2009.   [5]  T. Braun, W. Gl\u00e4nzel and A. Schubert, \"A Hirsch-type index for journals,\"  Scientometrics, vol. 69, no. 1, pp. 169-173, 2006.   [6]  B. Gonz\u00e1lez-Pereira, V. P. Guerrero Bote and F. de Moya Aneg\u00f3n, \"A new  approach to the metric of journals' scientific prestige: The SJR indicator,\" J.  Informetrics, vol. 4, no. 3, pp. 379-391, 2010.", "metadata": {"source": "data/12_dominika_2018_machinelearningvsrulebased.pdf", "page_number": 10, "chunk": 0, "title": "Machine Learning vs. Rules and Out-of-the-Box vs. Retrained: An Evaluation of Open-Source Bibliographic Reference and Citation Parsers"}}, "150": {"idx": 150, "content": "[7]  J.-F. Molinari and A. Molinari, \"A new methodology for ranking scientific  institutions,\" Scientometrics, vol. 75, no. 1, pp. 163-174, 2008.   [8]  D. Torres-Salinas, J. G. Moreno-Torres, E. D. L\u00f3pez-C\u00f3zar and F. Herrera, \"A  methodology for Institution-Field ranking based on a bidimensional analysis:  the IFQ2A index,\" Scientometrics, vol. 88, no. 3, pp. 771-786, 2011.   [9]  P. Ahlgren and C. Colliander, \"Document-document similarity approaches and  science  mapping: Experimental comparison of  five approaches,\"  J.  Informetrics, vol. 3, no. 1, pp. 49-63, 2009.   [10] J. Beel, Virtual Citation Proximity (VCP): Calculating Co-Citation-ProximityBased Document Relatedness for Uncited Documents with Machine Learning  [Proposal], 2017.   [11] J. Wu, K. M. Williams, H.-H. Chen, M. Khabsa, C. Caragea, S. Tuarob, A.  Ororbia, D. Jordan, P. Mitra and C. L. Giles, \"CiteSeerX: AI in a Digital  Library Search Engine,\" AI Magazine, vol. 36, no. 3, pp. 35-48, 2015.   [12] C. Xiong, R. Power and J. Callan, \"Explicit Semantic Ranking for Academic  Search via Knowledge Graph Embedding,\" in WWW, 2017.   [13] J. Beel, A. Aizawa, C. Breitinger and B. Gipp, \"Mr. DLib: Recommendationsas-a-Service (RaaS) for Academia,\" in JCDL, 2017.   [14] J. Beel, B. Gipp, S. Langer and C. Breitinger, \"Research-paper recommender  systems: a literature survey,\" Int. J. on Digital Libraries, vol. 17, no. 4, pp. 305338, 2016.   [15] D. Gupta, B. Morris, T. Catapano and G.", "metadata": {"source": "data/12_dominika_2018_machinelearningvsrulebased.pdf", "page_number": 10, "chunk": 1, "title": "Machine Learning vs. Rules and Out-of-the-Box vs. Retrained: An Evaluation of Open-Source Bibliographic Reference and Citation Parsers"}}, "151": {"idx": 151, "content": "Sautter, \"A new approach towards  bibliographic reference identification, parsing and inline citation matching,\" in  International Conference on Contemporary Computing, 2009.   [16] A. Constantin, S. Pettifer and A. Voronkov, \"PDFX: fully-automated pdf-toxml conversion of scientific literature,\" ACM Symposium on Document  Engineering, pp. 177-180, 2013.   [17] M.-Y. Day, R.-H. Tsai, C.-L. Sung, C.-C. Hsieh, C.-W. Lee, S.-H. Wu, K.-P.  Wu, C.-S. Ong and W.-L. Hsu, \"Reference metadata extraction using a  hierarchical knowledge representation framework,\" Decision Support Systems,  vol. 43, no. 1, pp. 152-167, 2007.   [18] E. Cortez, A. S. da Silva, M. A. Gon\u00e7alves, F. de S\u00e1 Mesquita and E. S. de  Moura, \"A flexible approach for extracting metadata from bibliographic  citations,\" JASIST, vol. 60, no. 6, pp. 1144-1158, 2009.   [19] Y.-L. Hsieh, S.-H. Liu, T.-H. Yang, Y.-H. Chen, Y.-C. Chang, G. Hsieh, C.-W.  Shih, C.-H. Lu and W.-L. Hsu, \"A Frame-Based Approach for Reference  Metadata Extraction,\" in Technologies and Applications of Artificial  Intelligence, 2014.   [20] C.-C. Chen, K.-H. Yang, C.-L. Chen and J.-M. Ho, \"BibPro: A Citation Parser  Based on Sequence Alignment,\" IEEE Trans. Knowl. Data Eng., vol. 24, no. 2,  pp. 236-250, 2012.   [21] J. Zou, D. X. Le and G. R. Thoma, \"Locating and parsing bibliographic  references in HTML medical articles,\" IJDAR, vol. 13, no. 2, pp. 107-119,  2010.   [22] X. Zhang, J. Zhou, D. X. Le and G. R.", "metadata": {"source": "data/12_dominika_2018_machinelearningvsrulebased.pdf", "page_number": 10, "chunk": 2, "title": "Machine Learning vs. Rules and Out-of-the-Box vs. Retrained: An Evaluation of Open-Source Bibliographic Reference and Citation Parsers"}}, "152": {"idx": 152, "content": "Thoma, \"A structural SVM approach for  reference parsing,\" BMC Bioinformatics, vol. 12, no. S-3, p. S7, 2011.   [23] E. Hetzner, \"A simple method for citation metadata extraction using hidden  markov models,\" in Joint Conference on Digital Libraries, Pittsburgh, 2008.   [24] P. Yin, M. Zhang, Z.-H. Deng and D. Yang, \"Metadata Extraction from  Bibliographies Using Bigram HMM,\" in International Conference on Asian  Digital Libraries, 2004.   [25] B. A. Ojokoh, M. Zhang and J. Tang, \"A trigram hidden Markov model for  metadata extraction from heterogeneous references,\" Inf. Sci., vol. 181, no. 9,  pp. 1538-1551, 2011.   [26] I. Councill, C. Giles and M.-Y. Kan, \"ParsCit: an open-source CRF reference  string parsing package,\" in International Conference on Language Resources  and Evaluation, 2008.   [27] P. Lopez, \"GROBID: combining automatic bibliographic data recognition and  term extraction for scholarship publications,\" Research and Advanced  Technology for Digital Libraries, pp. 473-474, 2009.   [28] D. Tkaczyk, P. Szostek, M. Fedoryszak, P. Dendek and L. Bolikowski,  \"CERMINE: automatic extraction of structured metadata from scientific  literature,\" International Journal on Document Analysis and Recognition, vol.  18, no. 4, pp. 317-335, 2015.   [29] Y.-M. Kim, P. Bellot, J. Tavernier, E. Faath and M. Dacos, \"Evaluation of  BILBO reference parsing in digital humanities via a comparison of different  tools,\" in ACM Symposium on Document Engineering, 2012.   [30] Q. Zhang, Y.", "metadata": {"source": "data/12_dominika_2018_machinelearningvsrulebased.pdf", "page_number": 10, "chunk": 3, "title": "Machine Learning vs. Rules and Out-of-the-Box vs. Retrained: An Evaluation of Open-Source Bibliographic Reference and Citation Parsers"}}, "153": {"idx": 153, "content": "Cao and H. Yu, \"Parsing citations in biomedical articles using  conditional random fields,\" Comp. in Bio. and Med., vol. 41, no. 4, pp. 190-194,  2011.   [31] D. Tkaczyk and L. Bolikowski, \"Extracting Contextual Information from  Scientific Literature Using CERMINE System,\" in Semantic Web Evaluation  Challenges - Second SemWebEval Challenge at ESWC, 2015.   [32] A. Di Iorio, C. Lange, A. Dimou and S. Vahdati, \"Semantic Publishing  Challenge - Assessing the Quality of Scientific Output by Information  Extraction and Interlinking,\" in SemWebEval@ESWC, 2015.   [33] D. Matsuoka, M. Ohta, A. Takasu and J. Adachi, \"Examination of effective  features for CRF-based bibliography extraction from reference strings,\" in  International Conference on Digital Information Management, 2016.         .", "metadata": {"source": "data/12_dominika_2018_machinelearningvsrulebased.pdf", "page_number": 10, "chunk": 4, "title": "Machine Learning vs. Rules and Out-of-the-Box vs. Retrained: An Evaluation of Open-Source Bibliographic Reference and Citation Parsers"}}, "154": {"idx": 154, "content": "J. Mol.  Bid.  (1981)  147, 195-197  Identification  of Common  Molecular  Subsequences  The  identification  of maximally  homologous  subsequences  among  sets of long  sequences  is an important  problem  in molecular  sequence analysis.  The problem  is  straightforward  only  if one restricts  consideration  to contiguous  subsequences  (segments)  containing  no internal  deletions  or insertions.  The more general problem  has its solution  in an extension  of sequence metrics  (Sellers  1974; Waterman  et al.,  1976) developed  to measure  the minimum  number  of \u201cevents\u201d  required  to convert  one sequence  into another.  These developments  in the modern  sequence  analysis  began with  the heuristic  homology  algorithm  of Needleman  & Wunsch  (1970)  which  first  introduced  an  iterative  matrix  method  of calculation.  Numerous  other heuristic  algorithms  have  been suggested  including  those of Fitch  (1966) and Dayhoff  (1969). More mathemat-  ically  rigorous  algorithms  were suggested  by Sankoff  (1972), Reichert  et al. (1973)  and Beyer  et al. (1979)  but these  were generally  not biologically  satisfying  or  interpretable.  Success came with Sellers (1974) development  of a true metric  measure  of the distance  between  sequences.  This metric  was later generalized  by Waterman  et al.  (1976)  to  include  deletions/insertions  of arbitrary  length.", "metadata": {"source": "data/18_smith_1981_identification.pdf", "page_number": 1, "chunk": 0, "title": "PII: 0022-2836(81)90087-5"}}, "155": {"idx": 155, "content": "This  metric  represents  the minimum  number  of \u201cmutational  events\u201d  required  to convert  one  sequence into another.  It is of interest  to note that Smith  et al. (1980) have recently  shown that under some conditions  the generalized  Sellers metric  is equivalent  to the  original  homology  algorithm  of Needleman  & Wunsch  (1970).  In this letter we extend  the above ideas to find a pair of segments,  one from each of  two  long  sequences,  such that  there  is no other  pair  of segments  with  greater  similarity  (homology).  The similarity  measure  used here allows for arbitrary  length  deletions  and insertions.  Algorithm  The  two  molecular  sequences  will  be h=alaz  . . . an and  IZj= blb,  b,.  A  similarity  a(a,b) is given  between  sequence  elements  a and b. Deletions  of length  k  are given weight  Wt. To find pairs of segments  with high degrees of similarity,  we set  up a matrix  H. First  set  Hto = Ho, = 0 for 0 I k I n and 0 I 1 I m.  Preliminary  values of H have the interpretation  that H, is the maximum  similarity  of two segments  ending in ai and bj, respectively.  These values are obtained  from the  relationship  Hij=max{Hi-,,j-1+S(ai,bj),  ~F,X {Hi-k,j-  W,}, ~2\"  {Hi,j-,-  W,}, 0},  (1)  1 li<n  and 1 <j<m.  195  0922-2836/80/09019&03  $02.00/O  0  1980 Academic  Press  Inc.  (London)  Ltd.  .", "metadata": {"source": "data/18_smith_1981_identification.pdf", "page_number": 1, "chunk": 1, "title": "PII: 0022-2836(81)90087-5"}}, "156": {"idx": 156, "content": "196  T.  P.  SMITH  AND  M.  S.  LVATER>lAS  The formula  for H, follows  by considering  the possibilities  for ending the  segments at any ai and b,.  (1) If ai and bj are associated, the similarity  is  Hi-l,j-l  +s(ai,bj).  (2) If ai is at the end of a deletion of length k, the similarity  is  Hi-k,j-Wk  (3) If bj is at the end of a deletion of length I, the similarity  is  Hi-k,j- cc',.  (4) Finally, a zero is included to prevent calculated negative similarity,  indicating  no similarity  up to ai and bj.t  The pair of segments with maximum  similarity  is found by first locating the  maximum element of H. The other matrix elements leading to this maximum value  are than sequentially  determined  with  a traceback procedure ending with an  element of H equal to zero. This procedure identifies  the segments as well as  produces the corresponding  alignment.  The pair of segments with the next best  similarity  is found by applying  the traceback procedure to the second largest,  element of H not associated with the first traceback.  A simple example is given in Figure 1. In this example the parameters s(aibj) and  W, required were chosen on an a priori statistical  basis. A match, ai = bj, produced  an s(aibj) value of unity while a mismatch produced a minus one-third. These values  have an average for long, random sequences over an equally probable four letter set  of zero.", "metadata": {"source": "data/18_smith_1981_identification.pdf", "page_number": 2, "chunk": 0, "title": "PII: 0022-2836(81)90087-5"}}, "157": {"idx": 157, "content": "The deletion weight must be chosen to be at least equal to the difference  between a match and a mismatch. The value used here was Wk = 1=0-t- 1/3*k.  A  0.0  0.0  0.0  0.0  04  0.0  0.0  0.0  0.0  04  0.0  04  04  04  A  0.0  0.0  1 ,o  0.0  04  04  04  0.0  04  0.0  0.0  0.0  1 ,o  04  A  0.0  0.0  1.0  0.7  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.7  L7  0.0  0.0  0.0  0.7  0.3  0.0  I.0  04  04  04  I.0  1 .o  0.0  0.i  G  0.0  0.0  04  1.0  0.3  0.0  04  0.i  1.0  0.0  0.0  0.7  0.7  I .o  c  0.0  I .o  0.0  o-o  2.0  1.3  0.3  1.0  0.3  2.0  0.7  0.3  0.3  0.3  (  0.0  1.0  0.1  0.0  1 .o  3.0  1.7  1.3  I.0  1.3  1.i  0.3  0.0  04  A  0.0  0.0  2.0  0.1  0.3  1.7  2.7  I.3  1.0  0.i  1 4  1.3  I .3  0.0  u  0.0  0.0  0.7  1.7  0.3  E-  2.1  2.3  1.0  0.5  1.7  2.0  I 4  1.0  u  0.0  0.0  0.3  0.3  1.3  I .o  1.3  2.3  2.0  0.7  1.7  2.7  1.7  I .o  G  0.0  0.0  0.0  l-3  0.0  1 .o  1.0  G  3.3  2.0  1.7  1.3  1.3  2.i  A  0.0  0.0  1 .o  0.0  I.0  0.3  0.7  0.7  56  3.0  I .i  I.3  2.3  2.1)  c  ~ 0.0  1.0  0.0  0.7  1 ,O  I.0  0.7  1 ,7  1.7  3.0  1.;  1.3  I.0  2.0  G  0.0  0.0  0.7  1.0  0.3  0.7  1.7  0.3  2.i  I.7  23  2.3  1.0  2.0  G  0.0  0.0  0.0  1.7  0.7  0.3  0.3  1.3  1 ,3  1.3  1 .3  2.3  24  2.0  FK:, 1. Hij matrix  generated  from the application  ofeqn  (1) to the sequences  A-4-U-G-(\u2018-(!-$-~\u2018-~~(~-.~~  C-G-G and C-A-G-C-C-U-C-G-C-U-U-A-G.  The underlined  elements  indicate  the trackback  path fkom the  maximal  element  3.30.", "metadata": {"source": "data/18_smith_1981_identification.pdf", "page_number": 2, "chunk": 1, "title": "PII: 0022-2836(81)90087-5"}}, "158": {"idx": 158, "content": "t Zero  need not be included  unless  there  are negative  values  ofs(a.b)  .", "metadata": {"source": "data/18_smith_1981_identification.pdf", "page_number": 2, "chunk": 2, "title": "PII: 0022-2836(81)90087-5"}}, "159": {"idx": 159, "content": "LETTERS  TO  THE  EDITOR  Note. in this simple example, that the alignment  obtained:  197  -G-C-C-A-U-U-G-  -G-C-C-UU-C.G-  contains both a mismatch and an internal  deletion. It is the identification  of the  latter which has not been previously  possible in any rigorous manner.  This algorithm  not only puts the search for pairs of maximally  similar segments  on a mathematically  rigorous basis but it can be efficiently  and simply programmed  on a computer.  Northern  Michigan University  T. F. SMITH  Los Alamos Scientific  Laboratory  P.0. Box 1663, Los Alamos  N. Mex. 87545. U.S.A.  M. S. WATERMAN  Received 14 July 1980  REFERENCES  Beyer, W. A., Smith, T. F., Stein. M. L. & Ulam, S. M. (1979). Math. Biosci. 19, 9-25.  Dayhoff. M. 0. (1969). Atlas of Protein Sequence and Structure,  National Biomedical Research  Foundation,  Silver Springs, Maryland.  Fitch, W. M. (1966). J. Mol. Biol. 16, 9-13.  Needleman, S. B. & Wunsch, C. D. (1970). J. Mol. Biol. 48, 443-453.  Reich&.  T. A., Cohen, D. N. & Wong, A. K. C. (1973). J. Theoret. Biol. 42, 245-261.  Sankoff, D. (1972). Proc. Nat. Acud. Sci., U.S.A. 61, 44.  Sellers. P. H. (1974). J. Appl. Math. (Siam), 26, 787-793.  Smith, T. F., Waterman,  M. S. & Fitch, W. M. (1981). J. Mol. Evol. In the press.  Waterman.  M. S., Smith, T. F. & Beyer, W. A. (1976). Advan. Math. 20, 367-387.  ,Votp added in proof: A weighting  similar to that given above was independently  developed  by Walter Goad of Los Alamos Scientific Laboratory.", "metadata": {"source": "data/18_smith_1981_identification.pdf", "page_number": 3, "chunk": 0, "title": "PII: 0022-2836(81)90087-5"}}, "160": {"idx": 160, "content": ".", "metadata": {"source": "data/18_smith_1981_identification.pdf", "page_number": 3, "chunk": 1, "title": "PII: 0022-2836(81)90087-5"}}, "161": {"idx": 161, "content": "A Simple Method for Citation Metadata Extraction using Hidden Markov Models Erik Hetzner California Digital Library 415 20th St Oakland, CA 94720 erik.hetzner@ucop.edu ABSTRACT This paper describes a simple method for extracting metadata \ufb01elds from citations using hidden Markov models. The method is easy to implement and can achieve levels of precision and recall for heterogeneous citations comparable to or greater than other HMM-based methods. The method consists largely of string manipulation and otherwise depends only on an implementation of the Viterbi algorithm, which is widely available, and so can be implemented by diverse digital library systems. Categories and Subject Descriptors H.3.7 [Information storage and retrieval]: Digital Libraries\u2014systems issues General Terms Algorithms Keywords Citation Management, Metadata Extraction 1. INTRODUCTION Digital libraries are often confronted with the problem of turning a textual citation into a more structured reference. A structured reference can be used to aid the tasks of citation grouping, and also mapping of the citation graph, which can provide a basis for data-mining of research papers. Although it is trivial for a human to divide a citation into its constituent \ufb01elds, it is not an easy task to make a computer do the same. The grammars used to produce citations are complex, varied, and not known in advance.", "metadata": {"source": "data/23_erik_2008_HMM.pdf", "page_number": 1, "chunk": 0, "title": ""}}, "162": {"idx": 162, "content": "Various ruleand knowledge-based [13, 7, 9], and machine-learning techniques [2, 12, 18, 22, 6, 4, 11] have been developed to extract metadata from citations. This paper describes a method for extracting structured references from plain text citations which is relatively simple and achieves acceptable precision Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro\ufb01t or commercial advantage and that copies bear this notice and the full citation on the \ufb01rst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci\ufb01c permission and/or a fee. JCDL\u201908, June 16\u201320, 2008, Pittsburgh, Pennsylvania, USA. Copyright 2008 ACM 978-1-59593-998-2/08/06 ...$5.00. and recall. Additionally, we feel that this method has potential for improvement. The engine of the method is a hidden Markov model which is learned by training on a small (less than 500) set of citations which have been marked up by hand. 2. CITATIONS AND REFERENCES We use citation to mean the free-form string, referencing a particular bibliographic item, which a human can use to locate that item. A reference is the more abstract structure which can be used, in combination with a grammar, to generate a citation. Our model of a reference, which we attempt to recover from a given citation, is of a set of labeled \ufb01elds, each of which is repeatable.", "metadata": {"source": "data/23_erik_2008_HMM.pdf", "page_number": 1, "chunk": 1, "title": ""}}, "163": {"idx": 163, "content": "Each author, for instance, is represented by a \ufb01eld labeled \u201cauthor\u201d which contains a string used to identify that author. This model is somewhat more strict than the BibTEX-in\ufb02uenced models which do not allow repeated \ufb01elds and instead combine all authors into a single labeled \ufb01eld. 3. HIDDEN MARKOV MODELS A discrete-emission hidden Markov model (HMM) [5] is a probabilistic model in which transitions between a \ufb01nite set of states are accompanied by the emission of a symbol from a \ufb01nite alphabet. They are described as \u201chidden\u201d because the emission sequence is known, and from this sequence the task is to determine the state sequence which generated it. An HMM begins in some starting state, qS, and then transitions to a new state q2, with probability P(q2|qS), in the process emitting a symbol \u03c3S with a probability given by P(\u03c3S|qS). It continues in this way until it reaches an end state qE, having emitted a sequence of symbols X. The Viterbi algorithm [5] makes use of the Markov property of an HMM (that the next state transition and symbol emission depend only upon the current state) to determine, in linear time with respect to the length of the emission sequence, the most likely path through the states of a model which might have generated a given sequence. The Viterbi algorithm is widely known and used, and implementations of the Viterbi algorithm are available for many systems, including free implementations for Perl [10], Java [17], C/C++/Python [1].", "metadata": {"source": "data/23_erik_2008_HMM.pdf", "page_number": 1, "chunk": 2, "title": ""}}, "164": {"idx": 164, "content": "HMMs have been used in the past for digital signal procession, speech recognition, bioinformatics and information extraction [5, 19, 14, 3, 21]. Although HMMs have been used before for metadata extraction from citations [22, 6, 18, 4, 11], our method achieves comparable or greater accuracy and does not depend on multiple HMMs, as in [6], a more 280 .", "metadata": {"source": "data/23_erik_2008_HMM.pdf", "page_number": 1, "chunk": 3, "title": ""}}, "165": {"idx": 165, "content": "complex bigram HMM [22], or a modi\ufb01ed Viterbi algorithm [22]. It bears many similarities to [11], although developed independently; however, it does not make use HTML tags, and uses a di\ufb00erent model, along with a reduced alphabet. It seems to achieve greater or comparable precision and accuracy to other HMM-based methods, although only [18] records measurements against the data set that we used. 4. DEFINING THE MODEL A discrete-emission, \ufb01rst-order hidden Markov model lies at the heart of this method. An HMM is de\ufb01ned by the set of states, the alphabet of symbols, the state transition probability matrix, the emission probability matrix, and the initial state probability matrix [5]. In our case, the set of states Q is derived from the labels of \ufb01elds that we wish to extract from the citations. The alphabet of symbols \u03a3 is hand-built from individual words, word classes, punctuation, and word features. The probabilities are derived from training data. 4.1 The states To build our set of states Q we begin with the labels of the \ufb01elds which we wish to extract from each citation. What label a \ufb01eld has depends upon the underlying reference model. For instance, conference proceedings might be considered a separate label from a book title, or they might be considered the same. Some labels are used in certain sets of citations but not in others.", "metadata": {"source": "data/23_erik_2008_HMM.pdf", "page_number": 2, "chunk": 0, "title": ""}}, "166": {"idx": 166, "content": "In our case we used the following labels, present in the Cora dataset, with the addition of (issue) number (which in the Cora dataset was subsumed under volume): author, booktitle, date, editor, institution, journaltitle, location, note, number, pages, publisher, techtitle, title, and volume. In [20] it is noted that having one state per label is not optimal, and we have found this to be the case. We de\ufb01ne two states in Q for each label: a \u201c\ufb01rst\u201d state and a \u201crest\u201d state. So in the case of the \u201cauthor\u201d label, we have an af state and an ar state. Other numbers of states were tried, but having a \ufb01rst and rest gave the best results. We further de\ufb01ned a set of \u201cseparator states\u201d, which represent words and punctuation that are not part of \ufb01elds, but which act to di\ufb00erentiate \ufb01elds. For instance, authors\u2019 names are often separated by a comma\u201c,\u201dor the word\u201cand\u201d, and the set of authors is often terminated by a period \u201c.\u201d, which also indicates the beginning of the title of a paper. The marks are not part of the labeled \ufb01elds which we wish to extract, but are generated by the citation grammar and are intended to aid the reader in di\ufb00erentiating the parts of a citation. In order to exploit them, we de\ufb01ne a set of separator states between each pair of \ufb01elds. For example, the state which separates an author\u2019s name and the title state is the a|t state.", "metadata": {"source": "data/23_erik_2008_HMM.pdf", "page_number": 2, "chunk": 1, "title": ""}}, "167": {"idx": 167, "content": "These separators states are non-active and do not contribute to labeled \ufb01elds, although they are used to split repeated, adjoining \ufb01elds with the same label. Finally, we de\ufb01ne a non-active terminating \u201cend\u201d state. 4.2 The alphabet Selection of a useful alphabet, \u03a3, is essential to acceptable performance of a hidden Markov model. If we take as our alphabet all words in the English language we end up with a dictionary which contains many words which our training documents will not contain, and we will be required to use complicated smoothing mechanisms to obtain useful probability matrices. Additionally, if we use the set of English words we will not exploit the relations between certain words, such as the months of the year, to help our model out. Instead, we have found it best to de\ufb01ne a mapping of tokens to a smaller alphabet of symbols. This alphabet is composed of symbols which represent punctuation, particular words, classes of words, and word features. We \ufb01rst de\ufb01ne a set of symbols to represent various punctuation, which in our method are considered to be individual tokens. The tokens that we map from include the comma \u201c,\u201d, period \u201c.\u201d, hyphen \u201c-\u201d, and so on. Each of these characters maps to one symbol. Unusual punctuation is mapped to a single miscellaneous punctuation symbol. Next we de\ufb01ne a set of symbols to represent certain key words.", "metadata": {"source": "data/23_erik_2008_HMM.pdf", "page_number": 2, "chunk": 2, "title": ""}}, "168": {"idx": 168, "content": "For instance, it is clear that the word \u201cproceedings\u201d and the abbreviation \u201cproc\u201d are important key words which can help identify conference proceedings. So we de\ufb01ne a single symbol to represent this particular word/abbreviation. Other tokens mapped to a single symbol include \u201cpress\u201d, \u201cuniversity\u201d, and \u201creport\u201d. We further de\ufb01ne a small number of word classes representing related words. For instance, it is clear that there exists a relationship between the months of the year, so a single symbol is de\ufb01ned to represent the months. Finally, we de\ufb01ne a set of symbols to match words features for the words which have not yet been matched. These features include an uppercase word, a lowercase word, a titlecase word, a string of numerals of length 4, and so forth. 4.3 Building the parameters To build our model\u2019s probability matrices we use fully labeled training data. In this data each state sequence and emission sequence is completely de\ufb01ned. To calculate the transition matrices we use the method described in [20]. For the probability of each transition from state qn to qm, we calculate: P(qm|qn) = c(qn \u2192qm) P q\u2208Q c(qn \u2192q) where c(q \u2192q\u2032) is the count of occurrences in the training date of a transition from state q to state q\u2032. We use the same method to de\ufb01ne the emission probabilities, P(\u03c3n|qn), for each qn \u2208Q and \u03c3n \u2208\u03a3, and the start probability, P(qS = qn) for each qn \u2208Q.", "metadata": {"source": "data/23_erik_2008_HMM.pdf", "page_number": 2, "chunk": 3, "title": ""}}, "169": {"idx": 169, "content": "4.4 Naive smoothing Because some state transitions and some symbol emissions will not be encountered in the training data, we will need to smooth our data somewhat. We use a very naive method, setting to a low constant (10\u22127) the probabilities of events which previously had zero probability. In order to ensure that the sums of P(q\u2032|q) and P(\u03c3|q) are 1 for all values of q\u2032 and \u03c3 we subtract from the probability of all originally non-zero probability events the value n m10\u22127 where n is the count of originally zero-probability events and m the count of originally non-zero-probability events. If we have a state for which we have no training data for transitions or emissions, we use the uniform distribution. 5. APPLYING THE METHOD Having built our model, we now describe the method by which a string of characters representing a citation is turned into a structured reference. 281 .", "metadata": {"source": "data/23_erik_2008_HMM.pdf", "page_number": 2, "chunk": 4, "title": ""}}, "170": {"idx": 170, "content": "Table 1: Precision and recall Token Field Token (stripped) Field (stripped) P R F1 P R F1 P R F1 P R F1 author 98.0 98.9 98.4 88.0 90.6 89.3 99.5 99.6 99.5 94.0 96.8 95.4 booktitle 89.4 91.4 90.4 62.5 67.2 64.7 90.4 93.8 92.1 70.8 76.1 73.4 date 94.2 97.3 95.7 89.9 94.3 92.0 96.7 99.4 98.0 94.6 99.3 96.9 editor 86.3 90.3 88.3 54.8 63.9 59.0 89.2 92.2 90.7 54.8 63.9 59.0 institution 61.2 88.2 72.3 44.4 57.1 50.0 61.9 92.9 74.3 66.7 85.7 75.0 journaltitle 88.8 70.6 78.7 88.4 74.5 80.9 89.0 73.0 80.2 88.4 74.5 80.9 location 76.5 69.0 72.6 56.8 56.8 56.8 79.2 76.0 77.6 73.0 73.0 73.0 note 29.5 66.7 40.9 33.3 50.0 40.0 34.1 75.0 46.9 33.3 50.0 40.0 number 95.8 100.0 97.9 95.5 100.0 97.7 95.7 100.0 97.8 95.5 100.0 97.7 pages 99.2 98.0 98.6 95.3 96.5 95.9 98.8 98.2 98.5 95.3 96.5 95.9 publisher 72.8 83.8 77.9 68.4 78.8 73.2 73.2 83.3 77.9 68.4 78.8 73.2 techtitle 93.2 95.3 94.3 75.0 75.0 75.0 93.1 93.1 93.1 75.0 75.0 75.0 title 98.7 95.5 97.1 88.5 87.2 87.9 99.2 96.6 97.9 95.0 93.6 94.3 volume 92.5 75.4 83.1 91.8 76.3 83.3 96.1 81.7 88.3 95.9 79.7 87.0 all 93.4 93.3 93.3 83.3 84.9 84.1 94.3 94.4 94.4 88.3 90.0 89.2 macro average 84.0 87.2 84.7 73.8 76.3 74.7 85.4 89.6 86.6 78.6 81.6 79.8 whole instance 45.8 45.8 65.5 61.3 5.1 Tokenizing a citation The \ufb01rst step of transforming the given string of characters that make up a citation into a string of symbols is to tokenize the citation.", "metadata": {"source": "data/23_erik_2008_HMM.pdf", "page_number": 3, "chunk": 0, "title": ""}}, "171": {"idx": 171, "content": "We consider as a separate token any character in the Unicode punctuation category [8], with the addition of few additional characters, including \u201c+\u201d, \u201c|\u201d, and \u201c\u02dc\u201d, which are not considering punctuation in the Unicode standard. We further consider as a token any string of characters not previously matched, separated from its neighbors by whitespace. At the end of this step we have a token sequence, \u27e8w1, . . . , wn\u27e9. 5.2 Processing tokens into symbols For each of our emission symbols we de\ufb01ne one or more regular expressions which will can be used to map a token to a symbol. For instance, the regular expression ^[Jj]an (uary)? is one regular expression de\ufb01ned for the month symbol. We apply, in order of precedence, this sequence of regular expressions to each token from the token sequence previously obtained, giving us the symbol sequence: \u27e8x1, . . . , xn\u27e9. Because our mapping from tokens to symbols is one way, we must retain our original token sequence in order to later zip together the states with the original token. 5.3 Viterbi At this stage we can pass our sequence of symbols \u27e8x1, . . . , xn\u27e9and our model, as built above, to our implementation of the Viterbi algorithm. We use the Python bindings of the GHMM [1] library for this step. The result is the sequence of states, \u27e8q1, . . . , qn\u27e9, most likely to have produced the symbol sequence which represents our citation.", "metadata": {"source": "data/23_erik_2008_HMM.pdf", "page_number": 3, "chunk": 1, "title": ""}}, "172": {"idx": 172, "content": "5.4 Joining the citation \ufb01elds Now we can zip the list of states obtained above with our original token sequence, obtaining a sequence of state-token tuples, \u27e8(q1, w1), . . . , (qn, wn)\u27e9. These states can be trivially joined into labeled \ufb01elds by joining each token in the list with the previous one, if the states represent the same label; that is, are either a \ufb01rst or rest state for that label. These tokens can be joined together with whitespace to obtain each labeled \ufb01eld string. 6. SOME RESULTS We tested our method against the Cora dataset of labeled citations [16, 15], after reworking the data somewhat to \ufb01t our model. These changes include: breaking the author and editor \ufb01elds into separate \ufb01elds for each person; moving some separator tokens between labeled states; correcting some errors in labeling; and converting some text which was formerly marked as a note into labeled \ufb01elds. We made these changes to every entry in the set, and trained from the \ufb01rst 350 entries, testing against the \ufb01nal 142 entries. We measured the per-\ufb01eld and per-token precision and recall for each label. The results of this measurement are in Table 1. The per-token measurements test the precision and recall of each token marked with a label state. The per-\ufb01eld measurements test the precision and recall for each joined and labeled \ufb01eld.", "metadata": {"source": "data/23_erik_2008_HMM.pdf", "page_number": 3, "chunk": 2, "title": ""}}, "173": {"idx": 173, "content": "Because accurate recovery of punctuation present in the original citation may or may not be useful for a given task, we also measured the per-token and per-\ufb01eld performance after stripping out punctuation. For the per-token measure, this meant ignoring all punctuation tokens when measuring performance. For the per-\ufb01eld measure, after grouping the results into labeled \ufb01elds, we dropped any leading or trailing punctuation before considering if \ufb01elds were a match. Since extra or missing punctuation at the beginning or end of a \ufb01eld is a common error in our method, this improves the performance with what may be a tolerable loss in accuracy. At the bottom are the macro average, the total for all \ufb01elds, and the special \u201cwhole instance\u201d measure. Whole instance measures the percentage of references whose machinelabeled tokens or \ufb01elds completely match the hand-labeled 282 .", "metadata": {"source": "data/23_erik_2008_HMM.pdf", "page_number": 3, "chunk": 3, "title": ""}}, "174": {"idx": 174, "content": "reference. With non-punctuation stripped data the whole instance accuracy is the same per-token and per-\ufb01eld. With the stripped data it is higher in the per-token comparison, because in this comparison we ignore some non-equal punctuation which might be used to split \ufb01elds. It is worth noting that Cora dataset is a very heterogeneous set of citation data, and that preliminary results with this method tested against the more homogeneous health sciences dataset used in [7] give whole instance accuracy greater than 90%. While these results are inferior to those obtained by the use of conditional random \ufb01elds in [18], we feel that there may still be a place for HMM-based methods for citation parsing, due to their relative ease of use, availability of implementations, and possibility for improvement. 7. IMPROVEMENTS It is certain that the symbol alphabet and the corresponding regular expressions used to map from tokens to symbols are suboptimal. Techniques could be used to optimize this mapping. We suspect that the use of domain knowledge in mapping between tokens and symbols would aid in accuracy. For example, dividing words into common words and proper names might improve labeling of authors and titles. Our mapping from \ufb01eld labels to model states is likewise crude. It would be useful to exploit some of the methods described in [20] to build better models from the training data.", "metadata": {"source": "data/23_erik_2008_HMM.pdf", "page_number": 4, "chunk": 0, "title": ""}}, "175": {"idx": 175, "content": "The employment of a second-order HMM, and a suitably extended Viterbi algorithm would likely also be an improvement on the accuracy of this method. 8. CONCLUSION We have shown that it is possible to achieve good results in citation metadata extraction using hidden Markov models, through a careful selection of states and symbols, reducing the size of the symbol alphabet, using two states for each label, and making use of separator states. The simple nature of this method and the easy availability of Viterbi algorithm implementations should allow digital library implementers to employ this method in their systems. Thanks to the anonymous reviewers for their valuable comments. The code used to generate the results in this paper may be found at: http://purl.net/net/egh/hmm-citation-parser/ 9. REFERENCES [1] Algorithmics group. Max Planck Institute for Molecular Genetics. GHMM: A LGPL-licensed hidden markov model library. http://ghmm.sourceforge.net/, 2008. [2] D. Besagni and A. Bela\u00a8\u0131d. Citation recognition for scienti\ufb01c publications in digital libraries. In Proc. of the First Intl. Workshop on Document Image Analysis for Libraries, pages 244\u2013252. IEEE Computer Society, 2004. [3] D. Bikel, S. Miller, R. Schwartz, and R. Weischedel. Nymble: a high-performance learning name-\ufb01nder. In Proc. of the 5th Conf. on Applied Natural Language Processing, pages 194\u2013201, Washington, D.C., 1997. [4] V. R. Borkar, K. Deshmukh, and S. Sarawagi.", "metadata": {"source": "data/23_erik_2008_HMM.pdf", "page_number": 4, "chunk": 1, "title": ""}}, "176": {"idx": 176, "content": "Automatic segmentation of text into structured records. In Proc. of the 2001 ACM SIGMOD Intl. Conf. on Management of Data, pages 175\u2013186, 2001. [5] E. Charniak. Statistical language learning. MIT Press, Cambridge, Mass., 1999. [6] J. Connan and C. Omlin. Bibliography extraction with hidden markov models. Technical Report US-CS-TR-00-6, Department of Computer Science, University of Stellenbosch, Feb. 2000. [7] E. Cortez, A. S. da Silva, M. A. Gon\u00b8calves, F. Mesquita, and E. S. de Moura. FLUX-CiM: \ufb02exible unsupervised extraction of citation metadata. In Proc. of the 7th ACM/IEEE Joint Conf. on Digital Libraries, pages 215\u2013224, Vancouver, BC, Canada, 2007. ACM. [8] M. Davis and K. Whistler. Unicode character database. http://www.unicode.org/Public/UNIDATA/UCD.html, 2008. [9] M.-Y. Day, R. T.-H. Tsai, C.-L. Sung, C.-C. Hsieh, C.-W. Lee, S.-H. Wu, K.-P. Wu, C.-S. Ong, and W.-L. Hsu. Reference metadata extraction using a hierarchical knowledge representation framework. Decision Support Systems, 43:152\u2013167, Feb. 2007. [10] K. Dejonghe. Algorithm::Viterbi. http://search.cpan.org/ ~koen/Algorithm-Viterbi-0.01/lib/Algorithm/Viterbi.pm, Nov. 2006. [11] J. Geng and J. Yang. AUTOBIB: Automatic extraction of bibliographic information on the web. In Proc. of the Intl. Database Engineering and Applications Symposium, pages 193\u2013204. IEEE Computer Society, 2004. [12] M. Kr\u00a8amer, H. Kaprykowsky, D. Keysers, and T. Breuel.", "metadata": {"source": "data/23_erik_2008_HMM.pdf", "page_number": 4, "chunk": 2, "title": ""}}, "177": {"idx": 177, "content": "Bibliographic meta-data extraction using probabilistic \ufb01nite state transducers. In Proc. of the Ninth Intl. Conf. on Document Analysis and Recognition, pages 609\u2013613. IEEE Computer Society, 2007. [13] S. Lawrence, C. L. Giles, and K. Bollacker. Digital libraries and autonomous citation indexing. IEEE Computer, 32:67\u201371, 1999. [14] T. R. Leek. Information extraction using hidden Markov models. Masters, University of California, San Diego, 1997. [15] A. McCallum. Andrew McCallum\u2019s code and data. http://www.cs.umass.edu/~mccallum/code-data.html, 2005. [16] A. McCallum, K. Nigam, J. Rennie, and K. Seymore. A machine learning approach to building domain-speci\ufb01c search engines. In The 16th Intl. Joint Conf. on Arti\ufb01cial Intelligence, 1999. [17] R. A. Milowski. jHMM. http://www.milowski.com/software/jhmm/, 2005. [18] F. Peng and A. McCallum. Accurate information extraction from research papers using conditional random \ufb01elds. In Proc. of Human Language Technology Conf. and North American Chapter of the Association for Computational Linguistics, 2004. [19] M. Perrow and D. Barber. Tagging of name records for genealogical data browsing. In Proc. of the 6th ACM/IEEE-CS Joint Conf. on Digital Libraries, pages 316\u2013325, Chapel Hill, NC, USA, 2006. ACM. [20] K. Seymore, A. McCallum, and R. Rosenfeld. Learning hidden markov model structure for information extraction. In Workshop on Machine Learning for Information Extraction, 1999. [21] A. Viterbi.", "metadata": {"source": "data/23_erik_2008_HMM.pdf", "page_number": 4, "chunk": 3, "title": ""}}, "178": {"idx": 178, "content": "A personal history of the viterbi algorithm. IEEE Signal Processing Magazine, 23:120\u2013142, 2006. [22] P. Yin, M. Zhang, Z. Deng, and D. Yang. Metadata extraction from bibliographies using bigram HMM. In Proc. of the 7th Intl. Conf. on Asian Digital Libraries, LCNS 3334, pages 310\u2013319, 2004. 283 .", "metadata": {"source": "data/23_erik_2008_HMM.pdf", "page_number": 4, "chunk": 4, "title": ""}}, "179": {"idx": 179, "content": "A Benchmark and Evaluation for Text Extraction from PDF Hannah Bast University of Freiburg 79110 Freiburg, Germany bast@cs.uni-freiburg.de Claudius Korzen University of Freiburg 79110 Freiburg, Germany korzen@cs.uni-freiburg.de ABSTRACT Extracting the body text from a PDF document is an important but surprisingly di\ufb03cult task. Te reason is that PDF is a layout-based format which speci\ufb01es the fonts and positions of the individual characters rather than the semantic units of the text (e.g., words or paragraphs) and their role in the document (e.g., body text or caption). Tere is an abundance of extraction tools, but their quality and the range of their functionality are hard to determine. In this paper, we show how to construct a high-quality benchmark of principally arbitrary size from parallel TeX and PDF data. We construct such a benchmark of 12,098 scienti\ufb01c articles from arXiv.org and make it publicly available. We establish a set of criteria for a clean and independent assessment of the semantic abilities of a given extraction tool. We provide an extensive evaluation of 14 state-of-the-art tools for text extraction from PDF on our benchmark according to our criteria. We include our own method, Icecite, which signi\ufb01cantly outperforms all other tools, but is still not perfect. We outline the remaining steps necessary to \ufb01nally make text extraction from PDF a \u201csolved problem\u201d.", "metadata": {"source": "data/7_bast_2017_abenchmark.pdf", "page_number": 1, "chunk": 0, "title": "A Benchmark and Evaluation for Text Extraction from PDF"}}, "180": {"idx": 180, "content": "KEYWORDS Text Extraction, PDF, Benchmark, Evaluation ACM Reference format: Hannah Bast and Claudius Korzen. 2017. A Benchmark and Evaluation for Text Extraction from PDF. In Proceedings of Joint Conference On Digital Libraries, Toronto, Ontario, Canada, June 2017 (JCDL\u201917), 10 pages. DOI: 10.1145/nnnnnnn.nnnnnnn 1 INTRODUCTION PDF continues to be one of the most popular electronic document formats. Google alone currently indexes over 3 billion PDF documents, more than for any other document format except HTML. Unfortunately, PDF is a layout-based format: it speci\ufb01es the positions and fonts of the individual characters, of which the text is composed; see Figure 1 for an example. Many applications require instead information about the semantic building blocks of the text (e.g., the words and the division into paragraphs and sections) and their semantic roles (e.g, whether a piece of text is part of the body text or of a footnote or of a caption). Tis semantic information is usually1 not provided as part of the PDF. 1PDF documents can be tagged with semantic information, but such tags are rarely provided, and almost never on the level needed for typical applications. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro\ufb01t or commercial advantage and that copies bear this notice and the full citation on the \ufb01rst page.", "metadata": {"source": "data/7_bast_2017_abenchmark.pdf", "page_number": 1, "chunk": 1, "title": "A Benchmark and Evaluation for Text Extraction from PDF"}}, "181": {"idx": 181, "content": "Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). JCDL\u201917, Toronto, Ontario, Canada \u00a9 2017 Copyright held by the owner/author(s). 978-x-xxxx-xxxx-x/YY/MM...$15.00 DOI: 10.1145/nnnnnnn.nnnnnnn 1.1 Kinds of semantic information In the following, we brie\ufb02y describe the kind of semantic information that we investigate in this paper. Word identi\ufb01cation. Tis is crucial for applications like search: a word that has not been identi\ufb01ed correctly will not be found. Word identi\ufb01cation in a PDF is non-trivial and challenging for a number of reasons. Te spacing between leters can vary from A Benchmark and Evaluation for Text Extraction from PDF Hannah Bast University of Freiburg 79110 Freiburg, Germany bast@cs.uni-freiburg.de Claudius Korzen University of Freiburg 79110 Freiburg, Germany korzen@cs.uni-freiburg.de ABSTRACT Extracting the body text from a PDF document is an important but surprisingly di\ufb03cult task. Te reason is that PDF is a layout-based format which speci\ufb01es the fonts and positions of the individual characters rather than the semantic units of the text (e.g., words or paragraphs) and their role in the document (e.g., body text or caption). Tere is an abundance of extraction tools, but their quality and the range of their functionality are hard to determine. In this paper, we show how to construct a high-quality benchmark of principally arbitrary size from parallel TeX and PDF data.", "metadata": {"source": "data/7_bast_2017_abenchmark.pdf", "page_number": 1, "chunk": 2, "title": "A Benchmark and Evaluation for Text Extraction from PDF"}}, "182": {"idx": 182, "content": "We construct such a benchmark of 12,098 scienti\ufb01c articles from arXiv.org and make it publicly available. We establish a set of criteria for a clean and independent assessment of the semantic abilities of a given extraction tool. We provide an extensive evaluation of 14 state-of-the-art tools for text extraction from PDF on our benchmark according to our criteria. We include our own method, Icecite, which signi\ufb01cantly outperforms all other tools, but is still not perfect. We outline the remaining steps necessary to \ufb01nally make text extraction from PDF a \u201csolved problem\u201d. KEYWORDS Text Extraction, PDF, Benchmark, Evaluation ACM Reference format: Hannah Bast and Claudius Korzen. 2017. A Benchmark and Evaluation for Text Extraction from PDF. In Proceedings of Joint Conference On Digital Libraries, Toronto, Ontario, Canada, June 2017 (JCDL\u201917), 10 pages. DOI: 10.1145/nnnnnnn.nnnnnnn 1 INTRODUCTION PDF continues to be one of the most popular electronic document formats. Google alone currently indexes over 3 billion PDF documents, more than for any other document format except HTML. Unfortunately, PDF is a layout-based format: it speci\ufb01es the positions and fonts of the individual characters, of which the text is composed; see Figure 1 for an example.", "metadata": {"source": "data/7_bast_2017_abenchmark.pdf", "page_number": 1, "chunk": 3, "title": "A Benchmark and Evaluation for Text Extraction from PDF"}}, "183": {"idx": 183, "content": "Many applications require instead information about the semantic building blocks of the text (e.g., the words and the division into paragraphs and sections) and their semantic roles (e.g, whether a piece of text is part of the body text or of a footnote or of a caption). Tis semantic information is usually1 not provided as part of the PDF. 1PDF documents can be tagged with semantic information, but such tags are rarely provided, and almost never on the level needed for typical applications. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro\ufb01t or commercial advantage and that copies bear this notice and the full citation on the \ufb01rst page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). JCDL\u201917, Toronto, Ontario, Canada \u00a9 2017 Copyright held by the owner/author(s). 978-x-xxxx-xxxx-x/YY/MM...$15.00 DOI: 10.1145/nnnnnnn.nnnnnnn 1.1 Kinds of semantic information In the following, we brie\ufb02y describe the kind of semantic information that we investigate in this paper. Word identi\ufb01cation. Tis is crucial for applications like search: a word that has not been been identi\ufb01ed correctly will not be found. Word identi\ufb01cation in a PDF is non-trivial and challenging for a number of reasons.", "metadata": {"source": "data/7_bast_2017_abenchmark.pdf", "page_number": 1, "chunk": 4, "title": "A Benchmark and Evaluation for Text Extraction from PDF"}}, "184": {"idx": 184, "content": "Te spacing between leters can vary from A Benchmark and Evaluation for Text Extraction from PDF Hannah Bast University of Freiburg 79110 Freiburg, Germany bast@cs.uni-freiburg.de Claudius Korzen University of Freiburg 79110 Freiburg, Germany korzen@cs.uni-freiburg.de ABSTRACT Extracting the body text from a PDF document is an important but surprisingly di\ufb03cult task. Te reason is that PDF is a layout-based format which speci\ufb01es the fonts and positions of the individual characters rather than the semantic units of the text (e.g., words or paragraphs) and their role in the document (e.g., body text or caption). Tere is an abundance of extraction tools, but their quality and the range of their functionality are hard to determine. In this paper, we show how to construct a high-quality benchmark of principally arbitrary size from parallel TeX and PDF data. We construct such a benchmark of 12,098 scienti\ufb01c articles from arXiv.org and make it publicly available. We establish a set of criteria for a clean and independent assessment of the semantic abilities of a given extraction tool. We provide an extensive evaluation of 14 state-of-the-art tools for text extraction from PDF on our benchmark according to our criteria. We include our own method, Icecite, which signi\ufb01cantly outperforms all other tools, but is still not perfect. We outline the remaining steps necessary to \ufb01nally make text extraction from PDF a \u201csolved problem\u201d.", "metadata": {"source": "data/7_bast_2017_abenchmark.pdf", "page_number": 1, "chunk": 5, "title": "A Benchmark and Evaluation for Text Extraction from PDF"}}, "185": {"idx": 185, "content": "KEYWORDS Text Extraction, PDF, Benchmark, Evaluation ACM Reference format: Hannah Bast and Claudius Korzen. 2017. A Benchmark and Evaluation for Text Extraction from PDF. In Proceedings of Joint Conference On Digital Libraries, Toronto, Ontario, Canada, June 2017 (JCDL\u201917), 10 pages. DOI: 10.1145/nnnnnnn.nnnnnnn 1 INTRODUCTION PDF continues to be one of the most popular electronic document formats. Google alone currently indexes over 3 billion PDF documents, more than for any other document format except HTML. Unfortunately, PDF is a layout-based format: it speci\ufb01es the positions and fonts of the individual characters, of which the text is composed; see Figure 1 for an example. Many applications require instead information about the semantic building blocks of the text (e.g., the words and the division into paragraphs and sections) and their semantic roles (e.g, whether a piece of text is part of the body text or of a footnote or of a caption). Tis semantic information is usually1 not provided as part of the PDF. 1PDF documents can be tagged with semantic information, but such tags are rarely provided, and almost never on the level needed for typical applications. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro\ufb01t or commercial advantage and that copies bear this notice and the full citation on the \ufb01rst page.", "metadata": {"source": "data/7_bast_2017_abenchmark.pdf", "page_number": 1, "chunk": 6, "title": "A Benchmark and Evaluation for Text Extraction from PDF"}}, "186": {"idx": 186, "content": "Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). JCDL\u201917, Toronto, Ontario, Canada \u00a9 2017 Copyright held by the owner/author(s). 978-x-xxxx-xxxx-x/YY/MM...$15.00 DOI: 10.1145/nnnnnnn.nnnnnnn 1.1 Kinds of semantic information In the following, we brie\ufb02y describe the kind of semantic information that we investigate in this paper. Word identi\ufb01cation. Tis is crucial for applications like search: a word that has not been been identi\ufb01ed correctly will not be found. Word identi\ufb01cation in a PDF is non-trivial and challenging for a number of reasons. Te spacing between leters can vary from A Benchmark and Evaluation for Text Extraction from PDF Hannah Bast University of Freiburg 79110 Freiburg, Germany bast@cs.uni-freiburg.de Claudius Korzen University of Freiburg 79110 Freiburg, Germany korzen@cs.uni-freiburg.de ABSTRACT Extracting the body text from a PDF document is an important but surprisingly di\ufb03cult task. Te reason is that PDF is a layout-based format which speci\ufb01es the fonts and positions of the individual characters rather than the semantic units of the text (e.g., words or paragraphs) and their role in the document (e.g., body text or caption). Tere is an abundance of extraction tools, but their quality and the range of their functionality are hard to determine. In this paper, we show how to construct a high-quality benchmark of principally arbitrary size from parallel TeX and PDF data.", "metadata": {"source": "data/7_bast_2017_abenchmark.pdf", "page_number": 1, "chunk": 7, "title": "A Benchmark and Evaluation for Text Extraction from PDF"}}, "187": {"idx": 187, "content": "We construct such a benchmark of 12,098 scienti\ufb01c articles from arXiv.org and make it publicly available. We establish a set of criteria for a clean and independent assessment of the semantic abilities of a given extraction tool. We provide an extensive evaluation of 14 state-of-the-art tools for text extraction from PDF on our benchmark according to our criteria. We include our own method, Icecite, which signi\ufb01cantly outperforms all other tools, but is still not perfect. We outline the remaining steps necessary to \ufb01nally make text extraction from PDF a \u201csolved problem\u201d. KEYWORDS Text Extraction, PDF, Benchmark, Evaluation ACM Reference format: Hannah Bast and Claudius Korzen. 2017. A Benchmark and Evaluation for Text Extraction from PDF. In Proceedings of Joint Conference On Digital Libraries, Toronto, Ontario, Canada, June 2017 (JCDL\u201917), 10 pages. DOI: 10.1145/nnnnnnn.nnnnnnn 1 INTRODUCTION PDF continues to be one of the most popular electronic document formats. Google alone currently indexes over 3 billion PDF documents, more than for any other document format except HTML. Unfortunately, PDF is a layout-based format: it speci\ufb01es the positions and fonts of the individual characters, of which the text is composed; see Figure 1 for an example.", "metadata": {"source": "data/7_bast_2017_abenchmark.pdf", "page_number": 1, "chunk": 8, "title": "A Benchmark and Evaluation for Text Extraction from PDF"}}, "188": {"idx": 188, "content": "Many applications require instead information about the semantic building blocks of the text (e.g., the words and the division into paragraphs and sections) and their semantic roles (e.g, whether a piece of text is part of the body text or of a footnote or of a caption). Tis semantic information is usually1 not provided as part of the PDF. 1PDF documents can be tagged with semantic information, but such tags are rarely provided, and almost never on the level needed for typical applications. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro\ufb01t or commercial advantage and that copies bear this notice and the full citation on the \ufb01rst page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). JCDL\u201917, Toronto, Ontario, Canada \u00a9 2017 Copyright held by the owner/author(s). 978-x-xxxx-xxxx-x/YY/MM...$15.00 DOI: 10.1145/nnnnnnn.nnnnnnn 1.1 Kinds of semantic information In the following, we brie\ufb02y describe the kind of semantic information that we investigate in this paper. Word identi\ufb01cation. Tis is crucial for applications like search: a word that has not been been identi\ufb01ed correctly will not be found. Word identi\ufb01cation in a PDF is non-trivial and challenging for a number of reasons.", "metadata": {"source": "data/7_bast_2017_abenchmark.pdf", "page_number": 1, "chunk": 9, "title": "A Benchmark and Evaluation for Text Extraction from PDF"}}, "189": {"idx": 189, "content": "Te spacing between leters can vary from A Benchmark and Evaluation for Text Extraction from PDF Hannah Bast University of Freiburg 79110 Freiburg, Germany bast@cs.uni-freiburg.de Claudius Korzen University of Freiburg 79110 Freiburg, Germany korzen@cs.uni-freiburg.de ABSTRACT Extracting the body text from a PDF document is an important but surprisingly di\ufb03cult task. Te reason is that PDF is a layout-based format which speci\ufb01es the fonts and positions of the individual characters rather than the semantic units of the text (e.g., words or paragraphs) and their role in the document (e.g., body text or caption). Tere is an abundance of extraction tools, but their quality and the range of their functionality are hard to determine. In this paper, we show how to construct a high-quality benchmark of principally arbitrary size from parallel TeX and PDF data. We construct such a benchmark of 12,098 scienti\ufb01c articles from arXiv.org and make it publicly available. We establish a set of criteria for a clean and independent assessment of the semantic abilities of a given extraction tool. We provide an extensive evaluation of 14 state-of-the-art tools for text extraction from PDF on our benchmark according to our criteria. We include our own method, Icecite, which signi\ufb01cantly outperforms all other tools, but is still not perfect. We outline the remaining steps necessary to \ufb01nally make text extraction from PDF a \u201csolved problem\u201d.", "metadata": {"source": "data/7_bast_2017_abenchmark.pdf", "page_number": 1, "chunk": 10, "title": "A Benchmark and Evaluation for Text Extraction from PDF"}}, "190": {"idx": 190, "content": "KEYWORDS Text Extraction, PDF, Benchmark, Evaluation ACM Reference format: Hannah Bast and Claudius Korzen. 2017. A Benchmark and Evaluation for Text Extraction from PDF. In Proceedings of Joint Conference On Digital Libraries, Toronto, Ontario, Canada, June 2017 (JCDL\u201917), 10 pages. DOI: 10.1145/nnnnnnn.nnnnnnn 1 INTRODUCTION PDF continues to be one of the most popular electronic document formats. Google alone currently indexes over 3 billion PDF documents, more than for any other document format except HTML. Unfortunately, PDF is a layout-based format: it speci\ufb01es the positions and fonts of the individual characters, of which the text is composed; see Figure 1 for an example. Many applications require instead information about the semantic building blocks of the text (e.g., the words and the division into paragraphs and sections) and their semantic roles (e.g, whether a piece of text is part of the body text or of a footnote or of a caption). Tis semantic information is usually1 not provided as part of the PDF. 1PDF documents can be tagged with semantic information, but such tags are rarely provided, and almost never on the level needed for typical applications. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro\ufb01t or commercial advantage and that copies bear this notice and the full citation on the \ufb01rst page.", "metadata": {"source": "data/7_bast_2017_abenchmark.pdf", "page_number": 1, "chunk": 11, "title": "A Benchmark and Evaluation for Text Extraction from PDF"}}, "191": {"idx": 191, "content": "Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). JCDL\u201917, Toronto, Ontario, Canada \u00a9 2017 Copyright held by the owner/author(s). 978-x-xxxx-xxxx-x/YY/MM...$15.00 DOI: 10.1145/nnnnnnn.nnnnnnn 1.1 Kinds of semantic information In the following, we brie\ufb02y describe the kind of semantic information that we investigate in this paper. Word identi\ufb01cation. Tis is crucial for applications like search: a word that has not been been identi\ufb01ed correctly will not be found. Word identi\ufb01cation in a PDF is non-trivial and challenging for a number of reasons. Te spacing between leters can vary from A Benchmark and Evaluation for Text Extraction from PDF Hannah Bast University of Freiburg 79110 Freiburg, Germany bast@cs.uni-freiburg.de Claudius Korzen University of Freiburg 79110 Freiburg, Germany korzen@cs.uni-freiburg.de ABSTRACT Extracting the body text from a PDF document is an important but surprisingly di\ufb03cult task. Te reason is that PDF is a layout-based format which speci\ufb01es the fonts and positions of the individual characters rather than the semantic units of the text (e.g., words or paragraphs) and their role in the document (e.g., body text or caption). Tere is an abundance of extraction tools, but their quality and the range of their functionality are hard to determine. In this paper, we show how to construct a high-quality benchmark of principally arbitrary size from parallel TeX and PDF data.", "metadata": {"source": "data/7_bast_2017_abenchmark.pdf", "page_number": 1, "chunk": 12, "title": "A Benchmark and Evaluation for Text Extraction from PDF"}}, "192": {"idx": 192, "content": "We construct such a benchmark of 12,098 scienti\ufb01c articles from arXiv.org and make it publicly available. We establish a set of criteria for a clean and independent assessment of the semantic abilities of a given extraction tool. We provide an extensive evaluation of 14 state-of-the-art tools for text extraction from PDF on our benchmark according to our criteria. We include our own method, Icecite, which signi\ufb01cantly outperforms all other tools, but is still not perfect. We outline the remaining steps necessary to \ufb01nally make text extraction from PDF a \u201csolved problem\u201d. KEYWORDS Text Extraction, PDF, Benchmark, Evaluation ACM Reference format: Hannah Bast and Claudius Korzen. 2017. A Benchmark and Evaluation for Text Extraction from PDF. In Proceedings of Joint Conference On Digital Libraries, Toronto, Ontario, Canada, June 2017 (JCDL\u201917), 10 pages. DOI: 10.1145/nnnnnnn.nnnnnnn 1 INTRODUCTION PDF continues to be one of the most popular electronic document formats. Google alone currently indexes over 3 billion PDF documents, more than for any other document format except HTML. Unfortunately, PDF is a layout-based format: it speci\ufb01es the positions and fonts of the individual characters, of which the text is composed; see Figure 1 for an example.", "metadata": {"source": "data/7_bast_2017_abenchmark.pdf", "page_number": 1, "chunk": 13, "title": "A Benchmark and Evaluation for Text Extraction from PDF"}}, "193": {"idx": 193, "content": "Many applications require instead information about the semantic building blocks of the text (e.g., the words and the division into paragraphs and sections) and their semantic roles (e.g, whether a piece of text is part of the body text or of a footnote or of a caption). Tis semantic information is usually1 not provided as part of the PDF. 1PDF documents can be tagged with semantic information, but such tags are rarely provided, and almost never on the level needed for typical applications. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro\ufb01t or commercial advantage and that copies bear this notice and the full citation on the \ufb01rst page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). JCDL\u201917, Toronto, Ontario, Canada \u00a9 2017 Copyright held by the owner/author(s). 978-x-xxxx-xxxx-x/YY/MM...$15.00 DOI: 10.1145/nnnnnnn.nnnnnnn 1.1 Kinds of semantic information In the following, we brie\ufb02y describe the kind of semantic information that we investigate in this paper. Word identi\ufb01cation. Tis is crucial for applications like search: a word that has not been been identi\ufb01ed correctly will not be found. Word identi\ufb01cation in a PDF is non-trivial and challenging for a number of reasons.", "metadata": {"source": "data/7_bast_2017_abenchmark.pdf", "page_number": 1, "chunk": 14, "title": "A Benchmark and Evaluation for Text Extraction from PDF"}}, "194": {"idx": 194, "content": "Te spacing between leters can vary from A Benchmark and Evaluation for Text Extraction from PDF Hannah Bast University of Freiburg 79110 Freiburg, Germany bast@cs.uni-freiburg.de Claudius Korzen University of Freiburg 79110 Freiburg, Germany korzen@cs.uni-freiburg.de ABSTRACT Extracting the body text from a PDF document is an important but surprisingly di\ufb03cult task. Te reason is that PDF is a layout-based format which speci\ufb01es the fonts and positions of the individual characters rather than the semantic units of the text (e.g., words or paragraphs) and their role in the document (e.g., body text or caption). Tere is an abundance of extraction tools, but their quality and the range of their functionality are hard to determine. In this paper, we show how to construct a high-quality benchmark of principally arbitrary size from parallel TeX and PDF data. We construct such a benchmark of 12,098 scienti\ufb01c articles from arXiv.org and make it publicly available. We establish a set of criteria for a clean and independent assessment of the semantic abilities of a given extraction tool. We provide an extensive evaluation of 14 state-of-the-art tools for text extraction from PDF on our benchmark according to our criteria. We include our own method, Icecite, which signi\ufb01cantly outperforms all other tools, but is still not perfect. We outline the remaining steps necessary to \ufb01nally make text extraction from PDF a \u201csolved problem\u201d.", "metadata": {"source": "data/7_bast_2017_abenchmark.pdf", "page_number": 1, "chunk": 15, "title": "A Benchmark and Evaluation for Text Extraction from PDF"}}, "195": {"idx": 195, "content": "KEYWORDS Text Extraction, PDF, Benchmark, Evaluation ACM Reference format: Hannah Bast and Claudius Korzen. 2017. A Benchmark and Evaluation for Text Extraction from PDF. In Proceedings of Joint Conference On Digital Libraries, Toronto, Ontario, Canada, June 2017 (JCDL\u201917), 10 pages. DOI: 10.1145/nnnnnnn.nnnnnnn 1 INTRODUCTION PDF continues to be one of the most popular electronic document formats. Google alone currently indexes over 3 billion PDF documents, more than for any other document format except HTML. Unfortunately, PDF is a layout-based format: it speci\ufb01es the positions and fonts of the individual characters, of which the text is composed; see Figure 1 for an example. Many applications require instead information about the semantic building blocks of the text (e.g., the words and the division into paragraphs and sections) and their semantic roles (e.g, whether a piece of text is part of the body text or of a footnote or of a caption). Tis semantic information is usually1 not provided as part of the PDF. 1PDF documents can be tagged with semantic information, but such tags are rarely provided, and almost never on the level needed for typical applications. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro\ufb01t or commercial advantage and that copies bear this notice and the full citation on the \ufb01rst page.", "metadata": {"source": "data/7_bast_2017_abenchmark.pdf", "page_number": 1, "chunk": 16, "title": "A Benchmark and Evaluation for Text Extraction from PDF"}}, "196": {"idx": 196, "content": "Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). JCDL\u201917, Toronto, Ontario, Canada \u00a9 2017 Copyright held by the owner/author(s). 978-x-xxxx-xxxx-x/YY/MM...$15.00 DOI: 10.1145/nnnnnnn.nnnnnnn 1.1 Kinds of semantic information In the following, we brie\ufb02y describe the kind of semantic information that we investigate in this paper. Word identi\ufb01cation. Tis is crucial for applications like search: a word that has not been been identi\ufb01ed correctly will not be found. Word identi\ufb01cation in a PDF is non-trivial and challenging for a number of reasons. Te spacing between leters can vary from A Benchmark and Evaluation for Text Extraction from PDF Hannah Bast University of Freiburg 79110 Freiburg, Germany bast@cs.uni-freiburg.de Claudius Korzen University of Freiburg 79110 Freiburg, Germany korzen@cs.uni-freiburg.de ABSTRACT Extracting the body text from a PDF document is an important but surprisingly di\ufb03cult task. Te reason is that PDF is a layout-based format which speci\ufb01es the fonts and positions of the individual characters rather than the semantic units of the text (e.g., words or paragraphs) and their role in the document (e.g., body text or caption). Tere is an abundance of extraction tools, but their quality and the range of their functionality are hard to determine. In this paper, we show how to construct a high-quality benchmark of principally arbitrary size from parallel TeX and PDF data.", "metadata": {"source": "data/7_bast_2017_abenchmark.pdf", "page_number": 1, "chunk": 17, "title": "A Benchmark and Evaluation for Text Extraction from PDF"}}, "197": {"idx": 197, "content": "We construct such a benchmark of 12,098 scienti\ufb01c articles from arXiv.org and make it publicly available. We establish a set of criteria for a clean and independent assessment of the semantic abilities of a given extraction tool. We provide an extensive evaluation of 14 state-of-the-art tools for text extraction from PDF on our benchmark according to our criteria. We include our own method, Icecite, which signi\ufb01cantly outperforms all other tools, but is still not perfect. We outline the remaining steps necessary to \ufb01nally make text extraction from PDF a \u201csolved problem\u201d. KEYWORDS Text Extraction, PDF, Benchmark, Evaluation ACM Reference format: Hannah Bast and Claudius Korzen. 2017. A Benchmark and Evaluation for Text Extraction from PDF. In Proceedings of Joint Conference On Digital Libraries, Toronto, Ontario, Canada, June 2017 (JCDL\u201917), 10 pages. DOI: 10.1145/nnnnnnn.nnnnnnn 1 INTRODUCTION PDF continues to be one of the most popular electronic document formats. Google alone currently indexes over 3 billion PDF documents, more than for any other document format except HTML. Unfortunately, PDF is a layout-based format: it speci\ufb01es the positions and fonts of the individual characters, of which the text is composed; see Figure 1 for an example.", "metadata": {"source": "data/7_bast_2017_abenchmark.pdf", "page_number": 1, "chunk": 18, "title": "A Benchmark and Evaluation for Text Extraction from PDF"}}, "198": {"idx": 198, "content": "Many applications require instead information about the semantic building blocks of the text (e.g., the words and the division into paragraphs and sections) and their semantic roles (e.g, whether a piece of text is part of the body text or of a footnote or of a caption). Tis semantic information is usually1 not provided as part of the PDF. 1PDF documents can be tagged with semantic information, but such tags are rarely provided, and almost never on the level needed for typical applications. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro\ufb01t or commercial advantage and that copies bear this notice and the full citation on the \ufb01rst page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). JCDL\u201917, Toronto, Ontario, Canada \u00a9 2017 Copyright held by the owner/author(s). 978-x-xxxx-xxxx-x/YY/MM...$15.00 DOI: 10.1145/nnnnnnn.nnnnnnn 1.1 Kinds of semantic information In the following, we brie\ufb02y describe the kind of semantic information that we investigate in this paper. Word identi\ufb01cation. Tis is crucial for applications like search: a word that has not been been identi\ufb01ed correctly will not be found. Word identi\ufb01cation in a PDF is non-trivial and challenging for a number of reasons.", "metadata": {"source": "data/7_bast_2017_abenchmark.pdf", "page_number": 1, "chunk": 19, "title": "A Benchmark and Evaluation for Text Extraction from PDF"}}, "199": {"idx": 199, "content": "Te spacing between leters can vary from A Benchmark and Evaluation for Text Extraction from PDF Hannah Bast University of Freiburg 79110 Freiburg, Germany bast@cs.uni-freiburg.de Claudius Korzen University of Freiburg 79110 Freiburg, Germany korzen@cs.uni-freiburg.de ABSTRACT Extracting the body text from a PDF document is an important but surprisingly di\ufb03cult task. Te reason is that PDF is a layout-based format which speci\ufb01es the fonts and positions of the individual characters rather than the semantic units of the text (e.g., words or paragraphs) and their role in the document (e.g., body text or caption). Tere is an abundance of extraction tools, but their quality and the range of their functionality are hard to determine. In this paper, we show how to construct a high-quality benchmark of principally arbitrary size from parallel TeX and PDF data. We construct such a benchmark of 12,098 scienti\ufb01c articles from arXiv.org and make it publicly available. We establish a set of criteria for a clean and independent assessment of the semantic abilities of a given extraction tool. We provide an extensive evaluation of 14 state-of-the-art tools for text extraction from PDF on our benchmark according to our criteria. We include our own method, Icecite, which signi\ufb01cantly outperforms all other tools, but is still not perfect. We outline the remaining steps necessary to \ufb01nally make text extraction from PDF a \u201csolved problem\u201d.", "metadata": {"source": "data/7_bast_2017_abenchmark.pdf", "page_number": 1, "chunk": 20, "title": "A Benchmark and Evaluation for Text Extraction from PDF"}}, "200": {"idx": 200, "content": "KEYWORDS Text Extraction, PDF, Benchmark, Evaluation ACM Reference format: Hannah Bast and Claudius Korzen. 2017. A Benchmark and Evaluation for Text Extraction from PDF. In Proceedings of Joint Conference On Digital Libraries, Toronto, Ontario, Canada, June 2017 (JCDL\u201917), 10 pages. DOI: 10.1145/nnnnnnn.nnnnnnn 1 INTRODUCTION PDF continues to be one of the most popular electronic document formats. Google alone currently indexes over 3 billion PDF documents, more than for any other document format except HTML. Unfortunately, PDF is a layout-based format: it speci\ufb01es the positions and fonts of the individual characters, of which the text is composed; see Figure 1 for an example. Many applications require instead information about the semantic building blocks of the text (e.g., the words and the division into paragraphs and sections) and their semantic roles (e.g, whether a piece of text is part of the body text or of a footnote or of a caption). Tis semantic information is usually1 not provided as part of the PDF. 1PDF documents can be tagged with semantic information, but such tags are rarely provided, and almost never on the level needed for typical applications. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro\ufb01t or commercial advantage and that copies bear this notice and the full citation on the \ufb01rst page.", "metadata": {"source": "data/7_bast_2017_abenchmark.pdf", "page_number": 1, "chunk": 21, "title": "A Benchmark and Evaluation for Text Extraction from PDF"}}, "201": {"idx": 201, "content": "Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). JCDL\u201917, Toronto, Ontario, Canada \u00a9 2017 Copyright held by the owner/author(s). 978-x-xxxx-xxxx-x/YY/MM...$15.00 DOI: 10.1145/nnnnnnn.nnnnnnn 1.1 Kinds of semantic information In the following, we brie\ufb02y describe the kind of semantic information that we investigate in this paper. Word identi\ufb01cation. Tis is crucial for applications like search: a word that has not been been identi\ufb01ed correctly will not be found. Word identi\ufb01cation in a PDF is non-trivial and challenging for a number of reasons. Te spacing between leters can vary from A Benchmark and Evaluation for Text Extraction from PDF Hannah Bast University of Freiburg 79110 Freiburg, Germany bast@cs.uni-freiburg.de Claudius Korzen University of Freiburg 79110 Freiburg, Germany korzen@cs.uni-freiburg.de ABSTRACT Extracting the body text from a PDF document is an important but surprisingly di\ufb03cult task. Te reason is that PDF is a layout-based format which speci\ufb01es the fonts and positions of the individual characters rather than the semantic units of the text (e.g., words or paragraphs) and their role in the document (e.g., body text or caption). Tere is an abundance of extraction tools, but their quality and the range of their functionality are hard to determine. In this paper, we show how to construct a high-quality benchmark of principally arbitrary size from parallel TeX and PDF data.", "metadata": {"source": "data/7_bast_2017_abenchmark.pdf", "page_number": 1, "chunk": 22, "title": "A Benchmark and Evaluation for Text Extraction from PDF"}}, "202": {"idx": 202, "content": "We construct such a benchmark of 12,098 scienti\ufb01c articles from arXiv.org and make it publicly available. We establish a set of criteria for a clean and independent assessment of the semantic abilities of a given extraction tool. We provide an extensive evaluation of 14 state-of-the-art tools for text extraction from PDF on our benchmark according to our criteria. We include our own method, Icecite, which signi\ufb01cantly outperforms all other tools, but is still not perfect. We outline the remaining steps necessary to \ufb01nally make text extraction from PDF a \u201csolved problem\u201d. KEYWORDS Text Extraction, PDF, Benchmark, Evaluation ACM Reference format: Hannah Bast and Claudius Korzen. 2017. A Benchmark and Evaluation for Text Extraction from PDF. In Proceedings of Joint Conference On Digital Libraries, Toronto, Ontario, Canada, June 2017 (JCDL\u201917), 10 pages. DOI: 10.1145/nnnnnnn.nnnnnnn 1 INTRODUCTION PDF continues to be one of the most popular electronic document formats. Google alone currently indexes over 3 billion PDF documents, more than for any other document format except HTML. Unfortunately, PDF is a layout-based format: it speci\ufb01es the positions and fonts of the individual characters, of which the text is composed; see Figure ? for an example.", "metadata": {"source": "data/7_bast_2017_abenchmark.pdf", "page_number": 1, "chunk": 23, "title": "A Benchmark and Evaluation for Text Extraction from PDF"}}, "203": {"idx": 203, "content": "Many applications require instead information about the semantic building blocks of the text (e.g., the words and the division into paragraphs and sections) and their semantic roles (e.g, whether a piece of text is part of the body text or of a footnote or of a caption). Tis semantic information is usually1 not provided as part of the PDF. 1PDF documents can be tagged with semantic information, but such tags are rarely provided, and almost never on the level needed for typical applications. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for pro\ufb01t or commercial advantage and that copies bear this notice and the full citation on the \ufb01rst page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). JCDL\u201917, Toronto, Ontario, Canada \u00a9 2017 Copyright held by the owner/author(s). 978-x-xxxx-xxxx-x/YY/MM...$15.00 DOI: 10.1145/nnnnnnn.nnnnnnn 1.1 Kinds of semantic information In the following, we brie\ufb02y describe the kind of semantic information that we investigate in this paper. Word identi\ufb01cation. Tis is crucial for applications like search: a word that has not been been identi\ufb01ed correctly will not be found. Word identi\ufb01cation in a PDF is non-trivial and challenging for a number of reasons.", "metadata": {"source": "data/7_bast_2017_abenchmark.pdf", "page_number": 1, "chunk": 24, "title": "A Benchmark and Evaluation for Text Extraction from PDF"}}, "204": {"idx": 204, "content": "Te spacing between leters can vary from line to line and even within a line, and there is no \ufb01xed rule to determine the extent of a word from the spacing alone.2 Long words can he hyphenated (especially frequent in formats with two or more columns), in which case they appear \u201cbroken\u201d in two parts at di\ufb00erent positions in the PDF. Words can contain ligatures (like \ufb03or T, see Figure 1), which are one character in the PDF but actually translate to multiple characters in the text. Words can also contain characters with diacritics (like `a or \u02dca), which are ofen two characters in the PDF but translate to a single character in the text. Word order. Determining the correct reading order of the words is crucial for re\ufb02ow applications, where the text is cast in a di\ufb00erent format (with di\ufb00erent font or page sizes). Re\ufb02ow is important for e-book readers or small devices, or when one simply wants or needs the text in raw text format. Word order can also be important in search, when proximity information is needed. Te order of the words within a line are easy to derive from the positions of the words in the PDF. However, the order between lines is much less clear. For example, PDFs with a two-column layout of the text ofen contain the lines in an order interleaving between the two columns. If text is output in that order \u2014 as indeed done by simple extraction tools \u2014 it is, of course, quite unreadable. Paragraph boundaries.", "metadata": {"source": "data/7_bast_2017_abenchmark.pdf", "page_number": 1, "chunk": 25, "title": "A Benchmark and Evaluation for Text Extraction from PDF"}}, "205": {"idx": 205, "content": "Deriving the beginning and end of a paragraph is again crucial for re\ufb02ow applications or when reading the text in plain text format.3 Tis task is even more challenging than word identi\ufb01cation and word order. Text from the same paragraph can be interrupted by a formula or a \ufb01gure, but still belong to the same paragraph; for example, this is the case for the paragraph interrupted by Figure 1 in Figure 1. Similarly, text from the same paragraph may end at the botom of one page or column and continue on the next page or column. But these same interruptions can also mark a real break between two paragraphs. Semantic roles. Te text elements in a PDF play di\ufb00erent semantic roles. For the purpose of this paper, we distinguish between 16 roles, including: title, body text, formulas, \ufb01gures; a complete list is given in Section 3.2. For re\ufb02ow applications, it is particular important to distinguish the body text from the rest. For a targeted search application, it might also be useful to know whether a particular word occurs in the body text or in the caption of a \ufb01gure. 2In Figure 1, the boxes of the characters within one word are directly adjacent; this is not the case for all PDF documents. But note the closeness of the boxes in of Joint in the text passage afer the abstract. 3For example, a wrong paragraph break ofen breaks a sentence apart. Figure 1: A page from a PDF document with bounding boxes around each character.", "metadata": {"source": "data/7_bast_2017_abenchmark.pdf", "page_number": 1, "chunk": 26, "title": "A Benchmark and Evaluation for Text Extraction from PDF"}}, "206": {"idx": 206, "content": "For some interesting places to look at, see the references to this \ufb01gure in the text. line to line and even within a line, and there is no \ufb01xed rule to determine the extent of a word from the spacing alone.2 Long words can he hyphenated (especially frequent in formats with two or more columns), in which case they appear \u201cbroken\u201d in two parts at di\ufb00erent positions in the PDF. Words can contain ligatures (like 2In Figure 1, the boxes of the characters within one word are directly adjacent; this is not the case for all PDF documents. But note the closeness of the boxes in of Joint in the text passage afer the abstract. Figure 1: A page from a PDF document with bounding boxes around each character. For some interesting places to look at, see the references to this \ufb01gure in the text. line to line and even within a line, and there is no \ufb01xed rule to determine the extent of a word from the spacing alone.2 Long words can he hyphenated (especially frequent in formats with two or more columns), in which case they appear \u201cbroken\u201d in two parts at di\ufb00erent positions in the PDF. Words can contain ligatures (like 2In Figure 1, the boxes of the characters within one word are directly adjacent; this is not the case for all PDF documents. But note the closeness of the boxes in of Joint in the text passage afer the abstract. Figure 1: A page from a PDF document with bounding boxes around each character. For some interesting places to look at, see the references to this \ufb01gure in the text.", "metadata": {"source": "data/7_bast_2017_abenchmark.pdf", "page_number": 1, "chunk": 27, "title": "A Benchmark and Evaluation for Text Extraction from PDF"}}, "207": {"idx": 207, "content": "line to line and even within a line, and there is no \ufb01xed rule to determine the extent of a word from the spacing alone.2 Long words can he hyphenated (especially frequent in formats with two or more columns), in which case they appear \u201cbroken\u201d in two parts at di\ufb00erent positions in the PDF. Words can contain ligatures (like 2In Figure 1, the boxes of the characters within one word are directly adjacent; this is not the case for all PDF documents. But note the closeness of the boxes in of Joint in the text passage afer the abstract. Figure 1: A page from a PDF document with bounding boxes around each character. For some interesting places to look at, see the references to this \ufb01gure in the text. line to line and even within a line, and there is no \ufb01xed rule to determine the extent of a word from the spacing alone.2 Long words can he hyphenated (especially frequent in formats with two or more columns), in which case they appear \u201cbroken\u201d in two parts at di\ufb00erent positions in the PDF. Words can contain ligatures (like 2In Figure 1, the boxes of the characters within one word are directly adjacent; this is not the case for all PDF documents. But note the closeness of the boxes in of Joint in the text passage afer the abstract. Figure 1: A page from a PDF document with bounding boxes around each character. For some interesting places to look at, see the references to this \ufb01gure in the text.", "metadata": {"source": "data/7_bast_2017_abenchmark.pdf", "page_number": 1, "chunk": 28, "title": "A Benchmark and Evaluation for Text Extraction from PDF"}}, "208": {"idx": 208, "content": "line to line and even within a line, and there is no \ufb01xed rule to determine the extent of a word from the spacing alone.2 Long words can he hyphenated (especially frequent in formats with two or more columns), in which case they appear \u201cbroken\u201d in two parts at di\ufb00erent positions in the PDF. Words can contain ligatures (like 2In Figure 1, the boxes of the characters within one word are directly adjacent; this is not the case for all PDF documents. But note the closeness of the boxes in of Joint in the text passage afer the abstract. Figure 1: A page from a PDF document with bounding boxes around each character. For some interesting places to look at, see the references to this \ufb01gure in the text. line to line and even within a line, and there is no \ufb01xed rule to determine the extent of a word from the spacing alone.2 Long words can he hyphenated (especially frequent in formats with two or more columns), in which case they appear \u201cbroken\u201d in two parts at di\ufb00erent positions in the PDF. Words can contain ligatures (like 2In Figure 1, the boxes of the characters within one word are directly adjacent; this is not the case for all PDF documents. But note the closeness of the boxes in of Joint in the text passage afer the abstract. Figure 1: A page from a PDF document with bounding boxes around each character. For some interesting places to look at, see the references to this \ufb01gure in the text.", "metadata": {"source": "data/7_bast_2017_abenchmark.pdf", "page_number": 1, "chunk": 29, "title": "A Benchmark and Evaluation for Text Extraction from PDF"}}, "209": {"idx": 209, "content": "line to line and even within a line, and there is no \ufb01xed rule to determine the extent of a word from the spacing alone.2 Long words can he hyphenated (especially frequent in formats with two or more columns), in which case they appear \u201cbroken\u201d in two parts at di\ufb00erent positions in the PDF. Words can contain ligatures (like 2In Figure 1, the boxes of the characters within one word are directly adjacent; this is not the case for all PDF documents. But note the closeness of the boxes in of Joint in the text passage afer the abstract. Figure 1: A page from a PDF document with bounding boxes around each character. For some interesting places to look at, see the references to this \ufb01gure in the text. line to line and even within a line, and there is no \ufb01xed rule to determine the extent of a word from the spacing alone.2 Long words can he hyphenated (especially frequent in formats with two or more columns), in which case they appear \u201cbroken\u201d in two parts at di\ufb00erent positions in the PDF. Words can contain ligatures (like 2In Figure 1, the boxes of the characters within one word are directly adjacent; this is not the case for all PDF documents. But note the closeness of the boxes in of Joint in the text passage afer the abstract. Figure 1: A page from a PDF document with bounding boxes around each character. For some interesting places to look at, see the references to this \ufb01gure in the text.", "metadata": {"source": "data/7_bast_2017_abenchmark.pdf", "page_number": 1, "chunk": 30, "title": "A Benchmark and Evaluation for Text Extraction from PDF"}}, "210": {"idx": 210, "content": "line to line and even within a line, and there is no \ufb01xed rule to determine the extent of a word from the spacing alone.2 Long words can he hyphenated (especially frequent in formats with two or more columns), in which case they appear \u201cbroken\u201d in two parts at di\ufb00erent positions in the PDF. Words can contain ligatures (like 2In Figure 1, the boxes of the characters within one word are directly adjacent; this is not the case for all PDF documents. But note the closeness of the boxes in of Joint in the text passage afer the abstract. .", "metadata": {"source": "data/7_bast_2017_abenchmark.pdf", "page_number": 1, "chunk": 31, "title": "A Benchmark and Evaluation for Text Extraction from PDF"}}, "211": {"idx": 211, "content": "\ufb03or T, see Figure 1), which are one character in the PDF but actually translate to multiple characters in the text. Words can also contain characters with diacritics (like `a or \u02dca), which are ofen two characters in the PDF but translate to a single character in the text. Word order. Determining the correct reading order of the words is crucial for re\ufb02ow applications, where the text is cast in a di\ufb00erent format (with di\ufb00erent font or page sizes). Re\ufb02ow is important for e-book readers or small devices, or when one simply wants or needs the text in raw text format. Word order can also be important in search, when proximity information is needed. Te order of the words within a line are easy to derive from the positions of the words in the PDF. However, the order between lines is much less clear. For example, PDFs with a two-column layout of the text ofen contain the lines in an order interleaving between the two columns. If text is output in that order \u2014 as indeed done by simple extraction tools \u2014 it is, of course, quite unreadable. Paragraph boundaries. Deriving the beginning and end of a paragraph is again crucial for re\ufb02ow applications or when reading the text in plain text format.3 Tis task is even more challenging than word identi\ufb01cation and word order. Text from the same paragraph can be interrupted by a formula or a \ufb01gure, but still belong to the same paragraph; for example, this is the case for the paragraph interrupted by Figure 1 in Figure 1.", "metadata": {"source": "data/7_bast_2017_abenchmark.pdf", "page_number": 2, "chunk": 0, "title": "A Benchmark and Evaluation for Text Extraction from PDF"}}, "212": {"idx": 212, "content": "Similarly, text from the same paragraph may end at the botom of one page or column and continue on the next page or column. But these same interruptions can also mark a real break between two paragraphs. Semantic roles. Te text elements in a PDF play di\ufb00erent semantic roles. For the purpose of this paper, we distinguish between 16 roles, including: title, body text, formulas, \ufb01gures; a complete list is given in Section 3.2. For re\ufb02ow applications, it is particular important to distinguish the body text from the rest. For a targeted search application, it might also be useful to know whether a particular word occurs in the body text or in the caption of a \ufb01gure. 1.2 Existing tools A large number of tools for text extraction from PDF exist. A Google query for text extraction from PDF provides countless hits with tools or pages recommending tools for this task. Te variety is confusing and there does not seem to be a clear winner. Most tools do not specify for which of the aspects above they are actually useful. All of the tools do word identi\ufb01cation and consider word order (they wouldn\u2019t be of much use if they didn\u2019t). Only the more sophisticated tools provide paragraph boundaries and semantic roles. So far, there has been no rigorous benchmark for this problem and no comprehensive evaluation of existing systems. Tis is surprising, given the practical importance of the problem, but it also hints at the complexity.", "metadata": {"source": "data/7_bast_2017_abenchmark.pdf", "page_number": 2, "chunk": 1, "title": "A Benchmark and Evaluation for Text Extraction from PDF"}}, "213": {"idx": 213, "content": "Bringing some clarity and order into this jungle has been the main motivation behind this paper. 1.3 Contributions Tis paper is about an extensive evaluation of existing PDF extraction tools, and about the non-trivial task of constructing a benchmark and developing meaningful criteria for carrying out such an evaluation. We consider the following as our main contributions. 3For example, a wrong paragraph break ofen breaks a sentence apart. \u2022 We describe how to construct a high-quality benchmark of principally arbitrary size from parallel TeX and PDF data (that is, for each TeX \ufb01le, the PDF produced from it). Te main component of this construction is a special-purpose TeX parser that can identify the logical text blocks of a document. \u2022 Using this mechanism, we construct a benchmark of 12,098 scienti\ufb01c articles from arXiv. Te articles were selected to represent a variety of topics and creation times (and thus formats) as wide as possible. Te benchmark and all our code is publicly available under htps://github.com/ckorzen/arxiv-benchmark. \u2022 We establish a set of criteria that allows for a clean assessment of a given extraction tool with respect to the aspects described in Section 1.1. Establishing and measuring these criteria independently turned out to be a challenging problem; see Section 4.3. \u2022 We provide an extensive evaluation of 14 state-of-the-art tools for text extraction from PDF on our benchmark according to our criteria.", "metadata": {"source": "data/7_bast_2017_abenchmark.pdf", "page_number": 2, "chunk": 2, "title": "A Benchmark and Evaluation for Text Extraction from PDF"}}, "214": {"idx": 214, "content": "For each tool, we provide a concise description of its main mechanism and of its strengths and weaknesses. We include our own method, Icecite, which signi\ufb01cantly outperforms all other systems, but is still not perfect. \u2022 We discuss the remaining steps necessary to build a fully satisfactory tool for text extraction from PDF. 2 RELATED WORK Tere are some related datasets which are used in various \ufb01elds of document analysis in order to train machine learning models or to evaluate the quality of obtained results. Te typical use cases are (1) dividing document pages into columns and blocks, known as page segmentation, (2) identifying the reading order of blocks in a page, (3) identifying the semantic roles of blocks, known as block classi\ufb01cation, (4) extracting speci\ufb01c blocks, known as metadata or information extraction, (5) extracting metadata from reference strings, known as reference extraction. We distinguish the datasets into three groups, each of them di\ufb00ering in the granularity of the provided data. First, datasets with metadata only, which usually provide data like titles, authors, abstracts or citations of a speci\ufb01c set of scienti\ufb01c articles. Second, datasets with unstructured full texts, which additionally provide the full texts of articles, but with no or only litle semantic markup.", "metadata": {"source": "data/7_bast_2017_abenchmark.pdf", "page_number": 2, "chunk": 3, "title": "A Benchmark and Evaluation for Text Extraction from PDF"}}, "215": {"idx": 215, "content": "Tird, datasets with structured full texts, which provide the full texts enriched with semantic markups that identify text blocks with their semantic roles and their positions in the outline hierarchy. 2.1 Datasets with metadata only Te DBLP dataset [19] provides bibliographic metadata (title, author(s), publication year, journal, volume, etc.) of about 3.7 million computer science articles. Te data are given as records in a single XML \ufb01le, where each record includes the metadata of a single article. Most of the records also include an external link that points to a PDF of the related article or to a page where the PDF can be found. Te data are highly accurate because the \ufb01nal step in the data curation pipeline is manual. Te Cora Information Extraction dataset [22] is split into two subsets. Te \ufb01rst subset includes titles, authors, a\ufb03liations and authors extracted from the headers of 935 computer science articles. Te second subset includes titles, authors, journals and volumes 2 .", "metadata": {"source": "data/7_bast_2017_abenchmark.pdf", "page_number": 2, "chunk": 4, "title": "A Benchmark and Evaluation for Text Extraction from PDF"}}, "216": {"idx": 216, "content": "extracted from 500 citation strings. Both subsets were extracted from PDF \ufb01les by several machine learning techniques. Te UMass Citation Field Extraction Dataset [1] consists of 1,829 manually labeled citation strings which originate from 1,200 articles from arXiv.org. It gives both coarse-grained labels (like authors, title, venue, date, etc.) and \ufb01ne-grained labels (e.g., booktitle, address, volume, etc. of a venue) for each citation string. Te Marmot datasets [14] provide not only bibliographic metadata but also (1) tables extracted from 2,000 PDF pages and (2) 9,500 formulas along with their bounding boxes, characters and graphics extracted from 194 PDF \ufb01les using Conditional Random Fields. Lipinski et al. [20] compiled a dataset consisting of metadata (title, authors, abstract and publication year) of 1,153 random articles from arXiv.org. Te dataset was used to evaluate the performance of seven PDF extraction tools, with respect to their accuracies on extracting the metadata from scienti\ufb01c articles. 2.2 Datasets with unstructured full texts Te CiteSeerX dataset [6] provides full bibliographic metadata and the full texts of approximately 6 million scienti\ufb01c articles, extracted from PDF \ufb01les using Support Vector Machines. Te data are given as XML \ufb01les, where each XML \ufb01le belongs to a single scienti\ufb01c article. Tey include speci\ufb01c markups in order to distinguish different blocks like titles, abstracts, authors, venues, references, etc.", "metadata": {"source": "data/7_bast_2017_abenchmark.pdf", "page_number": 3, "chunk": 0, "title": "A Benchmark and Evaluation for Text Extraction from PDF"}}, "217": {"idx": 217, "content": "However, the full texts themselves are given as unstructured continuous texts and do not allow to distinguish between any blocks or to identify the outline hierarchy. 2.3 Datasets with structured full texts Te Grotoap dataset [27] consists of 113 articles taken from the Directory of Open Access Journals (DOAJ). For each article, it provides the hierarchical structure of pages, blocks, lines, words and characters in XML \ufb01les. Basically, the data were extracted from PDF \ufb01les using Hidden Markov Models and corrected by human experts aferwards in order to get full and reliable data. Obviously, this approach does not scale to larger collections. Te follow-up dataset, Grotoap2 [28], consists of 13,210 articles taken from the Open Access Subset of PubMed Central [30] and provides the hierarchical structures of scienti\ufb01c articles in XML \ufb01les as before. Te data are extracted by a series of supervised and unsupervised machine learning algorithms, see [29]. Again, the extraction process is followed by a manual review step, but limited to a small random sample of articles, in order to identify common problems and to develop heuristics to correct them. Te ACL Anthology Reference Corpus [4] provides (1) metadata like the title, the author(s), the publication venue and the publication year, (2) the full texts, broken down into hierarchical blocks and (3) the parsed references of 22,878 articles from the ACL Anthology4. Te data acquisition is split into two steps.", "metadata": {"source": "data/7_bast_2017_abenchmark.pdf", "page_number": 3, "chunk": 1, "title": "A Benchmark and Evaluation for Text Extraction from PDF"}}, "218": {"idx": 218, "content": "In the \ufb01rst step, characters, words, lines, blocks, etc. are extracted from PDF \ufb01les using an OCR sofware (Nuance Omnipage). In the second step, the extracted data are post-processed by the tool ParsCit (which we will evaluate in Section 4) in order to obtain semantic information. PubMed Central [30] and BioMed Central [25] are the most extensive datasets in this group. Tey provide full bibliographic metadata, 4htp://aclweb.org/anthology/ the hierarchical structures of full texts (with sections, headings and paragraphs), \ufb01gures and tables. However, these data are publicly available only for a small subset of their archived articles. Usually, the data are either served by the publishers or extracted directly from the PDF \ufb01les, followed by an extensive manual review process in order to correct any extraction errors. However, the details of the underlying extraction techniques are neither published nor publicly accessible. Further, the articles of an archive ofen originate from a well-de\ufb01ned set of publishers and thus exhibit a homogeneous structure which greatly facilitates the extraction process and the ability to provide extensive data. Most of the datasets introduced in Sections 2.2 and 2.3 were derived directly from PDF \ufb01les. Hence, without manual reviewing, the problems outlined in Section 1.1 are inevitably solved imperfectly and are indeed a frequent source of errors.", "metadata": {"source": "data/7_bast_2017_abenchmark.pdf", "page_number": 3, "chunk": 2, "title": "A Benchmark and Evaluation for Text Extraction from PDF"}}, "219": {"idx": 219, "content": "In contrast, TeX is a markup language that provides semantic information like word boundaries, paragraph boundaries and semantic roles explicitly. Tus, TeX \ufb01les (with the PDFs built from them) are much more suitable to create high-quality benchmarks. Tose benchmarks are eligible for applications based not only on TeX-born PDF \ufb01les, but also on all digitally-born PDFs (e.g., created by Microsof Word) and even on image-based PDFs, as long as they were processed by any OCR sofware that identi\ufb01ed the characters, their bounding boxes and their fonts accurately. Te reason is that the listed PDF types do not show any type speci\ufb01c di\ufb00erences in the structure of their logical text blocks. 3 OUR BENCHMARK GENERATION Tis section is about the generation of a PDF extraction benchmark from TeX \ufb01les of scienti\ufb01c articles, divided into the following three steps: (1) parse TeX \ufb01les syntactically in order to identify and model the hierarchies of their TeX elements, see Section 3.1; (2) identify the logical text blocks (LTBs) from TeX elements using rules, see Section 3.2; (3) serialize the LTBs to \ufb01les, see Section 3.3. 3.1 Parsing TeX \ufb01les TeX5 is a language that allows to build statements using macros. Given a TeX \ufb01le, the goal of this step is to model these statements by a syntax tree representing the hierarchies of its TeX elements. For an illustration, see the TeX snippet given in Figure 2 (a).", "metadata": {"source": "data/7_bast_2017_abenchmark.pdf", "page_number": 3, "chunk": 3, "title": "A Benchmark and Evaluation for Text Extraction from PDF"}}, "220": {"idx": 220, "content": "We want to compute the syntax tree given in Figure 2 (b), representing the hierarchies of TeX elements afer the expansion of macro calls. We proceed in three steps. First, we introduce a grammar that describes the basic syntax of the TeX language. Second, based on this grammar, we generate a parser in order to build a syntax tree that models the hierarchies of the TeX elements before macro calls were expanded. Tird, we search the syntax tree for macro calls in order to expand them recursively. 3.1.1 The TeX grammar. In this section, we give a slightly simpli\ufb01ed version of our grammar that describes the syntax of the basic TeX elements, in EBNF notation. In fact, the original grammar is a bit more extensive to handle (1) several special syntax cases of widely used plain TeX commands like $, $$ or \\def\\tex{TeX}; 5To be precise, there is a di\ufb00erence between plain TeX and LaTeX. However, we use the term TeX in a generic sense for both types. 3 .", "metadata": {"source": "data/7_bast_2017_abenchmark.pdf", "page_number": 3, "chunk": 4, "title": "A Benchmark and Evaluation for Text Extraction from PDF"}}, "221": {"idx": 221, "content": "\\newcommand{\\tex}{TeX} \\section[s1]{Parsing \\tex.} {\\it \\tex is a language.} \u27e8doc\u27e9 \u27e8cmd\u27e9 \\section \u27e8opt\u27e9 \u27e8text\u27e9 s1 \u27e8arg\u27e9 \u27e8text\u27e9 Parsing TeX. \u27e8group\u27e9 \u27e8cmd\u27e9 \\it \u27e8text\u27e9 TeX is a language. (a) (b) Figure 2: (a) A simple TeX snippet with typical TeX elements. Boilerplate commands like \\documentclass{\u2026} or \\begin{document} were omitted for reasons of brevity. (b) Te syntax tree that represents the hierarchies of the elements in (a), without the macro de\ufb01nition in the \ufb01rst line. (2) any number of whitespaces and newlines within the elements or (3) the starred variants of commands like \\section*{...} or \\begin{figure*}.", "metadata": {"source": "data/7_bast_2017_abenchmark.pdf", "page_number": 4, "chunk": 0, "title": "A Benchmark and Evaluation for Text Extraction from PDF"}}, "222": {"idx": 222, "content": "However, the following grammar does not lack any signi\ufb01cant features and is detailed enough to illustrate the most important aspects of the TeX language: (1) \u27e8doc\u27e9::= ( \u27e8element\u27e9)* (2) \u27e8element\u27e9::= \u27e8group\u27e9| \u27e8cmd\u27e9| \u27e8marker\u27e9| \u27e8comment\u27e9| \u27e8text\u27e9 (3) \u27e8group\u27e9::= \u2019{\u2019 ( \u27e8element\u27e9)* \u2019}\u2019 (4) \u27e8cmd\u27e9::= \u27e8break-cmd\u27e9| \u27e8ctrl-cmd\u27e9| \u27e8symb-cmd\u27e9 (5) \u27e8break-cmd\u27e9::= ( \u2019\\n\u2019 | \u2019\\r\\n\u2019 )+ (6) \u27e8ctrl-cmd\u27e9::= \u2019\\\u2019 ( \u27e8leter\u27e9)+ ( \u27e8arg\u27e9| \u27e8opt\u27e9)* (7) \u27e8symb-cmd\u27e9::= \u2019\\\u2019 \u27e8non-leter\u27e9[ \u27e8arg\u27e9| \u27e8opt\u27e9| \u27e8leter\u27e9] (8) \u27e8arg\u27e9::= \u2019{\u2019 ( \u27e8element\u27e9)* \u2019}\u2019 (9) \u27e8opt\u27e9::= \u2019[\u2019 ( \u27e8element\u27e9)* \u2019]\u2019 (10) \u27e8marker\u27e9::= \u2019#\u2019 \u27e8digit\u27e9 (11) \u27e8comment\u27e9::= \u2019%\u2019 ( \u27e8element\u27e9)* \u27e8break-cmd\u27e9 (12) \u27e8text\u27e9::= ( \u27e8char\u27e9| \u27e8whitespace\u27e9)+ (13) \u27e8char\u27e9::= \u27e8leter\u27e9| \u27e8digit\u27e9| \u27e8non-leter\u27e9 (14) \u27e8whitespace\u27e9::= \u2019 \u2019 | \u2019\\t\u2019 (15) \u27e8leter\u27e9::= [\u2019A\u2019-\u2019Z\u2019, \u2019a\u2019-\u2019z\u2019] (16) \u27e8digit\u27e9::= [\u20190\u2019-\u20199\u2019] (17) \u27e8non-leter\u27e9::= [\u02c6\u2019A\u2019-\u2019Z\u2019, \u2019a\u2019-\u2019z\u2019, \u20190\u2019-\u20199\u2019] Te grammar consists of 17 production rules, where the non-terminal \u27e8doc\u27e9is the start symbol and may expand to any number of TeX elements, see rule (1). A TeX element is either given by a group, a command, a marker, a comment or a text phrase, see rule (2). On commands, we distinguish between break commands, control commands, and symbol commands, see rule (4). A break command describes any kind of a line break. A control command describes a command that follows the \u201cregular\u201d command syntax with potential argument groups and option groups, like \\today, \\section{...} or \\begin{table}[h].", "metadata": {"source": "data/7_bast_2017_abenchmark.pdf", "page_number": 4, "chunk": 1, "title": "A Benchmark and Evaluation for Text Extraction from PDF"}}, "223": {"idx": 223, "content": "A symbol command describes a command that is mainly but not exclusively used to encode a special character, like \\#, \\\\[10pt], \\\"a or \\\"{a}. Further, a marker is a placeholder for an argument group in a macro de\ufb01nition and a comment is a piece of text which we will exclude from further processing. 3.1.2 The generation of the TeX parser. Given the grammar introduced above, the next step is to generate a parser that builds the syntax tree. We use JavaCC, a parser generator that creates LL(k) parsers from given LL(k) grammars. In general, an LL(k) parser is a top-down parser that reads input sequences from lef to right in order to \ufb01nd lefmost derivations in a grammar, starting at the start symbol. At any time, an LL(k) parser looks at k lookahead symbols in the input sequence to decide which production rule to apply, where k is as large as a production rule can be chosen unambiguously. In our case, k = 2, because for the sequence \u2019\\\u2019 (a backslash) of length 1 the parser needs to look at one more symbol to decide which kind of command (\u27e8break-cmd\u27e9, \u27e8ctrl-cmd\u27e9or \u27e8symb-cmd\u27e9) is denoted by the sequence. JavaCC allows to associate each production rule with so-called parser actions, which are in fact Java code snippets that are executed when the production rule was derived. Tey consume series of tokens, which can be seen as associations between substrings in the input sequence and the production rules. We use this mechanism to construct the syntax tree and a macro dictionary.", "metadata": {"source": "data/7_bast_2017_abenchmark.pdf", "page_number": 4, "chunk": 2, "title": "A Benchmark and Evaluation for Text Extraction from PDF"}}, "224": {"idx": 224, "content": "In principle, the constructed syntax tree re\ufb02ects the hierarchy given by the grammar introduced above. It is a rooted and ordered tree, where each node correlates to one of the following production rules: \u27e8doc\u27e9, \u27e8group\u27e9, \u27e8cmd\u27e9, \u27e8arg\u27e9, \u27e8opt\u27e9, \u27e8marker\u27e9or \u27e8text\u27e9. Te DFS order of nodes correlates to the order of the related elements in the TeX \ufb01le. Te \u27e8doc\u27e9, \u27e8group\u27e9, \u27e8arg\u27e9and \u27e8opt\u27e9nodes may have any number of child nodes representing the enclosed elements. \u27e8arg\u27e9and \u27e8opt\u27e9nodes exist only beneath \u27e8cmd\u27e9nodes; \u27e8text\u27e9and \u27e8marker\u27e9nodes do not have any child nodes. Te macro dictionary is a dictionary that holds all macro de\ufb01nitions. Whenever we identify a macro de\ufb01nition (like \\newcommand {\\tex}{TeX}), we insert it with the macro name (\\tex) as the key and the syntax tree that represents the replacement ({TeX}), called replacement tree, as the value. 3.1.3 The expansion of macro calls. Given the syntax tree and the macro dictionary, the last step of the parsing process is to expand the macro calls in the syntax tree recursively. We traverse the syntax tree in DFS order to identify macro calls by looking up the name of each command in the macro dictionary. If a macro call was found, each marker in the associated replacement tree is replaced by the related argument group of the macro call. Aferwards, the subtree in the syntax tree representing the macro call is replaced by the resulting replacement tree.", "metadata": {"source": "data/7_bast_2017_abenchmark.pdf", "page_number": 4, "chunk": 3, "title": "A Benchmark and Evaluation for Text Extraction from PDF"}}, "225": {"idx": 225, "content": "Tis process is done in a recursive fashion in order to identify and expand nested macro calls. 3.2 Identifying logical text blocks Given a syntax tree with expanded macro calls, the next step is to identify the LTBs with one of the following 16 semantic roles: title, author, a\ufb03liation, date, abstract, heading, paragraph of the body text, formula, \ufb01gure, table, caption, listing-item, footnote, acknowledgements, references and appendix. Our procedure is rule-based and is sketched in the algorithm below. For the sake of brevity, a Python-like syntax is used. However, the original code is writen in Java. Te procedure accepts a syntax tree and a dictionary of rules, where each rule de\ufb01nes features for a speci\ufb01c TeX command that give details about how to handle the command on identifying the LTBs. Te output is a list of LTBs, where each LTB has the atributes level (an integer representing its level in the outline hierarchy, which defaults to 0), text (its textual 4 .", "metadata": {"source": "data/7_bast_2017_abenchmark.pdf", "page_number": 4, "chunk": 4, "title": "A Benchmark and Evaluation for Text Extraction from PDF"}}, "226": {"idx": 226, "content": "Algorithm: Te procedure of identifying LTBs using rules Input: tree A syntax tree. rules A dictionary of rules. Output: List of LTBs (the identi\ufb01ed logical text blocks). 1 def identify_blocks(tree, rules): 2 level = 0 # The hierarchy level. 3 itr = dfs_iterator(tree) # DFS order. 4 stack = [LTB(level=level)] # The active LTBs. 5 finished = [] # The finished LTBs. 6 for element in itr: 7 if type(element) is Text: 8 stack[-1].text += element.text 9 if type(element) is Command: 10 rule = rules.get(element) 11 if rule is None: 12 element.args = [] # Do not visit args. 13 element.opts = [] # Do not visit opts. 14 continue 15 if rule.hierarchy_level > 0: 16 level = rule.hierarchy_level 17 if rule.starts_ltb > 1: 18 finished.append(stack.pop()) 19 if rule.starts_ltb > 0: 20 stack.push(LTB(level=level)) 21 if rule.semantic_role is not None: 22 stack[-1].role = rule.semantic_role 23 if rule.text_phrase is not None: 24 stack[-1].text += rule.text_phrase 25 if rule.end_command is not None: 26 itr.skip_to(rule.end_command) 27 for i in len(element.args): 28 if i not in rule.args_to_visit: 29 element.args[i] = None 30 if rule.ends_ltb: 31 finished.append(stack.pop()) 32 # Remove remaining blocks from stack. 33 while len(stack) > 0: 34 finished.append(stack.pop()) 35 return finished content, which defaults to the empty string) and role (its semantic role, which defaults to \u201cbody text\u201d).", "metadata": {"source": "data/7_bast_2017_abenchmark.pdf", "page_number": 5, "chunk": 0, "title": "A Benchmark and Evaluation for Text Extraction from PDF"}}, "227": {"idx": 227, "content": "Te basic idea is to traverse the syntax tree in DFS order (see line 3) and to have a stack of active LTBs, initialized with a single, empty LTB (see line 4) and a list of \ufb01nished LTBs (see line 5). On visiting a node, one or more of the following actions may be triggered, depending on the type of the related TeX element: (A1) Push a new LTB to the stack. (A2) Append a text phrase to the topmost LTB in the stack. (A3) Set the semantic role of the topmost LTB in the stack. (A4) Set the hierarchy level for LTBs to be created subsequently. (A5) Pop the topmost LTB and add it to list of \ufb01nished blocks. (A6) Skip to a given node in the syntax tree. In case of a text, action (A2) is triggered, see line 8. In case of a command, the triggered action(s) depend on the related rule, see lines 9-31. Details about the rules are given in Section 3.2.1. If there is no such rule for a command, the complete subtree de\ufb01ned by the command is ignored (the arguments and options are removed, such that they are not visited by the iterator, see lines 11-14). In case of a group, option or argument, no special action is triggered and the algorithm continues with the next node in DFS order. Once the traversal of the tree is completed, all remaining LTBs in the stack are popped and are added to the list of \ufb01nished LTBs, see lines 33-34. Finally, the list of \ufb01nished LTBs is returned, see line 35. 3.2.1 The rules.", "metadata": {"source": "data/7_bast_2017_abenchmark.pdf", "page_number": 5, "chunk": 1, "title": "A Benchmark and Evaluation for Text Extraction from PDF"}}, "228": {"idx": 228, "content": "Te rules are given as a dictionary of Rule objects, where each Rule gives the following seven features for a referred command: Hierarchy level (hierarchy_level): A digit between 1 and 5. It denotes the level of the section in the outline hierarchy, in case of the command de\ufb01nes a section heading. A higher value means a deeper level. Triggers action (A4) if a value is given, see lines 15-16. Starts new LTB? (starts_ltb): A digit; either 1 or 2, where 1 means: Te command introduces a new LTB (and (A1) is triggered, see lines 19-20); 2 means: Te command ends the LTB and introduces a new one (and (A5) and (A1) are triggered, see lines 17-20). Semantic role (semantic_role): Te semantic role that is induced by the command. If set, action (A3) is triggered, see lines 21-22. Text phrase (text_phrase): A text phrase to append to the current LTB. It is used (1) to de\ufb01ne the text phrase that is in fact encoded by the command (e.g., a special character); or (2) to de\ufb01ne a placeholder for an LTB for which (a) it is unclear from the TeX \ufb01le how it is visualized in the PDF \ufb01le (like a citation produced by e.g. the command \\cite{...}) or (b) there are no standardized ways to serialize it to plain text properly, which is the case for tables, \ufb01gures and formulas. Placeholders are ignored in the evaluation, see Section 4.3.2 for details. If set, (A2) is triggered (lines 23-24).", "metadata": {"source": "data/7_bast_2017_abenchmark.pdf", "page_number": 5, "chunk": 2, "title": "A Benchmark and Evaluation for Text Extraction from PDF"}}, "229": {"idx": 229, "content": "End command (end_command): Te command that denotes the end of the TeX environment (e.g., \\end{table}), in case of the command introduces one (e.g., \\begin{table}). Tis property is needed to skip to the end of the environment, in case of the environment should be replaced by a text phrase, see lines 25-26. Arguments to visit (args_to_visit): List of indices of argument groups to examine. Tis feature is used to decide whether an argument of a command is relevant to the identi\ufb01cation of LTBs or not. For example, the argument {Introduction} in the command \\section{Introduction} is relevant, because it contains textual content of an LTB. In contrast, the argument {5pt} in the command \\vspace{5pt} is not relevant, as it does not a\ufb00ect any properties of an LTB. All arguments, which are not covered by this list, are ignored (are cleared, see lines 27-29). Ends current LTB? (ends_ltb): A boolean that indicates whether the command ends the current LTB. Triggers action (A5) if the value is set to true, see lines 30-31. Overall, our dictionary contains about 1200 rules. Figure 3 gives an excerpt with the values of four concrete Rule objects. Te complete rules are given at htps://github.com/ckorzen/arxiv-benchmark. 3.3 Serializing logical text blocks Given the list of identi\ufb01ed LTBs, the last step is to serialize them to \ufb01les, optionally \ufb01ltered by given semantic roles. Our benchmark 5 .", "metadata": {"source": "data/7_bast_2017_abenchmark.pdf", "page_number": 5, "chunk": 3, "title": "A Benchmark and Evaluation for Text Extraction from PDF"}}, "230": {"idx": 230, "content": "rules[\u201d\\section\u201d] = Rule ( starts ltb = 2, semantic role = \u201dheading\u201d, hierarchy level = 1, ends ltb = true, args to visit = [0], ) rules[\u201d\\footnote\u201d] = Rule ( starts ltb = 1, semantic role = \u201dfootnote\u201d, ends ltb = true, args to visit = [0], ) rules[\u201d\\n\\n\u201d] = Rule ( starts ltb = 2, ends ltb = true, ) rules[\u201d\\%\u201d] = Rule ( text = %, ) Figure 3: Te initialization and indexing of four concrete Rule objects for the commands \\section, \\footnote, \\n\\n and \\%. Each rule is indexed by the name of the referred command and gives features on how to handle the command. For example, the feature starts ltb in the rule for command \\n\\n is 2 (denoting that the command ends the previous LTB and starts a new one), because in TeX \ufb01les, paragraphs are separated by blank lines and we want to identify each paragraph as a single LTB. For more details about the meaning of the individual features, see Section 3.2.1. generator provides the following output formats: plain text, XML and JSON. In case of plain text, the textual contents of the selected LTBs are joined in a \ufb02at way, separated by blank lines and keeping their order in the TeX \ufb01le. In case of XML or JSON, the texts of the LTBs are enriched with descriptive markups, giving their semantic roles and re\ufb02ecting their order in the TeX \ufb01le and their outline hierarchies. 3.4 Common pitfalls In this section, we describe two TeX-speci\ufb01c pitfalls, which can lead to a faulty ground truth if not considered appropriately.", "metadata": {"source": "data/7_bast_2017_abenchmark.pdf", "page_number": 6, "chunk": 0, "title": "A Benchmark and Evaluation for Text Extraction from PDF"}}, "231": {"idx": 231, "content": "First, there may be some LTBs, which are present in the PDF \ufb01le but not directly deducible from the TeX \ufb01le \u2013 either because (1) they are not de\ufb01ned in the TeX \ufb01le but in some supplementary styor cls-\ufb01les of included packages or (2) they are only de\ufb01ned at compile time, e.g. because of conditional macros consisting of \\if and \\else commands. Related examples are page headers, page footers, page numbers or section numberings. All of them won\u2019t be extracted by our benchmark generator. Second, authors occasionally misuse or ignore convenient TeX commands. A common example is the \u201chard coding\u201d of section headers (e.g., the use of {\\large \\bf Introduction} instead of \\section{Introduction}) or citations (e.g., \u2019[2]\u2019 instead of \\cite{foo}). Our rule-based approach is not \ufb02exible enough to handle those cases. It means that, for example, sections like references or appendices may be identi\ufb01ed as part of the body text mistakenly. 3.5 Usage As seen in Section 3.3, our benchmark generator provides built-in options in order to produce various kinds of benchmarks, with individual compositions of LTBs and various output formats. Tus, it is applicable to a wide variety of other applications or evaluations related to document analysis and metadata extraction. Te code of our benchmark generator is publicly available and can be found under the link given above. Tere you will \ufb01nd detailed instructions and examples on how to use and how to customize the generator to personal needs.", "metadata": {"source": "data/7_bast_2017_abenchmark.pdf", "page_number": 6, "chunk": 1, "title": "A Benchmark and Evaluation for Text Extraction from PDF"}}, "232": {"idx": 232, "content": "4 EVALUATION OF CURRENT TOOLS In this section, we evaluate and compare 14 state-of-the-art tools for text extraction from PDF \ufb01les. In Section 4.1, we introduce the evaluated tools, each with a concise description of its main mechanism, strengths and weaknesses. In Section 4.2, we describe our benchmark, which was constructed using the method described in Section 3. In Section 4.3, we describe our evaluation methods, in particular, the criteria we use to assess and compare the semantic abilities of the tools. Section 4.4 provides the evaluation results. 4.1 Te PDF extraction tools We have evaluated the following 14 tools. An overview and comparison of their feature sets is given in Table 1. pdfotext [12] is probably the most familiar PDF extraction tool. It converts any PDF \ufb01les to plain text \ufb01les rapidly, but does not make any e\ufb00ort to identify paragraph boundaries or semantic roles or only the body text. pdfohtml [18] converts a given PDF \ufb01le to XML or HTML, broken down into text lines. It does not identify paragraphs or semantic roles, extracts characters with diacritics as two characters and does not merge hyphenated words. pdfoxml [11] converts a given PDF \ufb01le to XML, broken down into \u201dblocks\u201d (which do not correlate to paragraphs), text lines and words. Ligatures, diacritics and hyphenated words are not handled. PdfBox [2] is a widespread PDF library by Apache that is able to convert a given PDF \ufb01le to plain text.", "metadata": {"source": "data/7_bast_2017_abenchmark.pdf", "page_number": 6, "chunk": 2, "title": "A Benchmark and Evaluation for Text Extraction from PDF"}}, "233": {"idx": 233, "content": "It does not identify paragraph boundaries or semantic roles, but handles ligatures and characters with diacritics. Hyphenated words are not merged. pdf2xml [26] uses Apache Tika (which uses PdfBox under the hood) and pdfotext to extract text from a given PDF \ufb01le. In a postprocessing step, the tool combines the result of both tools in order to improve the identi\ufb01cation of word boundaries. ParsCit [15] does not actually extract text from a PDF \ufb01le but processes the results of third-party tools (like pdfotext) to extract the body text and parse reference strings. Its abilities therefore depend on the utilized third-party tool. In our evaluation, we use pdfotext. LA-PdfText [5] is a tool that focuses on PDF \ufb01les of scienti\ufb01c articles and extracts LTBs based on (user-de\ufb01ned) rules, which must be de\ufb01ned for each di\ufb00erent article layout [23]. However, there are some default rules, which we use in the evaluation. PdfMiner [24] is a tool that is able to analyze the structure of a given PDF \ufb01le and converts it to plain text, XML or HTML, broken down into paragraphs, lines and characters. Ligatures, characters with diacritics and hyphenated words are not handled properly. 6 .", "metadata": {"source": "data/7_bast_2017_abenchmark.pdf", "page_number": 6, "chunk": 3, "title": "A Benchmark and Evaluation for Text Extraction from PDF"}}, "234": {"idx": 234, "content": "System PA OR RO LI DI HY FORMAT pdfotext [12] \u2013 \u27131 \u2013 \u2713 \u2713 \u2713 txt pdfohtml [18] \u2013 \u2713 \u2013 \u2713 \u2013 \u2013 xml, html pdfoxml [11] \u2013 \u2713 \u2013 \u2013 \u2013 \u2013 xml PdfBox [2] \u2013 \u2713 \u2013 \u2713 \u2713 \u2013 txt pdf2xml [26] \u2713 \u27131 \u2013 \u2713 \u2013 \u2713 xml, html ParsCit [15] \u20132 \u20132 \u2713 \u20132 \u20132 \u20132 xml LA-PdfText [5] \u2013 \u27131 \u27133 \u2713 \u2013 \u2013 txt PdfMiner [24] \u2713 \u27131 \u2013 \u2013 \u2013 \u2013 txt, xml, html pdfXtk [13] \u2013 \u2713 \u2013 \u2713 \u2013 \u2013 xml, html pdf-extract [31] \u2013 \u27131 \u2013 \u2713 \u2013 \u2013 xml pdfx [7] \u2013 \u2713 \u2713 \u2713 \u2713 \u2713 xml PDFExtract [3] \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 xml Grobid [21] \u2013 \u2713 \u2713 \u2713 \u2713 \u2713 xml Icecite [17] \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 txt, xml, json Table 1: Overview of the features of 14 PDF extraction tools, broken down into: PA: identi\ufb01cation of paragraph boundaries; OR: identi\ufb01cation of the reading order; RO: identi\ufb01cation of semantic roles; LI: translation of ligatures; DI: extraction of characters with diacritics as single characters; HY: merging of hyphenated words. If a feature is fully provided by a tool, it is denoted by a \u201d\u2713\u201d. A number next to an entry points to one of the following constraints: (1) lines from di\ufb00erent text columns are mixed sometimes; (2) depends on the used 3rd-party tool; (3) depends on the used rules. Te last column FORMAT gives the available output format(s). pdfXtk [13] is built upon PdfBox and converts a given PDF \ufb01le to XML or HTML, broken down into \u201dblocks\u201d (which do not correlate to paragraphs), lines, words and characters. Characters with diacritics and hyphenated words are not handled properly.", "metadata": {"source": "data/7_bast_2017_abenchmark.pdf", "page_number": 7, "chunk": 0, "title": "A Benchmark and Evaluation for Text Extraction from PDF"}}, "235": {"idx": 235, "content": "pdf-extract [31] converts PDF \ufb01les to XML, broken down into \u201dregions\u201d (which do not correlate to paragraphs) and text lines. Its only semantic ability is to distinguish reference sections from nonreference sections and to split them into individual references. pdfx [7] is a rule-based tool that analyzes fonts and layout speci\ufb01cs in order to construct a geometrical model of a PDF \ufb01le and to identify the title, sections, tables, etc. from it [8]. Te sections are broken down into \u201dregions\u201d, which do not correlate to paragraphs. PDFExtract [3] is one of the most powerful tools. It converts PDFs of scienti\ufb01c articles to XML and is able to identify the semantic roles title, abstract, headings and paragraphs. It handles ligatures, characters with diacritics, and hyphenated words. Grobid [21] is another powerful tool that breaks down PDFs into several LTBs, like title, abstract, sections (but not paragraphs), etc. using Conditional Random Fields. Further it is able to handle ligatures, characters with diacritics, and hyphenated words. Icecite [17] is our own tool, which extracts LTBs from scienti\ufb01c articles, with a focus on paragraphs of the body text. In principle, it is based on a rule-based approach that analyzes the distances, positions and fonts of characters, words and text lines. Another focus is the precise extraction of words, including an accurate handling of ligatures, diacritics and hyphenated words.", "metadata": {"source": "data/7_bast_2017_abenchmark.pdf", "page_number": 7, "chunk": 1, "title": "A Benchmark and Evaluation for Text Extraction from PDF"}}, "236": {"idx": 236, "content": "Tere are some other related tools, which were not included in the evaluation, because (1) they are commercial tools (like JPedal6 or PDFlib TET7); (2) their methods and feature sets are very similar to an already included tool (e.g. iText8, which is similar to PdfBox); or (3) they are described in a scienti\ufb01c article, but there are no executables provided [10] or they are not available anymore [16]. 4.2 Te Benchmark Our benchmark consists of 12,098 scienti\ufb01c articles, taken from arXiv.org [9], a digital library that hosts about 1.2 million scienti\ufb01c articles (on topics like physics, mathematics, computer science, biology, \ufb01nance and statistics), indexed by month, beginning from August 1991. For most of them, arXiv provides both, a PDF \ufb01le and the related TeX source \ufb01le(s). From each month, we selected 1% of the articles randomly, resulting in 12,098 articles. Tis sample yields a good variety of topics, creation times and thus formats of the articles from arXiv9. We also tried larger sample sizes, but experienced only minimal variances in our evaluation results (\u00b1 0.5%). For each article, the benchmark contains a ground truth \ufb01le and the related PDF \ufb01le. Each ground truth \ufb01le was generated via the benchmark generator described in Section 3 and contains the title, the section headings and the body text paragraphs of a particular article in plain text format.", "metadata": {"source": "data/7_bast_2017_abenchmark.pdf", "page_number": 7, "chunk": 2, "title": "A Benchmark and Evaluation for Text Extraction from PDF"}}, "237": {"idx": 237, "content": "Te PDF \ufb01les we use are not those provided by arXiv, due to occasional (contentual) mismatches with the corresponding TeX \ufb01les, but we regenerated them from the provided TeX \ufb01les. 4.3 Evaluation methods For each tool, the PDF \ufb01les of the benchmark were processed in batches. We have chosen reasonable input parameters in order to get output \ufb01les that re\ufb02ect, as much as possible, the structure of the ground truth \ufb01les. Te exact parameters for each tool can be found under htps://github.com/ckorzen/arxiv-benchmark. For the tools with XML output, we translated the output to plain text by identifying the relevant text parts. If semantic roles were provided, we only selected those parts that are also present in the ground truth \ufb01les. If texts were broken down into any kind of blocks (like paragraphs, columns, or sections), we have separated them by blank lines (like in the ground truth \ufb01les). Te main purpose of the evaluation was to assess each tool by comparing its output \ufb01les with the ground truth \ufb01les using a set of easily interpretable and independent criteria. Tis was harder than expected, especially the \u201cindependent\u201d part. In the following, we \ufb01rst de\ufb01ne our evaluation criteria and then explain how we compute them (which turned out be non-trivial). 4.3.1 Establishing the evaluation criteria.", "metadata": {"source": "data/7_bast_2017_abenchmark.pdf", "page_number": 7, "chunk": 3, "title": "A Benchmark and Evaluation for Text Extraction from PDF"}}, "238": {"idx": 238, "content": "We are looking for easily interpretable and independent criteria that assess the quality of an output \ufb01le with respect to four aspects: (1) paragraph boundaries, (2) distinction of body text and non-body text, (3) reading 6htp://www.idrsolutions.com/jpedal 7htp://www.pd\ufb02ib.com/products/tet 8htp://www.itextpdf.com/ 9htps://arxiv.org/help/stats/2016 by area/index/ 7 .", "metadata": {"source": "data/7_bast_2017_abenchmark.pdf", "page_number": 7, "chunk": 4, "title": "A Benchmark and Evaluation for Text Extraction from PDF"}}, "239": {"idx": 239, "content": "order, and (4) word boundaries. Independence here means that it should be possible, in principle, to perform well for any subset of criteria but poorly for the others. We eventually came up with three groups of criteria, that measure the di\ufb00erences between an output \ufb01le and the related ground truth \ufb01le. \u2022 Newline di\ufb00erences capture the quality of the detection of paragraph boundaries and are broken down into: \u2013 NL+: the number of spurious newlines in the output \ufb01le. \u2013 NL\u2212: the number of missing newlines in the output \ufb01le. \u2022 Paragraph di\ufb00erences capture the quality of the distinction between body and non-body text and of the reading order. Tey are broken down into: \u2013 P +: the number of spurious paragraphs in the output \ufb01le. \u2013 P \u2212: the number of missing paragraphs in the output \ufb01le. \u2013 P \u2191\u2193: the number of rearranged paragraphs in the output \ufb01le. \u2022 Word di\ufb00erences capture the quality of the recognition of individual words and their boundaries and are broken down into: \u2013 W +: the number of spurious words in the output \ufb01le. \u2013 W \u2212: the number of missing words in the output \ufb01le. \u2013 W \u223c: the number of misspelled words in the output \ufb01le. Tese criteria are indeed easily interpretable and independent. For example, a tool can perform well with respect to W \u223c, if it handles ligatures and hyphenated words properly; but poorly with respect to NL+ and NL\u2212, if it does not identify any paragraph boundaries. 4.3.2 Measuring the evaluation criteria.", "metadata": {"source": "data/7_bast_2017_abenchmark.pdf", "page_number": 8, "chunk": 0, "title": "A Benchmark and Evaluation for Text Extraction from PDF"}}, "240": {"idx": 240, "content": "Te evaluation criteria introduced above are easily interpretable, but measuring them is non-trivial. In particular, for a given output \ufb01le O and ground truth \ufb01le G, there are multiple ways to assign values to these criteria. An example for this is given in Figures 4 and 5. We address this problem by computing an assignment that minimizes Z = (NL++ NL\u2212) + (W ++W \u2212+W \u223c) + c \u00b7 (P++ P\u2212+ P\u2191\u2193), where c \u22651 is a (constant) penalty score, introduced to increase the weight for paragraph di\ufb00erences compared to newline- and word di\ufb00erences. In the evaluation, we use c = 5. In the following, we describe our heuristic algorithm doc-di\ufb00, that \ufb01nds, in most cases, an optimal assignment to the evaluation criteria with minimal Z. Let wO resp. wG be the list of words per paragraph in O resp. G, transformed to lower case and without any punctuation marks. For Figure 4, wO is given by [[text, extraction, pdf ], [a, benchmark, and], [evaluation, for]] and wG is given by [[a, benchmark, and, evaluation, for, text, extraction, from, pdf ]], where each list at index i contains the words of paragraph i. Te approach of doc-di\ufb00is to compare wO and wG wordwise and to classify the di\ufb00erences into the following type of phrases: \u2022 Common phrase (= [word1, \u2026, wordi]): a sequence of i consecutive words which are common to wO and wG.", "metadata": {"source": "data/7_bast_2017_abenchmark.pdf", "page_number": 8, "chunk": 1, "title": "A Benchmark and Evaluation for Text Extraction from PDF"}}, "241": {"idx": 241, "content": "\u2022 Di\ufb00ering phrase (\u223c[word1, \u2026, wordj], [word1, \u2026, wordk]): a sequence of j spurious words, which occur in wO but not in wG; and of k missing words, which occur in wG but not in wO. \u2022 Rearranged phrase (\u2191\u2193[word1, \u2026, wordm], [word1, \u2026, wordn]): a sequence ofm words inwO and n words inwG, which are (almost) equal (m \u2248n), but their positions in wO and wG do not correlate. Te phrases are computed in two rounds. In the \ufb01rst round, the common and di\ufb00ering phrases are computed by an algorithm called output \ufb01le O ground truth \ufb01le G Text Extraction PDF. <BLANKLINE> A Benchmark and <BLANKLINE> Evaluation for A Benchmark and Evaluation for Text Extraction from PDF. Figure 4: An excerpt of an output \ufb01le O with three paragraphs and the related ground truth \ufb01le G with a single paragraph. Assignment 1: P +: 3 , P \u2212: 1 Assignment 2: P +: 1 , NL+: 1 , W \u22124 Assignment 3: P \u2191\u2193: 1, NL+: 2, W \u2212: 1 Text Extraction PDF. <BLANKLINE> A Benchmark and <BLANKLINE> Evaluation for A Benchmark and Evaluation for Text Extraction from PDF. Text Extraction PDF. <BLANKLINE> A Benchmark and <BLANKLINE> Evaluation for Text Extraction from PDF. <BLANKLINE> A Benchmark and <BLANKLINE> Evaluation for <BLANKLINE> Text Extraction from PDF. Figure 5: Tree di\ufb00erent assignments to the evaluation criteria from Section 4.3.1 in order to assess O against G from Figure 4, with related visualizations. word-di\ufb00, which works similar to the Unix di\ufb00command, but based on words instead of lines.", "metadata": {"source": "data/7_bast_2017_abenchmark.pdf", "page_number": 8, "chunk": 2, "title": "A Benchmark and Evaluation for Text Extraction from PDF"}}, "242": {"idx": 242, "content": "Te phrases are computed per paragraph and know the related paragraph numbers in wO and wG. For the example above, word-di\ufb00computes the phrasesp1: (\u223c[text, extraction, pdf ], []); p2: (= [a, benchmark, and]); p3: (= [evaluation, for]) and p4: (\u223c[], [text, extraction, from, pdf ]). In the second round, the rearranged phrases are computed by an algorithm called rearr-di\ufb00, which is a local alignment algorithm and works similar to the Smith-Waterman algorithm, but based on words instead of characters. In principle, rearr-di\ufb00looks at the differing phrases, identi\ufb01es similar word regions between spurious and missing words, wraps them into rearranged phrases and associates the rearranged phrases with the related di\ufb00ering phrases. For the phrases p1,\u2026, p4 in the example above, rearr-di\ufb00identi\ufb01es a similar word region between the spurious words of phrase p1 and the missing words of phrase p4 and creates the rearranged phrase p5: (\u2191\u2193[text, extraction, pdf ], [text, extraction, from, pdf ]). Initially, all computed rearranged phrases are seen as preliminary phrases and could be refused while assigning values to the evaluation criteria, see below. Given the phrases, the next step is to assign concrete values to the evaluation criteria. Doc-di\ufb00proceeds again in two rounds, in which each phrase pi is seen as a standalone unit with individual evaluation criteria W + i , W \u2212 i , W \u223c i , P + i , etc.", "metadata": {"source": "data/7_bast_2017_abenchmark.pdf", "page_number": 8, "chunk": 3, "title": "A Benchmark and Evaluation for Text Extraction from PDF"}}, "243": {"idx": 243, "content": "(called phrase criteria) and an individual score Zi that scores the phrase criteria equivalently to Z. In the \ufb01rst round, doc-di\ufb00examines the rearranged and differing phrases in order to assign the values for word- and paragraph di\ufb00erences. For each phrase pi, doc-di\ufb00simulates various type-dependent evaluation scenarios, where each scenario Sj is again given by individual evaluation criteria W + Sj, W \u2212 Sj, W \u223c Sj, P + Sj, etc. 8 .", "metadata": {"source": "data/7_bast_2017_abenchmark.pdf", "page_number": 8, "chunk": 4, "title": "A Benchmark and Evaluation for Text Extraction from PDF"}}, "244": {"idx": 244, "content": "(called scenario criteria) and a score ZSj that scores the scenario criteria equivalently to the score Z. In case pi is a rearranged phrase, the scenarios are: S1: P \u2191\u2193= 1; plus the di\ufb00erences resulting from doc-di\ufb00(w pi O , w pi G ) S2: P += 1 (if m > 0); P \u2212= 1 (if n > 0). S3: W \u223c= min(m,n); W += m \u2212min(m,n); W \u2212= n \u2212min(m,n) where w pi O resp. w pi G is the list of related words in pi from wO resp. wG, m = |w pi O | and n = |w pi G |. To clarify, scenario S1 de\ufb01nes the evaluation criteria that would result when pi would indeed be rearranged, S2 the criteria that would result when pi would be assessed only by paragraph di\ufb00erences, and S3 the criteria that would result when pi would be assessed only by word di\ufb00erences. If S1 is the scenario with the minimal score, pi will be accepted as rearranged phrase and the related scenario criteria will be added to the phrase criteria of pi. Otherwise, pi is refused. For example, for phrase p5, the scenario criteria of S1 are: P \u2191\u2193= 1, W \u2212= 1; and of S2: P += 1, P \u2212= 1 and of S3: W \u223c= 3, W += 0, W \u2212= 1. Te related evaluation scores are given by ZS1 = c + 1; ZS2 = 2c and ZS3 = 4. Tus, p5 is accepted as a rearranged phrase only if c \u22643. Otherwise, p5 is refused. In case pi is a di\ufb00ering phrase, the simulated scenarios are S2 and S3, where m resp. n is given by the number of spurious resp. missing words in pi which are not a member of an accepted rearranged phrase.", "metadata": {"source": "data/7_bast_2017_abenchmark.pdf", "page_number": 9, "chunk": 0, "title": "A Benchmark and Evaluation for Text Extraction from PDF"}}, "245": {"idx": 245, "content": "Tere is a special scenario S4, where none of the evaluation criteria are a\ufb00ected, if the spurious words consist of at least one placeholder (see Section 3.2.1 for details about the concept of placeholders). Te criteria of the scenario with the minimal score ZSi are added to the phrase criteria of pi. In case of a tie, the criteria of the scenario which comes \ufb01rst in the introduced order are chosen. For p1 in the example above, the scenario criteria depend on whether p5 is accepted or not. If p5 is accepted, there are no scenario criteria given, because m = 0 and n = 0. If p5 is refused, m = 3 and n = 0 and the criteria of S2 are: P += 1, P \u2212= 0 and of S3: W \u223c= 0; W += 3, W \u2212= 0. Te related scenario scores are given by ZS2 = c and ZS3 = 3, meaning that the scenario criteria of S2 are added to the phrase criteria if c \u22643 and of S3 otherwise. In the second round, doc-di\ufb00iterates over the phrases in order to assign the values for the newline di\ufb00erences. For each phrase pi, doc-di\ufb00analyzes the paragraph numbers of pi and pi\u22121 in order to identify paragraph breaks in wO and wG. If there is a paragraph break in wO but not in wG, an NL+ is added to the phrase criteria of pi. Analogously, if there is a paragraph break in wG but not in wO, an NL\u2212is added. For the example above, a NL+ is added to the phrase criteria of p3, because there is a paragraph break between p2 and p3 in wO, but not in wG. At the end, the \ufb01nal assignment results from the union of all computed phrase criteria.", "metadata": {"source": "data/7_bast_2017_abenchmark.pdf", "page_number": 9, "chunk": 1, "title": "A Benchmark and Evaluation for Text Extraction from PDF"}}, "246": {"idx": 246, "content": "For example, if c \u22643, the \ufb01nal assignment would be: P \u2191\u2193= 1, NL+= 2; W \u2212= 1 (which corresponds to assignment 3 in Figure 5) and W += 3, W \u2212= 4, NL+= 2 if c > 3. 4.4 Evaluation results Table 2 gives an overview of the evaluation results for each of the evaluated PDF extraction tools, broken down by the evaluation criteria computed by the doc-di\ufb00algorithm explained above. For most of the tools, either NL+, NL\u2212or both are prety high. Low values in both criteria are only achieved by those tools, which indeed identify paragraph boundaries, in particular Icecite. Te comparatively large NL\u2212value for PDFExtract is caused by the fact that it does not consider isolated formulas as single paragraphs. PdfMiner has problems with identifying the correct paragraph boundaries if paragraphs were split by page breaks, column breaks or LTBs like \ufb01gures, tables or captions. Te same is true for the criteria P + and P \u2212: low values in both criteria are only achieved by the more sophisticated tools, which are able to identify the semantic roles of LTBs (like Parscit, pdfx, PDFExtract, Grobid and Icecite). In particular, tools like pdfotext and PdfBox show low P \u2212values, but high P + values, because they extract full texts without considering semantic roles. Te large P \u2212 value of LA-PdfText is due to the fact that we used the default rules (see Section 4.1), which resulted in a lot of missing LTBs. In principle, all tools are able to identify the correct reading order of words.", "metadata": {"source": "data/7_bast_2017_abenchmark.pdf", "page_number": 9, "chunk": 2, "title": "A Benchmark and Evaluation for Text Extraction from PDF"}}, "247": {"idx": 247, "content": "However, some tools have problems with two-column articles, as illustrated by the large values in the P \u2191\u2193criteria for pdf2xml and pdf-extract. In the criteria W + and W \u2212, pdf-extract has problems with the correct extraction of subscripts and superscripts. In many cases, the tool extracted them as separate text lines, as they did not share the same baseline with the belonging text line. Finally, the value for W \u223cis large for those tools, which do not translate ligatures into multiple characters and/or do not extract characters with diacritics as single characters and/or merge hyphenated words (like pdfoxml, PdfMiner or pdf-extract). Only Icecite yields satisfactory results in all criteria (close to the optimum among the evaluated tools). However, Icecite is work in progress and not perfect yet either: \u2022 Our rule-based approach on identifying LTBs, which is not \ufb02exible enough to handle each single anomaly in the structures of scienti\ufb01c articles properly. \u2022 Characters (in particular ligatures and special characters) which are printed in so called Type-3 fonts, where the characters are in fact not of textual nature but are drawn into the PDF and therefore are not identi\ufb01able as text. \u2022 Compound words with mandatory hyphens (like sugar-free) which seem to be hyphenated words because they are split at the mandatory hyphen across two text lines. In most cases, Icecite handles them as normal hyphenated words and removes the hyphen mistakenly (merges sugar-free to sugarfree).", "metadata": {"source": "data/7_bast_2017_abenchmark.pdf", "page_number": 9, "chunk": 3, "title": "A Benchmark and Evaluation for Text Extraction from PDF"}}, "248": {"idx": 248, "content": "Te second and third issues are well known problems, which were also observed in most other tools. In particular, the second is a general issue of PDF, which needs more sophisticated methods to solve (OCR-based or learning-based). 5 CONCLUSION We have presented an evaluation on the semantic abilities of 14 PDF extraction tools, based on a high-quality benchmark, which we have constructed from parallel TeX and PDF data. We found that our own PDF extraction tool, Icecite, signi\ufb01cantly outperforms other tools with respect to (1) paragraph boundaries, (2) body text paragraphs, (3) reading order, and (4) word boundaries. However, it is still not perfect due to the limits of its rule-based approach. We are con\ufb01dent that a learning-based approach can \ufb01x the open problems. 9 .", "metadata": {"source": "data/7_bast_2017_abenchmark.pdf", "page_number": 9, "chunk": 4, "title": "A Benchmark and Evaluation for Text Extraction from PDF"}}, "249": {"idx": 249, "content": "", "metadata": {"source": "data/7_bast_2017_abenchmark.pdf", "page_number": 10, "chunk": 0, "title": "A Benchmark and Evaluation for Text Extraction from PDF"}}, "250": {"idx": 250, "content": "System Features NL+ NL\u2212 P + P \u2212 P \u2191\u2193 W + W \u2212 W \u223c ERR T\u009c pdfotext [12] \u2013 O \u2013 L DH 14 (16%) 44 (53%) 60 (29%) 2.3 (0.6%) 1.4 (1.9%) 24 (0.7%) 2.4 (0.1%) 41 (1.2%) 2 0.3 pdfohtml [18] \u2013 O \u2013 L \u2013 \u2013 3.6 (4.3%) 70 (84%) 9.2 (31%) 4.2 (3.2%) 0.1 (0.1%) 16 (0.5%) 1.6 (0.0%) 95 (2.9%) 0 2.2 pdfoxml [11] \u2013 O \u2013 \u2013 \u2013 \u2013 33 (40%) 20 (25%) 80 (31%) 1.8 (0.5%) 0.1 (0.1%) 21 (0.6%) 1.6 (0.0%) 154 (4.7%) 1 0.7 PdfBox [2] \u2013 O \u2013 L D \u2013 3.0 (3.6%) 70 (85%) 7.6 (27%) 0.9 (0.2%) 0.0 (0.1%) 17 (0.5%) 1.5 (0.0%) 53 (1.6%) 2 8.8 pdf2xml [26] P O \u2013 L \u2013 H 33 (40%) 39 (48%) 44 (21%) 40 (30%) 7.8 (9.5%) 8.6 (0.3%) 3.6 (0.1%) 34 (0.9%) 1444 37 ParsCit [15] \u2013 \u2013 R \u2013 \u2013 \u2013 15 (18%) 39 (47%) 10 (10%) 14 (6.4%) 1.3 (1.8%) 16 (0.5%) 2.3 (0.1%) 37 (1.1%) 1 6.8 LA-PdfText [5] \u2013 O R L \u2013 \u2013 5.5 (6.4%) 23 (28%) 4.8 (3.1%) 52 (73%) 2.9 (5.9%) 5.7 (0.1%) 6.1 (0.1%) 26 (0.6%) 324 24 PdfMiner [24] P O \u2013 \u2013 \u2013 \u2013 32 (38%) 18 (21%) 84 (30%) 3.6 (1.0%) 1.4 (2.1%) 34 (1.0%) 2.6 (0.1%) 110 (3.3%) 23 16 pdfXtk [13] \u2013 O \u2013 L \u2013 \u2013 7.9 (9.7%) 68 (84%) 12 (29%) 4.5 (3.5%) 0.1 (0.1%) 59 (1.8%) 6.1 (0.2%) 95 (3.0%) 739 22 pdf-extract [31] \u2013 O \u2013 L \u2013 \u2013 95 (114%) 53 (64%) 99 (32%) 8.4 (3.1%) 4.1 (7.7%) 74 (2.1%) 41 (1.2%) 149 (4.2%) 72 34 pdfx [7] \u2013 O R L DH 6.6 (8.8%) 32 (42%) 9.4 (9.6%) 19 (27%) 0.3 (0.4%) 35 (1.1%) 2.2 (0.1%) 55 (1.7%) 812 70 PDFExtract [3] P O R L DH 9.5 (11%) 33 (40%) 28 (21%) 22 (25%) 0.8 (0.9%) 12 (0.4%) 2.8 (0.1%) 61 (1.8%) 176 46 Grobid [21] \u2013 O R L DH 9.5 (11%) 30 (36%) 7.5 (6.7%) 11 (15%) 0.0 (0.0%) 14 (0.4%) 1.6 (0.0%) 63 (1.9%) 29 42 Icecite [17] P O R L DH 3.4 (4.0%) 10 (13%) 6.2 (4.2%) 7.7 (5.5%) 0.1 (0.1%) 10 (0.3%) 1.7 (0.1%) 21 (0.6%) 34 41 Table 2: Summary of the evaluation results of 14 PDF extraction tools.", "metadata": {"source": "data/7_bast_2017_abenchmark.pdf", "page_number": 10, "chunk": 1, "title": "A Benchmark and Evaluation for Text Extraction from PDF"}}, "251": {"idx": 251, "content": "Te second column gives a summary of Table 1, for convenience. Te evaluation results are given in columns 3-10, broken down into the criteria NL+: the number of spurious newlines; NL\u2212: the number of missing newlines; P +: the number of spurious paragraphs; P \u2212: the number of missing paragraphs; P \u2191\u2193: the number of reordered paragraphs; W +: the number of spurious words; W \u2212: the number of missing words; W \u223c: the number of misspelled words. For each criterion, its absolute value and a percentage is given, which is computed as follows: for NL+ and NL\u2212, it is the absolute value divided by the number of newlines in the ground truth; for the other criteria, it is the number of a\ufb00ected words relative to the number of words in the ground truth \ufb01les. Te best values in each criteria are printed in blue and bold, the two worst values in red. Te column ERR gives the aggregated number of PDF \ufb01les where (a) the extraction process resulted in an error or (b) the runtime of the extraction process exceeded the timeout of \ufb01ve minutes. Te column T\u009c gives the average time needed to process a single PDF \ufb01le, in seconds. REFERENCES [1] S. Anzaroot and A. McCallum. A New Dataset for Fine-Grained Citation Field Extraction. In ICML Workshop (PEER), 2013. [2] Apache. PdfBox. htps://pdfox.apache.org/, 2017. [3] \u00d8. R. Berg. PDFExtract. htps://github.com/oyvindberg/PDFExtract/, 2011. [4] S. Bird, R. Dale, B. J. Dorr, B. R. Gibson, M. T. Joseph, M. Kan, D. Lee, B. Powley, D. R. Radev, and Y. F. Tan.", "metadata": {"source": "data/7_bast_2017_abenchmark.pdf", "page_number": 10, "chunk": 2, "title": "A Benchmark and Evaluation for Text Extraction from PDF"}}, "252": {"idx": 252, "content": "Te ACL Anthology Reference Corpus: A Reference Dataset for Bibliographic Research in Computational Linguistics. In LREC, 2008. [5] G. Burns. LA-PdfText. htps://github.com/GullyAPCBurns/lapdfext, 2013. [6] C. Caragea, J. Wu, A. M. Ciobanu, K. Williams, J. P. F. Ram\u00b4\u0131rez, H. Chen, Z. Wu, and C. L. Giles. CiteSeerX : A Scholarly Big Dataset. In ECIR, 2014. [7] A. Constantin, S. Petifer, and A. Voronkov. pdfx. htp://pdfx.cs.man.ac.uk/, 2011. [8] A. Constantin, S. Petifer, and A. Voronkov. PDFX: Fully-automated PDF-to-XML Conversion of Scienti\ufb01c Literature. In DocEng, 2013. [9] Cornell University. arXiv.org e-Print archive. htps://arxiv.org/, 2017. [10] N. V. Cuong, M. K. Chandrasekaran, M. Kan, and W. S. Lee. Scholarly Document Information Extraction using Extensible Features for E\ufb03cient Higher Order Semi-CRFs. In JCDL, 2015. [11] H. Dejean and E. Giguet. pdfoxml. htps://sourceforge.net/projects/pdf2xml/, 2016. [12] FooLabs. Xpdf: A PDF Viewer for X. htp://www.foolabs.com/xpdf, 2014. [13] T. Hassan. pdfXtk. htps://github.com/tamirhassan/pdfxtk, 2013. [14] Institute of Computer Science and Technology of Peking University. Marmot Datasets. htp://www.icst.pku.edu.cn/cpdp/data/marmot data.htm, 2016. [15] M.-Y. Kan. ParsCit. htps://github.com/knmnyn/ParsCit, 2016. [16] S. Klamp\ufb02, M. Granitzer, K. Jack, and R. Kern. Unsupervised Document Structure Analysis of Digital Scienti\ufb01c Articles. JCDL, 2014. [17] C. Korzen. Icecite. htps://github.com/ckorzen/icecite, 2017. [18] M. Kruk.", "metadata": {"source": "data/7_bast_2017_abenchmark.pdf", "page_number": 10, "chunk": 3, "title": "A Benchmark and Evaluation for Text Extraction from PDF"}}, "253": {"idx": 253, "content": "pdfohtml. htps://sourceforge.net/projects/pdfohtml/, 2013. [19] M. Ley. DBLP - Some Lessons Learned. PVLDB, 2009. [20] M. Lipinski, K. Yao, C. Breitinger, J. Beel, and B. Gipp. Evaluation of Header Metadata Extraction Approaches and Tools for Scienti\ufb01c PDF Documents. In JCDL, 2013. [21] P. Lopez. Grobid. htps://github.com/kermit2/grobid, 2017. [22] A. McCallum, K. Nigam, J. Rennie, and K. Seymore. Automating the Construction of Internet Portals with Machine Learning. Inf. Retr., 2000. [23] C. Ramakrishnan, A. Patnia, E. H. Hovy, and G. A. P. C. Burns. Layout-Aware Text Extraction from Full-Text PDF of Scienti\ufb01c Articles. Source Code for Biology and Medicine, 2012. [24] Y. Shinyama. PdfMiner. htps://github.com/euske/pdfminer, 2016. [25] Springer Nature. BioMed Central. htps://www.biomedcentral.com/, 2017. [26] J. Tiedemann. pdf2xml. htps://bitbucket.org/tiedemann/pdf2xml/, 2016. [27] D. Tkaczyk, A. Czeczko, K. Rusek, L. Bolikowski, and R. Bogacewicz. GROTOAP: Ground Truth for Open Access Publications. In JCDL, 2012. [28] D. Tkaczyk, P. Szostek, and L. Bolikowski. GROTOAP2 - Te Methodology of Creating a Large Ground Truth Dataset of Scienti\ufb01c Articles. D-Lib Magazine, 2014. [29] D. Tkaczyk, P. Szostek, P. J. Dendek, M. Fedoryszak, and L. Bolikowski. CERMINE - Automatic Extraction of Metadata and References from Scienti\ufb01c Literature. In DAS, 2014. [30] U.S. National Institutes of Health\u2019s National Library of Medicine. PubMed Central. htps://www.ncbi.nlm.nih.gov/pmc/, 2017.", "metadata": {"source": "data/7_bast_2017_abenchmark.pdf", "page_number": 10, "chunk": 4, "title": "A Benchmark and Evaluation for Text Extraction from PDF"}}, "254": {"idx": 254, "content": "[31] K. J. Ward. pdf-extract. htps://github.com/CrossRef/pdfextract/, 2015. 10 .", "metadata": {"source": "data/7_bast_2017_abenchmark.pdf", "page_number": 10, "chunk": 5, "title": "A Benchmark and Evaluation for Text Extraction from PDF"}}, "255": {"idx": 255, "content": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4969\u20134983 July 5 - 10, 2020. c\u20dd2020 Association for Computational Linguistics 4969 S2ORC: The Semantic Scholar Open Research Corpus Kyle Lo\u2020\u2217 Lucy Lu Wang\u2020\u2217 Mark Neumann\u2020 Rodney Kinney\u2020 Daniel S. Weld\u2020\u2021 \u2020Allen Institute for Arti\ufb01cial Intelligence \u2021Paul G. Allen School of Computer Science & Engineering, University of Washington {kylel, lucyw}@allenai.org Abstract We introduce S2ORC,1 a large corpus of 81.1M English-language academic papers spanning many academic disciplines. The corpus consists of rich metadata, paper abstracts, resolved bibliographic references, as well as structured full text for 8.1M open access papers. Full text is annotated with automaticallydetected inline mentions of citations, \ufb01gures, and tables, each linked to their corresponding paper objects. In S2ORC, we aggregate papers from hundreds of academic publishers and digital archives into a uni\ufb01ed source, and create the largest publicly-available collection of machine-readable academic text to date. We hope this resource will facilitate research and development of tools and tasks for text mining over academic text. 1 Introduction Academic papers are an increasingly important textual domain for natural language processing (NLP) research.", "metadata": {"source": "data/5_kyle_2020_s2orc.pdf", "page_number": 1, "chunk": 0, "title": "S2ORC: The Semantic Scholar Open Research Corpus"}}, "256": {"idx": 256, "content": "Aside from capturing valuable knowledge from humankind\u2019s collective research efforts, academic papers exhibit many interesting characteristics \u2013 thousands of words organized into sections, objects such as tables, \ufb01gures and equations, frequent inline references to these objects, footnotes, other papers, and more. Different types of resources have been used to support research over academic papers. Citation graphs like AMiner\u2019s Open Academic Graph (Tang et al., 2008), the Microsoft Academic Graph (MAG) (Shen et al., 2018), and the Semantic Scholar literature graph (Ammar et al., 2018), have had widespread application in bibliometrics, science-of-science, information retrieval, and network analysis. Digital archives like arXiv,2 \u2217denotes equal contribution 1Instructions for access to the data and model are available at https://github.com/allenai/s2orc/. 2https://arxiv.org Figure 1: Inline citations and references to \ufb01gures and tables are annotated in S2ORC\u2019s structured full text. Citations are linked to bibliography entries, which are linked to other papers in S2ORC. Figure and table references are linked to their captions. PubMed Central,3 CiteSeerX (Giles et al., 1998),4 and the ACL Anthology (Bird et al., 2008),5 are popular resources for deriving large text corpora for summarization and language modeling or, with further annotation, development of datasets for tasks like entity extraction, text classi\ufb01cation, parsing, and discourse analysis.", "metadata": {"source": "data/5_kyle_2020_s2orc.pdf", "page_number": 1, "chunk": 1, "title": "S2ORC: The Semantic Scholar Open Research Corpus"}}, "257": {"idx": 257, "content": "We focus on bibliometrically-enhanced derivations of these corpora, such as the ACL Anthology Network (AAN) (Radev et al., 2009)6 derived from the ACL Anthology, RefSeer (Huang et al., 2015) derived from CiteSeerX, and Saier and F\u00a8arber (2019) derived from arXiv, which combine useful aspects of citation graphs and raw text corpora. These resources provide citation mentions linked to paper identi\ufb01ers in their corresponding digital archives, such as the ACL Anthology and CiteSeerX, or to nodes in citation graphs such as MAG, enabling new forms of cross-paper discourse analysis (e.g., studying how or why papers are related). 3https://www.ncbi.nlm.nih.gov/pmc 4https://citeseerx.ist.psu.edu 5https://www.aclweb.org/anthology 6http://aan.how/ .", "metadata": {"source": "data/5_kyle_2020_s2orc.pdf", "page_number": 1, "chunk": 2, "title": "S2ORC: The Semantic Scholar Open Research Corpus"}}, "258": {"idx": 258, "content": "4970 Corpus Papers w/ body text Citation contexts References to tables / \ufb01gures / equations Linked to graph Academic disciplines S2ORC (PDF-parse) 8.1M full text yes S2ORC (full) multi S2ORC (LATEX-parse) 1.5M full text yes S2ORC (full) physics, math, CS PubMed Central (OA) 2.6M full text yes PubMed bio, med AAN (Radev et al., 2009) 25k full text no ACL Anthology comp ling Saier and F\u00a8arber (2019)\u2020 1.0M snippets no MAG physics, math, CS RefSeer (Huang et al., 2015) 1.0M snippets no CiteSeerX multi Table 1: A comparison of S2ORC with other publicly-available academic text corpora. Of the other corpora: PubMed Central (OA) links to PubMed, which contains 30M papers at the time of writing. AAN links to the ACL Anthology (which contained 25k papers at the time of dataset construction, and 54k papers at the time of writing). Saier and F\u00a8arber (2019) is derived from arXiv and links to MAG (which contained 213M papers and other non-paper documents at the time of dataset construction, and 226M nodes at the time of writing). RefSeer links to CiteSeerX (which contained 1M papers at the time of dataset construction, and 6M papers at the time of writing). S2ORC contains three times more full text papers than PubMed Central (OA), the next largest corpus with bibliometric enhancements, while covering a more diverse set of academic disciplines. Citations in S2ORC are linked to the full set of S2ORC papers, 81.1M paper nodes derived from Semantic Scholar.", "metadata": {"source": "data/5_kyle_2020_s2orc.pdf", "page_number": 2, "chunk": 0, "title": "S2ORC: The Semantic Scholar Open Research Corpus"}}, "259": {"idx": 259, "content": "In addition, the LATEX subset of S2ORC captures additional structure omitted by Saier and F\u00a8arber (2019), who also parse LATEX sources from arXiv. \u2020Saier and F\u00a8arber (2020) is an update to this work which now includes full text. It is released concurrently with this work. Yet, existing corpora are not without their limitations. Some cover a small number of papers (e.g. AAN), are domain-speci\ufb01c (e.g. AAN, PubMed Central, Saier and F\u00a8arber (2019)), or may not provide usable full text (e.g. Saier and F\u00a8arber (2019) and RefSeer). To address these issues, we introduce S2ORC,7 the Semantic Scholar8 Open Research Corpus, a large publicly-available collection of 81.1M academic papers covering dozens of academic disciplines. Each paper is associated with metadata and abstracts aggregated from hundreds of trusted sources such as academic publishers and literature archives like PubMed and arXiv. Notably, we release structured, machinereadable full text extracted from PDFs for 8.1M papers which we\u2019ve identi\ufb01ed as having open access status. S2ORC full text preserves meaningful structure, e.g., paragraph breaks, section headers, inline citation mentions, references to tables and \ufb01gures, and resolved citation links to other papers. Additionally, we provide 1.5M full text LATEX parses from which we have extracted, in addition to citations and references, the source text of tables and mathematical formulas.", "metadata": {"source": "data/5_kyle_2020_s2orc.pdf", "page_number": 2, "chunk": 1, "title": "S2ORC: The Semantic Scholar Open Research Corpus"}}, "260": {"idx": 260, "content": "As shown in Table 1, S2ORC provides substantially more structured full text papers and covers a more diverse set of academic disciplines than other resources. 7pronounced \u201cstork\u201d 8The papers included in S2ORC are a curated subset of the papers in the Semantic Scholar literature graph (Ammar et al., 2018) that focuses only on English-language papers with abstracts or full text available. See \u00a72.5 for details on \ufb01ltering through Semantic Scholar papers. In this paper, we describe the construction of S2ORC (\u00a72). We provide summary statistics of the corpus (\u00a73) and evaluate the data quality (\u00a74). We then evaluate a BERT model pretrained on S2ORC (\u00a75), and discuss potential applications to a variety of NLP and analysis tasks over academic text (\u00a76). Finally, we compare S2ORC with other publicly-available academic text corpora (\u00a77). 2 Constructing the corpus S2ORC is constructed using data from the Semantic Scholar literature corpus (Ammar et al., 2018). Papers in Semantic Scholar are derived from numerous sources: obtained directly from publishers, from resources such as MAG, from various archives such as arXiv or PubMed, or crawled from the open Internet. Semantic Scholar clusters these papers based on title similarity and DOI overlap, resulting in an initial set of approximately 200M paper clusters.", "metadata": {"source": "data/5_kyle_2020_s2orc.pdf", "page_number": 2, "chunk": 2, "title": "S2ORC: The Semantic Scholar Open Research Corpus"}}, "261": {"idx": 261, "content": "To construct S2ORC, we must overcome challenges in (i) paper metadata aggregation, (ii) identifying open access publications, and (iii) clustering papers, in addition to identifying, extracting, and cleaning the full text and bibliometric annotations associated with each paper. The pipeline for creating S2ORC is: 1) Process PDFs and LATEX sources to derive metadata, clean full text, inline citations and references, and bibliography entries, 2) Select the best metadata and full text parses for each paper cluster, .", "metadata": {"source": "data/5_kyle_2020_s2orc.pdf", "page_number": 2, "chunk": 3, "title": "S2ORC: The Semantic Scholar Open Research Corpus"}}, "262": {"idx": 262, "content": "4971 3) Filter paper clusters with insuf\ufb01cient metadata or content, and 4) Resolve bibliography links between paper clusters in the corpus. Details for these steps are provided below. See Appendix \u00a7A for de\ufb01nitions of terminology. The output of this pipeline is visualized in Figure 1. 2.1 Processing PDFs We process PDFs from the Semantic Scholar corpus using SCIENCEPARSE v3.0.09 and GROBID v0.5.510 (Lopez, 2009). Our processing pipeline is described below. Selecting PDFs We remove PDFs which are less likely to be academic papers. SCIENCEPARSE and GROBID are not optimized for processing nonpaper academic documents such as dissertations, reports, slides, etc., and this \ufb01ltering step is necessary to increase output data quality. See Appendix \u00a7B for \ufb01lter details. There are around 31.3M PDFs associated with approximately 200M initial paper clusters, and 30.5M PDFs are selected for processing based on these \ufb01ltering criteria. Extracting structured data from PDFs We use SCIENCEPARSE to extract title and authors from each PDF.11 We then use GROBID to process each PDF.", "metadata": {"source": "data/5_kyle_2020_s2orc.pdf", "page_number": 3, "chunk": 0, "title": "S2ORC: The Semantic Scholar Open Research Corpus"}}, "263": {"idx": 263, "content": "From the XML output of GROBID, we extract (i) metadata such as title, authors, and abstract, (ii) paragraphs from the body text organized under section headings, (iii) \ufb01gure and table captions, (iv) equations, table content, headers, and footers, which we remove from the body text, (v) inline citations in the abstract and body text, (vi) parsed bibliography entries with title, authors, year, and venue identi\ufb01ed, and (vi) links between inline citation mentions and their corresponding bibliography entries. Postprocessing GROBID output We postprocess GROBID output using regular expressions to classify the parenthetical citation style of a paper as BRACKET (e.g. [2]), NAME-YEAR (e.g. ABC, 2019), or OTHER (superscripts and other mixed styles). We focus on addressing two types of common errors in GROBID\u2019s inline citation extractions: (i) false positives resulting from superscripts or equation references being recognized as 9https://github.com/allenai/scienceparse 10https://github.com/kermitt2/grobid 11Our evaluations suggest SCIENCEPARSE outperforms GROBID for title and author extraction. inline citations in papers with BRACKET-style citations, and (ii) false negatives resulting from an inability to expand bracket citation ranges (e.g. \u201c[3]-[5]\u201d should be expanded to \u201c[3], [4], [5]\u201d before linking). False positives are detected using regular expressions and removed from GROBID output.", "metadata": {"source": "data/5_kyle_2020_s2orc.pdf", "page_number": 3, "chunk": 1, "title": "S2ORC: The Semantic Scholar Open Research Corpus"}}, "264": {"idx": 264, "content": "Bracket citation ranges are manually expanded and linked to their corresponding bibliography entries. The resulting parses are expressed in JSON format.12 2.2 Processing LATEX source LATEX document source is available for a majority of arXiv submissions, and where available, are used to construct a full text parse. We retrieve body text, section headers, \ufb01gure/table captions, table representations, equations, and inline citations and references directly from LATEX source. Inspired by Saier and F\u00a8arber (2019), we \ufb01rst convert LATEX source into XML documents and then extract structured information from the XML. Due to direct access to source, the accuracy of citation span, reference, caption, section header, and equation detection is near-perfect. We process 1.5M papers from LATEX source derived from arXiv, all of which are included as part of S2ORC. Surprisingly, due to the diversity of ways in which authors de\ufb01ne metadata in LATEX, the quality of metadata extracted from LATEX documents is worse than those extracted from PDF. Therefore, we do not use LATEX-derived metadata for paper clustering or metadata selection. 2.3 Selecting canonical metadata Canonical values for title, authors and other metadata \ufb01elds are selected from among the papers in a cluster. First, if a cluster contains multiple PDFs, we select one to be canonical. This can occur, for example, in a cluster containing an arXiv preprint and its eventual camera-ready version.", "metadata": {"source": "data/5_kyle_2020_s2orc.pdf", "page_number": 3, "chunk": 2, "title": "S2ORC: The Semantic Scholar Open Research Corpus"}}, "265": {"idx": 265, "content": "We preferentially select PDFs from open access sources and break ties by prioritizing PDFs for which there exist richer publisher-provided metadata (e.g. abstract, year, venue, DOI). If the selected PDF is associated with publisher-provided metadata, we select those publisher-provided metadata \ufb01elds to be canonical. In cases where publisher-provided metadata is incomplete, we use majority voting to select 12The S2ORC data format is described at https:// github.com/allenai/s2orc .", "metadata": {"source": "data/5_kyle_2020_s2orc.pdf", "page_number": 3, "chunk": 3, "title": "S2ORC: The Semantic Scholar Open Research Corpus"}}, "266": {"idx": 266, "content": "4972 canonical metadata values. We break ties by minimizing the total number of sources from which we select metadata (e.g., if IEEE provides title, authors and abstract, DBLP provides title and authors, and arXiv provides title and abstract, we prioritize selecting IEEE over the union of DBLP and arXiv). S2ORC metadata \ufb01elds include title, author, year, venue, journal, abstract, and identi\ufb01ers (DOI, PubMed, PubMed Central (PMC), arXiv, and ACL Anthology). In cases where the title and authors are not provided by any publishers, we derive the values for these \ufb01elds from the parsed PDF, prioritizing SCIENCEPARSE over GROBID. We further comment on paper clustering as it pertains to metadata selection in Appendix \u00a7C. 2.4 Assembling the corpus We construct the \ufb01nal corpus by assembling clustered paper metadata with GROBID and LATEX parse objects. We associate the GROBID parse with the S2ORC paper object if a valid GROBID parse is produced from the PDF, and the PDF is open access. Open access status is assigned if a paper is derived from arXiv, ACL Anthology, PubMed Central (OA), and/or associated with an open-access DOI in the Unpaywall database.13 If the PDF is not open access, we only include the bibliography from the GROBID parse in S2ORC. If arXiv LATEX source is available for the paper cluster, we also associate the LATEX parse with the S2ORC paper object.", "metadata": {"source": "data/5_kyle_2020_s2orc.pdf", "page_number": 4, "chunk": 0, "title": "S2ORC: The Semantic Scholar Open Research Corpus"}}, "267": {"idx": 267, "content": "2.5 Filtering paper clusters We further \ufb01lter paper clusters to remove papers with (i) no title, (ii) no authors, (iii) fewer than 100 characters of abstract and body text, and (iv) where English is not the primary language. The \ufb01rst three \ufb01lters remove papers that provide little value for bibliometric-based or text-based analyses. The English language \ufb01lter14 reduces GROBID parsing errors. All \ufb01lters are applied in series. Subsequently, 95.5M paper clusters are \ufb01ltered out based on the aforementioned criteria and removed from the corpus. The distribution of \ufb01ltered papers is given in Table 2. We note that a large number of paper clusters are \ufb01ltered out; 80.0M of these \ufb01ltered clusters have no associated publisher-provided abstract or associated PDF and 13Unpaywall 2019-04-19 data dump 14We use the cld2 tool for language detection with a threshold of 0.9 over the English language score. do not provide signi\ufb01cant value to our dataset in their current state. Although these papers that lack text may be useful as cite-able nodes in S2ORC, they are generally of lower quality and are \ufb01ltered out of the corpus to improve corpus quality. Filter Number of papers No title 20k No authors 0.3M < 100 chars of text 80.0M Not English 15.2M Table 2: Post-processing data quality \ufb01lters for papers 2.6 Linking bibliographies to papers Each bibliography entry in both GROBID and LATEX parses are linked to the most similar papers in the corpus.", "metadata": {"source": "data/5_kyle_2020_s2orc.pdf", "page_number": 4, "chunk": 1, "title": "S2ORC: The Semantic Scholar Open Research Corpus"}}, "268": {"idx": 268, "content": "For linking, we score each bibliography entry and paper cluster pair using a similarity score computed between their titles. Each title is \ufb01rst normalized (i.e. white spaces stripped, lower-cased, special characters removed) and represented by its character 3-grams. The similarity score Stitle is computed as the harmonic mean between a Jaccard index and a containment metric: Stitle = 2 \u00d7 J \u00d7 C J + C (1) where the Jaccard index J and containment metric C are computed from the n-grams of the two titles N1 and N2 as: J = |N1 \u2229N2| |N1 \u222aN2| C = |N1 \u2229N2| min (|N1|, |N2|) For each bibliography entry, the bibliographypaper pair with the highest similarity score above 0.8 is output as the correct link. Otherwise, the bibliography entry remains unlinked. We perform an evaluation of linking performance in \u00a74. 3 The S2ORC dataset The resulting corpus consists of 81.1M papers. Our publisher-provided abstract coverage is 90.4%, or 73.4M papers. Our PDF coverage is 35.6%, or 28.9M papers. These PDFs are processed using the pipeline discussed in \u00a72.1. The .", "metadata": {"source": "data/5_kyle_2020_s2orc.pdf", "page_number": 4, "chunk": 2, "title": "S2ORC: The Semantic Scholar Open Research Corpus"}}, "269": {"idx": 269, "content": "4973 Total papers 81.1M Papers w/ PDF 28.9M (35.6%) Papers w/ bibliographies 27.6M (34.1%) Papers w/ GROBID full text 8.1M (10.0%) Papers w/ LaTeX full text 1.5M (1.8%) Papers w/ publisher abstract 73.4M (90.4%) Papers w/ DOIs 52.2M (64.3%) Papers w/ Pubmed IDs 21.5M (26.5%) Papers w/ PMC IDs 4.7M (5.8%) Papers w/ ArXiv IDs 1.7M (2.0%) Papers w/ ACL IDs 42k (0.1%) Table 3: Statistics on paper provenance. We note that categories are not mutually exclusive and do not sum to 100%. All papers in S2ORC have either a publisherprovided abstract or an associated PDF from which we derive full text and/or bibliography entries, or both. Statistic GROBID LATEX Paragraphs (abstract) 1.1 - Paragraphs (body) 9.9 93.3* Inline cite spans (abstract) 0.7 - Inline cite spans (body) 45.2 46.8 Bibliography entries 27.6 21.9 Linked bib. entries 19.3 6.8\u2020 Table 4: Extraction and linking statistics over PDF and LATEX parses. Reported values are averaged over all open access papers, which consist of 8.1M GROBIDparsed PDFs and 1.5M parsed LATEX sources. *LATEX preserves line breaks rather than paragraph breaks. \u2020The lower number of linked bibliography entries in LATEX parses is due to large numbers of papers (mostly in the \ufb01eld of physics) for which the bibliography entries are formatted without paper titles. Our linking algorithm strongly depends on titles and fails to link these entries.", "metadata": {"source": "data/5_kyle_2020_s2orc.pdf", "page_number": 5, "chunk": 0, "title": "S2ORC: The Semantic Scholar Open Research Corpus"}}, "270": {"idx": 270, "content": "vast majority of these PDFs are successfully processed using GROBID, and we extract bibliography entries for 27.6M of the 28.9M PDFs. We identify 8.1M of the 28.9M PDFs as open access (\u00a72.4), and we provide full text for all papers in this open access subset. For the 1.5M papers for which LATEX source is available through arXiv, we further obtain and provide LATEX parses (\u00a72.2). Using these extracted bibliographies, we resolve a total 380.5M citation links between papers (\u00a72.6), 156.5M of which can be tied back to their inline citation mentions in the full text. See Table 3 for more provenance statistics. We provide statistics for the GROBID and LATEX full text parses and bibliography linking in Figure 2: Distribution of papers by Microsoft Academic \ufb01eld of study. Table 4. On average, LATEX parses contain many more \u201cparagraphs\u201d of body text, because LATEX source \ufb01les preserve line breaks rather than paragraph breaks. We speculate that differences in bibliography entry and linking counts between the GROBID and LATEX parses are due to a combination of: (i) challenges in LATEX bibliography expansion and parsing, and (ii) differences in bibliography formatting in some math and physics venues (where bibliography entries do not include paper titles, which we depend on for bibliography linking). The distribution of academic disciplines in S2ORC is given in Figure 2 using Microsoft Academic \ufb01elds of study.", "metadata": {"source": "data/5_kyle_2020_s2orc.pdf", "page_number": 5, "chunk": 1, "title": "S2ORC: The Semantic Scholar Open Research Corpus"}}, "271": {"idx": 271, "content": "Not all papers in S2ORC can be found in Microsoft Academic \u2013 those not found are denoted as Unclassi\ufb01ed. Approximately 677k papers have more than one primary Microsoft Academic \ufb01eld of study; Figure 2 represents only the top \ufb01eld of study for each paper. 4 Evaluation To evaluate the quality of our metadata selection, we randomly sample 500 paper clusters, restricting to those with PDFs. Within each sampled cluster, we determine whether the canonical title and authors match the title and authors in the selected canonical PDF. Inline citation detection and bibliography parsing are dependent on GROBID (Lopez, 2009). Ahmad and Afzal (2018) evaluate GROBID for de- .", "metadata": {"source": "data/5_kyle_2020_s2orc.pdf", "page_number": 5, "chunk": 2, "title": "S2ORC: The Semantic Scholar Open Research Corpus"}}, "272": {"idx": 272, "content": "4974 Domain Dataset Reference Task SCIBERT S2ORCSCIBERT BC5CDR Li et al. (2016) NER 90.01 90.41 \u00b1 0.06 JNLPBA Collier and Kim (2004) NER 77.28 77.70 \u00b1 0.25 NCBI-disease Do\u02d8gan et al. (2014) NER 88.57 88.70 \u00b1 0.52 Biomed EBM-NLP Nye et al. (2018) PICO 72.28 72.35 \u00b1 0.95 GENIA Kim et al. (2003) DEP (LAS) 90.43 90.80 \u00b1 0.19 GENIA Kim et al. (2003) DEP (UAS) 91.99 92.31 \u00b1 0.18 ChemProt Krallinger et al. (2017) REL 83.64 84.59 \u00b1 0.93 SciERC Luan et al. (2018) NER 67.57 68.93 \u00b1 0.19 CS SciERC Luan et al. (2018) REL 79.97 81.77 \u00b1 1.64 ACL-ARC Jurgens et al. (2018) CLS 70.98 68.45 \u00b1 2.47 Biomed & CS SciCite Cohan et al. (2019) CLS 85.49 84.76 \u00b1 0.37 Multi-domain PaperField Beltagy et al. (2019) CLS 65.71 65.99 \u00b1 0.08 Table 5: S2ORC-SCIBERT test results are comparable with reported SCIBERT test results on the set of tasks and datasets from Beltagy et al. (2019), to which we refer the reader for descriptions. Reported statistics are spanlevel F1 for NER, token-level F1 for PICO, dependency parsing (DEP), and macro-F1 for relation (REL) and text (CLS) classi\ufb01cation. We report micro-F1 for ChemProt. All S2ORC-SCIBERT results are the mean \u00b1 standard deviation of 5 runs with different random seeds. Beltagy et al. (2019) do not report standard deviation or number of runs. tecting inline citations using a corpus of 5k CiteSeer papers, and found GROBID to have an F1score of 0.89 on this task. Tkaczyk et al.", "metadata": {"source": "data/5_kyle_2020_s2orc.pdf", "page_number": 6, "chunk": 0, "title": "S2ORC: The Semantic Scholar Open Research Corpus"}}, "273": {"idx": 273, "content": "(2018) report GROBID as the best among 10 out-of-the-box tools for parsing bibliographies, also achieving an F1 of 0.89 in an evaluation corpus of 9.5k papers. We perform an evaluation over 200 randomly sampled papers from S2ORC and found comparable F1-scores for GROBID performance on both tasks. For bibliography linking, we randomly sample S2ORC papers (500 GROBID PDF parses and 100 LATEX parses) and select one linked bibliography entry from each sampled paper (while avoiding selecting multiple entries linked to the same paper). We determine whether the title and authors in the bibliography entry agree with the title and authors of the linked paper. We present these evaluation results in Table 6 and detail valuation criteria in Appendix \u00a7D. Evaluated task Title Authors Paper clustering 0.93 0.89 Bib. linking (GROBID) 1.00 0.96 Bib. linking (LATEX) 1.00 0.92 Table 6: Accuracy of paper clustering and bibliography linking for titles and authors in sampled evaluation sets. 5 Pretraining BERT on S2ORC To demonstrate the suitability of S2ORC for language model pretraining, we train BERT-Base (Devlin et al., 2019) on the parsed full text of S2ORC and show that the resulting model (S2ORC-SCIBERT) performs similarly to SCIBERT (Beltagy et al., 2019) on a diverse suite of scienti\ufb01c NLP tasks and datasets.", "metadata": {"source": "data/5_kyle_2020_s2orc.pdf", "page_number": 6, "chunk": 1, "title": "S2ORC: The Semantic Scholar Open Research Corpus"}}, "274": {"idx": 274, "content": "While SCIBERT is a BERT-Base model also trained on multiple domains of scienti\ufb01c text, key differences in its pretraining corpus and vocabulary and those used for S2ORC-SCIBERT are: \u2022 Domain: Beltagy et al. (2019) report a pretraining corpus consisting of 82% biomedical and 18% computer science papers. Our S2ORC pretraining corpus consists of a more balanced distribution of papers across diverse academic disciplines (see Figure 2), such that biomedical (42.7%) and computer science (7.2%) papers only comprise half the corpus. \u2022 Preprocessing: S2ORC identi\ufb01es \ufb01gure captions, table text and captions, headers, footers, and footnotes. We exclude these from the pretraining corpus. We tokenize and sentencize the text using scispaCy (Neumann et al., 2019). We also use heuristic \ufb01lters to remove ill-formed paragraphs (such as those containing too many symbols). \u2022 Size: The resulting S2ORC pretraining cor- .", "metadata": {"source": "data/5_kyle_2020_s2orc.pdf", "page_number": 6, "chunk": 2, "title": "S2ORC: The Semantic Scholar Open Research Corpus"}}, "275": {"idx": 275, "content": "4975 pus contains 16.4B tokens, nearly \ufb01ve times larger than the corpus for SCIBERT. \u2022 Vocab: Following Beltagy et al. (2019), we construct a cased WordPiece (Wu et al., 2016) vocabulary of size 31k using 15% of the S2ORC pretraining corpus. The Jaccard index between the S2ORC-SCIBERT and SCIBERT vocabularies is 0.536. We follow a similar setup to Beltagy et al. (2019) for both pretraining and \ufb01ne-tuning S2ORC-SCIBERT. Like SCIBERT, S2ORCSCIBERT is pretrained from scratch using the original BERT code15 and default BERT-Base con\ufb01gurations on a single TPU v3-8 for one week. Also like SCIBERT, S2ORC-SCIBERT is \ufb01netuned on all tasks by optimizing a cross entropy loss using Adam (Kingma and Ba, 2014), a linear learning rate decay with 10% warm-up, batch size of 32, and dropout of 0.1. We search over an equal-sized grid of hyperparameters as Beltagy et al. (2019). We \ufb01ne-tune for 1 to 4 epochs with a maximum learning rate of 1e-5, 2e-5, 3e-5, or 5e-5. For each task, we select the optimal combination of these two hyperparameters using the development set and report the corresponding test set results. For details, we refer the reader to SCIBERT code,16 which we use for all experiments. The results in Table 5 show that S2ORCSCIBERT outperforms SCIBERT on many tasks despite including a large percentage of data outside of the biomedical and computer science domains.", "metadata": {"source": "data/5_kyle_2020_s2orc.pdf", "page_number": 7, "chunk": 0, "title": "S2ORC: The Semantic Scholar Open Research Corpus"}}, "276": {"idx": 276, "content": "As the pretraining corpus for SCIBERT is not publicly-available, S2ORC can serve as a large pretraining corpus for evaluating and comparing pretraining approaches on academic text. We also release S2ORC-SCIBERT to serve as a baseline for research. 6 Applications of S2ORC S2ORC can be used for many NLP and analysis tasks over academic text. We give a summary of potential applications below. The combination of structured full text annotated with linked inline citations makes S2ORC well-suited for a variety of citation-related textbased tasks. Without any additional supervision, S2ORC can be used directly for both inline (He 15https://github.com/google-research/ bert 16https://github.com/allenai/scibert et al., 2010; Duma and Klein, 2014; Jeong et al., 2019) and document-level (Yu et al., 2012; Liu et al., 2015; Bhagavatula et al., 2018) citation recommendation. Among document-level recommenders, S2ORC is well-suited to the setting of Liu et al. (2015), who use inline citation contexts to \ufb01lter document-level recommendations. Figure 3: Word2vec embeddings associated with 20k papers in six AI-related arXiv categories visualized using t-SNE (van der Maaten and Hinton, 2008). Example papers from two randomly selected sub-regions A and B are given in Table 7.", "metadata": {"source": "data/5_kyle_2020_s2orc.pdf", "page_number": 7, "chunk": 1, "title": "S2ORC: The Semantic Scholar Open Research Corpus"}}, "277": {"idx": 277, "content": "Region A cs.LG \u201cOn Unifying Deep Generative Models\u201d stat.ML \u201cLearning Disentangled Representations with Semi-Supervised Deep Generative Models\u201d cs.LG \u201cDenoising Criterion for Variational AutoEncoding Framework\u201d cs.CV \u201cVariational methods for conditional multimodal deep learning\u201d Region B cs.CL \u201cTransA: An Adaptive Approach for Knowledge Graph Embedding\u201d cs.AI \u201cTorusE: Knowledge Graph Embedding on a Lie Group\u201d cs.CV \u201cImage-embodied Knowledge Representation Learning\u201d stat.ML \u201cNeural Embeddings of Graphs in Hyperbolic Space\u201d Table 7: Sampled papers in clusters from t-SNE embedding space in Figure 3. Region A consists of papers related to deep generative models; region B consists of papers concerned with graph representation learning. Other tasks that leverage citation contexts in- .", "metadata": {"source": "data/5_kyle_2020_s2orc.pdf", "page_number": 7, "chunk": 2, "title": "S2ORC: The Semantic Scholar Open Research Corpus"}}, "278": {"idx": 278, "content": "4976 clude classifying citation intent (Teufel et al., 2006; Jurgens et al., 2018; Cohan et al., 2019), identifying citation sentiment (Athar and Teufel, 2012), identifying meaningful citations (Valenzuela et al., 2015), extracting key phrases (Caragea et al., 2014), and citation context-based paper summarization (Teufel et al., 2006; Qazvinian and Radev, 2008; Cohan and Goharian, 2015; Mitrovi\u00b4c and M\u00a8uller, 2015). The models in these papers require labeled citation contexts for training. S2ORC could potentially bene\ufb01t task performance without additional annotation, for example, by pretraining language models on S2ORC citation contexts before \ufb01ne-tuning to these tasks. Cohan et al. (2019) \ufb01nd that long citation contexts (beyond sentence boundary) are important for tasks like summarization; the wider citation contexts available in S2ORC could be used to augment existing datasets for document-level tasks. Citation contexts can also be used for the more general tasks of identifying similar papers (Kanakia et al., 2019; Eto, 2019; Haruna et al., 2018; Small, 1973) or bibliometric analysis (Ding et al., 2014; Trujillo and Long, 2018; Asatani et al., 2018). Towards these tasks, the citation contexts in S2ORC can provide insight into how and why papers are cited. We illustrate this by following Berger et al.", "metadata": {"source": "data/5_kyle_2020_s2orc.pdf", "page_number": 8, "chunk": 0, "title": "S2ORC: The Semantic Scholar Open Research Corpus"}}, "279": {"idx": 279, "content": "(2016) in training a word2vec skip-gram model (Mikolov et al., 2013) using full text citation contexts in S2ORC, where each inline citation span is replaced with its linked paper identi\ufb01er. When training over this modi\ufb01ed text, the word2vec model learns embeddings corresponding to each unique paper identi\ufb01er, which can be leveraged as paper embeddings. The resulting embeddings shown in Figure 3 and Table 7 form clusters corresponding closely to arXiv Machine Learning categories. Upon inspection, papers of different categories in the same embedding sub-region share research themes (see Table 7), indicating that these paper embeddings trained from citation contexts capture coherent topic similarity and relatedness. These paper embeddings can be used to identify similar papers, using the similarity between two papers\u2019 citing contexts as a proxy for paper similarity. The LATEX subset of S2ORC also provides unique opportunities for research. In addition to citations and references, we also extract and parse tables from LATEX source into a structured format. There is an opportunity to use these tables for corpus-level results extraction and aggregation. The LATEX subset also has \ufb01ne-grained extraction and labeling of mathematical formulas, which can be used to understand proof construction, or to assist in symbol co-reference resolution.", "metadata": {"source": "data/5_kyle_2020_s2orc.pdf", "page_number": 8, "chunk": 1, "title": "S2ORC: The Semantic Scholar Open Research Corpus"}}, "280": {"idx": 280, "content": "7 Related work The ACL Anthology Network (AAN) (Radev et al., 2009) is a bibliometric-enhanced corpus covering papers in the \ufb01eld of computational linguistics. It is built from the ACL Anthology (Bird et al., 2008) and consists of 24.6k papers manually augmented with citation information. The PubMed Central Open Access corpus is a large corpus of 2.6M papers in the biomedical domain with citations linked to PubMed identi\ufb01ers.17 CiteSeerX (Giles et al., 1998), consists of papers collected primarily via web crawl, without integrating metadata provided by sources outside of the PDF. Although citation contexts are no longer available through CiteSeerX, the RefSeer dataset (Huang et al., 2015)18 is a dataset of short citation context snippets derived from 1.0M papers from CiteSeerX. More recently, Saier and F\u00a8arber (2019) introduce a corpus built using 1.0M arXiv publications. They use LATEX source to extract text, citation spans and bibliography entries, which are linked to papers in the Microsoft Academic Graph. The citation context they provide are extracted snippets and no bibliography parses are provided. An updated version of this dataset (Saier and F\u00a8arber, 2020) released concurrently with this work now includes full text. Compared with these resources, S2ORC represents a signi\ufb01cantly larger dataset of linked papers covering broad domains of science by leveraging PDF parsing in addition to LATEX source.", "metadata": {"source": "data/5_kyle_2020_s2orc.pdf", "page_number": 8, "chunk": 2, "title": "S2ORC: The Semantic Scholar Open Research Corpus"}}, "281": {"idx": 281, "content": "S2ORC also provides clean full text for text mining and NLP needs with additional enhancements such as annotations of table and \ufb01gure references and captions. S2ORC\u2019s wealth of metadata and structured text allows it to be \ufb02exibly adapted to a variety of downstream tasks. 8 Conclusion We introduce S2ORC, the largest publiclyavailable corpus of English-language academic papers covering dozens of academic disciplines. 17https://www.ncbi.nlm.nih.gov/pmc/ tools/openftlist/ 18https://psu.app.box.com/v/refseer .", "metadata": {"source": "data/5_kyle_2020_s2orc.pdf", "page_number": 8, "chunk": 3, "title": "S2ORC: The Semantic Scholar Open Research Corpus"}}, "282": {"idx": 282, "content": "4977 S2ORC consists of 81.1M papers, 380.5M resolved citation links, and structured full text from 8.1M open-access PDFs and 1.5M LATEX source \ufb01les. We aggregate metadata and abstracts from hundreds of trusted sources. Full text is augmented with sections, citation mentions, and references to tables and \ufb01gures. We demonstrate that S2ORC can be used effectively for downstream NLP tasks in academic paper analysis. The pipeline for creating S2ORC was used to construct the CORD-19 corpus (Wang et al., 2020), which saw fervent adoption as the canonical resource for COVID-19 text mining. CORD-19 is aimed at assisting biomedical experts and policy makers process large amounts of COVID-19 literature in the search for effective treatments and management policies. With over 75K dataset downloads, dozens of search and question-answering systems, and hundreds of participating teams across two shared tasks19 in the \ufb01rst month of its release, there is little doubt of the resource\u2019s impact. Our hope with the release of S2ORC is to ensure such text mining resources are available to researchers even beyond periods of global crisis. Acknowledgements This work was supported in part by ONR grant N00014-18-1-2193, and the University of Washington WRF/Cable Professorship. We thank Doug Downey, Oren Etzioni, Andrew Head, and Bryan Newbold for their valuable feedback on the manuscript.", "metadata": {"source": "data/5_kyle_2020_s2orc.pdf", "page_number": 9, "chunk": 0, "title": "S2ORC: The Semantic Scholar Open Research Corpus"}}, "283": {"idx": 283, "content": "We also thank Isabel Cachola, Dallas Card, Mike D\u2019Arcy, Suchin Gururangan, Daniel King, Rik Koncel-Kedziorski, Susan Liu, Kelvin Luu, Noah Smith, Gabi Stanovsky, and Dave Wadden for feedback on the dataset during early development. Finally, we thank the Semantic Scholar team for assisting with data access and system infrastructure. References Riaz Ahmad and Muhammad Tanvir Afzal. 2018. Cad: an algorithm for citation-anchors detection in research papers. Scientometrics, 117:1405\u20131423. Waleed Ammar, Dirk Groeneveld, Chandra Bhagavatula, Iz Beltagy, Miles Crawford, Doug Downey, Jason Dunkelberger, Ahmed Elgohary, Sergey Feldman, Vu Ha, Rodney Kinney, Sebastian Kohlmeier, 19The Kaggle CORD-19 and TREC-COVID competitions. See Wang et al. (2020) for details. Kyle Lo, Tyler Murray, Hsu-Han Ooi, Matthew Peters, Joanna Power, Sam Skjonsberg, Lucy Wang, Chris Wilhelm, Zheng Yuan, Madeleine van Zuylen, and Oren Etzioni. 2018. Construction of the literature graph in semantic scholar. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 3 (Industry Papers), pages 84\u201391, New Orleans - Louisiana. Association for Computational Linguistics. Kimitaka Asatani, Junichiro Mori, Masanao Ochi, and Ichiro Sakata. 2018. Detecting trends in academic research from a citation network using network representation learning. In PloS one. Awais Athar and Simone Teufel. 2012.", "metadata": {"source": "data/5_kyle_2020_s2orc.pdf", "page_number": 9, "chunk": 1, "title": "S2ORC: The Semantic Scholar Open Research Corpus"}}, "284": {"idx": 284, "content": "Contextenhanced citation sentiment detection. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 597\u2013601, Montr\u00b4eal, Canada. Association for Computational Linguistics. Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciBERT: A pretrained language model for scienti\ufb01c text. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3615\u20133620, Hong Kong, China. Association for Computational Linguistics. Matthew Berger, Katherine McDonough, and Lee M Seversky. 2016. cite2vec: Citation-driven document exploration via word embeddings. IEEE transactions on visualization and computer graphics, 23(1):691\u2013700. Chandra Bhagavatula, Sergey Feldman, Russell Power, and Waleed Ammar. 2018. Content-based citation recommendation. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 238\u2013251, New Orleans, Louisiana. Association for Computational Linguistics. Steven Bird, Robert Dale, Bonnie Dorr, Bryan Gibson, Mark Joseph, Min-Yen Kan, Dongwon Lee, Brett Powley, Dragomir Radev, and Yee Fan Tan. 2008. The ACL anthology reference corpus: A reference dataset for bibliographic research in computational linguistics.", "metadata": {"source": "data/5_kyle_2020_s2orc.pdf", "page_number": 9, "chunk": 2, "title": "S2ORC: The Semantic Scholar Open Research Corpus"}}, "285": {"idx": 285, "content": "In Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC\u201908), Marrakech, Morocco. European Language Resources Association (ELRA). Cornelia Caragea, Florin Adrian Bulgarov, Andreea Godea, and Sujatha Das Gollapalli. 2014. Citationenhanced keyphrase extraction from research papers: A supervised approach. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1435\u2013 1446, Doha, Qatar. Association for Computational Linguistics. .", "metadata": {"source": "data/5_kyle_2020_s2orc.pdf", "page_number": 9, "chunk": 3, "title": "S2ORC: The Semantic Scholar Open Research Corpus"}}, "286": {"idx": 286, "content": "4978 Arman Cohan, Waleed Ammar, Madeleine van Zuylen, and Field Cady. 2019. Structural scaffolds for citation intent classi\ufb01cation in scienti\ufb01c publications. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3586\u20133596, Minneapolis, Minnesota. Association for Computational Linguistics. Arman Cohan and Nazli Goharian. 2015. Scienti\ufb01c article summarization using citation-context and article\u2019s discourse structure. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 390\u2013400, Lisbon, Portugal. Association for Computational Linguistics. Nigel Collier and Jin-Dong Kim. 2004. Introduction to the bio-entity recognition task at JNLPBA. In Proceedings of the International Joint Workshop on Natural Language Processing in Biomedicine and its Applications (NLPBA/BioNLP), pages 73\u2013 78, Geneva, Switzerland. COLING. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota. Association for Computational Linguistics. Ying Ding, Guo Zhang, Tamy Chambers, Min Song, Xiaolong Wang, and Cheng xiang Zhai. 2014.", "metadata": {"source": "data/5_kyle_2020_s2orc.pdf", "page_number": 10, "chunk": 0, "title": "S2ORC: The Semantic Scholar Open Research Corpus"}}, "287": {"idx": 287, "content": "Content-based citation analysis: The next generation of citation analysis. JASIST, 65:1820\u20131833. Rezarta Islamaj Do\u02d8gan, Robert Leaman, and Zhiyong Lu. 2014. Ncbi disease corpus: a resource for disease name recognition and concept normalization. Journal of biomedical informatics, 47:1\u201310. Daniel Duma and Ewan Klein. 2014. Citation resolution: A method for evaluating context-based citation recommendation systems. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 358\u2013363, Baltimore, Maryland. Association for Computational Linguistics. Masaki Eto. 2019. Extended co-citation search: Graph-based document retrieval on a co-citation network containing citation context information. Inf. Process. Manage., 56. C. L. Giles, K. D. Bollacker, and S. Lawrence. 1998. Citeseer: an automatic citation indexing system. In Proceedings of the ACM International Conference on Digital Libraries, pages 89\u201398. ACM. Proceedings of the 1998 3rd ACM Conference on Digital Libraries ; Conference date: 23-06-1998 Through 26-06-1998. Khalid Haruna, Maizatul Akmar Ismail, Abdullahi Baffa Bichi, Victor I. Chang, Sutrisna Wibawa, and Tutut Herawan. 2018. A citation-based recommender system for scholarly paper recommendation. In ICCSA. Qi He, Jian Pei, Daniel Kifer, Prasenjit Mitra, and Lee Giles. 2010. Context-aware citation recommendation.", "metadata": {"source": "data/5_kyle_2020_s2orc.pdf", "page_number": 10, "chunk": 1, "title": "S2ORC: The Semantic Scholar Open Research Corpus"}}, "288": {"idx": 288, "content": "In Proceedings of the 19th International Conference on World Wide Web, WWW \u201910, page 421\u2013430, New York, NY, USA. Association for Computing Machinery. Wenyi Huang, Zhaohui Wu, Chen Liang, Prasenjit Mitra, and C. Lee Giles. 2015. A neural probabilistic model for context based citation recommendation. In Proceedings of the Twenty-Ninth AAAI Conference on Arti\ufb01cial Intelligence, AAAI\u201915, page 2404\u20132410. AAAI Press. Chanwoo Jeong, Sion Jang, Hyuna Shin, Eunjeong Park, and Sungchul Choi. 2019. A context-aware citation recommendation model with bert and graph convolutional networks. arXiv. David Jurgens, Srijan Kumar, Raine Hoover, Dan McFarland, and Dan Jurafsky. 2018. Measuring the evolution of a scienti\ufb01c \ufb01eld through citation frames. Transactions of the Association for Computational Linguistics, 6:391\u2013406. Anshul Kanakia, Zhihong Shen, Darrin Eide, and Kuansan Wang. 2019. A scalable hybrid research paper recommender system for microsoft academic. In The World Wide Web Conference, WWW \u201919, page 2893\u20132899, New York, NY, USA. Association for Computing Machinery. J-D Kim, Tomoko Ohta, Yuka Tateisi, and Jun\u2019ichi Tsujii. 2003. Genia corpus\u2014a semantically annotated corpus for bio-textmining. Bioinformatics, 19(suppl 1):i180\u2013i182. Diederik Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. In International Conference on Learning Representations.", "metadata": {"source": "data/5_kyle_2020_s2orc.pdf", "page_number": 10, "chunk": 2, "title": "S2ORC: The Semantic Scholar Open Research Corpus"}}, "289": {"idx": 289, "content": "Martin Krallinger, Obdulia Rabal, Saber Ahmad Akhondi, Mart\u00b4\u0131n P\u00b4erez P\u00b4erez, J\u00b4es\u00b4us L\u00b4opez Santamar\u00b4\u0131a, Gael P\u00b4erez Rodr\u00b4\u0131guez, Georgios Tsatsaronis, Ander Intxaurrondo, Jos\u00b4e Antonio Baso L\u00b4opez, Umesh Nandal, Erin M. van Buel, A. Poorna Chandrasekhar, Marleen Rodenburg, Astrid L\u00e6greid, Marius A. Doornenbal, Julen Oyarz\u00b4abal, An\u00b4alia Lourenc\u00b8o, and Alfonso Valencia. 2017. Overview of the biocreative vi chemical-protein interaction track. In N/A. Jiao Li, Yueping Sun, Robin J Johnson, Daniela Sciaky, Chih-Hsuan Wei, Robert Leaman, Allan Peter Davis, Carolyn J Mattingly, Thomas C Wiegers, and Zhiyong Lu. 2016. Biocreative V CDR task corpus: a resource for chemical disease relation extraction. Database, 2016. .", "metadata": {"source": "data/5_kyle_2020_s2orc.pdf", "page_number": 10, "chunk": 3, "title": "S2ORC: The Semantic Scholar Open Research Corpus"}}, "290": {"idx": 290, "content": "4979 Haifeng Liu, Xiangjie Kong, Xiaomei Bai, Wei Wang, Teshome Megersa Bekele, and Feng Xia. 2015. Context-based collaborative \ufb01ltering for citation recommendation. IEEE Access, 3:1695\u20131703. Nelson F. Liu, Matt Gardner, Yonatan Belinkov, Matthew E. Peters, and Noah A. Smith. 2019a. Linguistic knowledge and transferability of contextual representations. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1073\u20131094, Minneapolis, Minnesota. Association for Computational Linguistics. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke S. Zettlemoyer, and Veselin Stoyanov. 2019b. RoBERTa: A robustly optimized BERT pretraining approach. arXiv. Patrice Lopez. 2009. Grobid: Combining automatic bibliographic data recognition and term extraction for scholarship publications. In Proceedings of the 13th European Conference on Research and Advanced Technology for Digital Libraries, ECDL\u201909, page 473\u2013474, Berlin, Heidelberg. Springer-Verlag. Yi Luan, Luheng He, Mari Ostendorf, and Hannaneh Hajishirzi. 2018. Multi-task identi\ufb01cation of entities, relations, and coreference for scienti\ufb01c knowledge graph construction. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3219\u20133232, Brussels, Belgium. Association for Computational Linguistics.", "metadata": {"source": "data/5_kyle_2020_s2orc.pdf", "page_number": 11, "chunk": 0, "title": "S2ORC: The Semantic Scholar Open Research Corpus"}}, "291": {"idx": 291, "content": "Laurens van der Maaten and Geoffrey E. Hinton. 2008. Visualizing data using t-sne. In Journal of Machine Learning Research. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Distributed representations of words and phrases and their compositionality. In Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 2, NIPS\u201913, page 3111\u20133119, Red Hook, NY, USA. Curran Associates Inc. Sandra Mitrovi\u00b4c and Henning M\u00a8uller. 2015. Summarizing citation contexts of scienti\ufb01c publications. In Experimental IR Meets Multilinguality, Multimodality, and Interaction, pages 154\u2013165, Cham. Springer International Publishing. Mark Neumann, Daniel King, Iz Beltagy, and Waleed Ammar. 2019. ScispaCy: Fast and robust models for biomedical natural language processing. In Proceedings of the 18th BioNLP Workshop and Shared Task, pages 319\u2013327, Florence, Italy. Association for Computational Linguistics. Benjamin Nye, Junyi Jessy Li, Roma Patel, Yinfei Yang, Iain Marshall, Ani Nenkova, and Byron Wallace. 2018. A corpus with multi-level annotations of patients, interventions and outcomes to support language processing for medical literature. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 197\u2013207, Melbourne, Australia. Association for Computational Linguistics.", "metadata": {"source": "data/5_kyle_2020_s2orc.pdf", "page_number": 11, "chunk": 1, "title": "S2ORC: The Semantic Scholar Open Research Corpus"}}, "292": {"idx": 292, "content": "Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018a. Deep contextualized word representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 2227\u20132237, New Orleans, Louisiana. Association for Computational Linguistics. Matthew Peters, Mark Neumann, Luke Zettlemoyer, and Wen-tau Yih. 2018b. Dissecting contextual word embeddings: Architecture and representation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1499\u20131509, Brussels, Belgium. Association for Computational Linguistics. Vahed Qazvinian and Dragomir R. Radev. 2008. Scienti\ufb01c paper summarization using citation summary networks. In Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 689\u2013696, Manchester, UK. Coling 2008 Organizing Committee. Dragomir R. Radev, Pradeep Muthukrishnan, and Vahed Qazvinian. 2009. The acl anthology network corpus. In Proceedings of the 2009 Workshop on Text and Citation Analysis for Scholarly Digital Libraries, NLPIR4DL \u201909, page 54\u201361, USA. Association for Computational Linguistics. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. N/A. Tarek Saier and Michael F\u00a8arber. 2019.", "metadata": {"source": "data/5_kyle_2020_s2orc.pdf", "page_number": 11, "chunk": 2, "title": "S2ORC: The Semantic Scholar Open Research Corpus"}}, "293": {"idx": 293, "content": "Bibliometricenhanced arxiv: A data set for paper-based and citation-based tasks. In Proceedings of the 8th International Workshop on Bibliometric-enhanced Information Retrieval (BIR 2019) co-located with the 41st European Conference on Information Retrieval (ECIR 2019), Cologne, Germany, April 14, 2019, volume 2345 of CEUR Workshop Proceedings, pages 14\u201326. CEUR-WS.org. Tarek Saier and Michael F\u00a8arber. 2020. unarxive: a large scholarly data set with publications\u2019 full-text, annotated in-text citations, and links to metadata. Scientometrics. Zhihong Shen, Hao Ma, and Kuansan Wang. 2018. A web-scale system for scienti\ufb01c knowledge exploration. In Proceedings of ACL 2018, System Demonstrations, pages 87\u201392, Melbourne, Australia. Association for Computational Linguistics. Henry Small. 1973. Co-citation in the scienti\ufb01c literature: A new measure of the relationship between .", "metadata": {"source": "data/5_kyle_2020_s2orc.pdf", "page_number": 11, "chunk": 3, "title": "S2ORC: The Semantic Scholar Open Research Corpus"}}, "294": {"idx": 294, "content": "4980 two documents. Journal of the American Society for Information Science, 24(4):265\u2013269. Jie Tang, Jing Zhang, Limin Yao, Juanzi Li, Li Zhang, and Zhong Su. 2008. Arnetminer: Extraction and mining of academic social networks. In Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201908, page 990\u2013998, New York, NY, USA. Association for Computing Machinery. Simone Teufel, Advaith Siddharthan, and Dan Tidhar. 2006. Automatic classi\ufb01cation of citation function. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 103\u2013110, Sydney, Australia. Association for Computational Linguistics. Dominika Tkaczyk, Andrew Collins, Paraic Sheridan, and Joeran Beel. 2018. Machine learning vs. rules and out-of-the-box vs. retrained: An evaluation of open-source bibliographic reference and citation parsers. In Proceedings of the 18th ACM/IEEE on Joint Conference on Digital Libraries, JCDL \u201918, page 99\u2013108, New York, NY, USA. Association for Computing Machinery. Caleb M. Trujillo and Tammy M. Long. 2018. Document co-citation analysis to enhance transdisciplinary research. Science Advances, 4(1). Marco Valenzuela, Vu Ha, and Oren Etzioni. 2015. Identifying meaningful citations. AAAI.", "metadata": {"source": "data/5_kyle_2020_s2orc.pdf", "page_number": 12, "chunk": 0, "title": "S2ORC: The Semantic Scholar Open Research Corpus"}}, "295": {"idx": 295, "content": "Lucy Lu Wang, Kyle Lo, Yoganand Chandrasekhar, Russell Reas, Jiangjiang Yang, Darrin Eide, Kathryn Funk, Rodney Kinney, Ziyang Liu, William Merrill, Paul Mooney, Dewey Murdick, Devvret Rishi, Jerry Sheehan, Zhihong Shen, Brandon Stilson, Alex D. Wade, Kuansan Wang, Chris Wilhelm, Boya Xie, Douglas Raymond, Daniel S. Weld, Oren Etzioni, and Sebastian Kohlmeier. 2020. CORD-19: The Covid-19 Open Research Dataset. Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Gregory S. Corrado, Macduff Hughes, and Jeffrey Dean. 2016. Google\u2019s neural machine translation system: Bridging the gap between human and machine translation. arXiv, abs/1609.08144. Xiao Yu, Quanquan Gu, Mianwei Zhou, and Jiawei Han. 2012. Citation prediction in heterogeneous bibliographic networks. In SDM. .", "metadata": {"source": "data/5_kyle_2020_s2orc.pdf", "page_number": 12, "chunk": 1, "title": "S2ORC: The Semantic Scholar Open Research Corpus"}}, "296": {"idx": 296, "content": "4981 A Background & Terminology In this work, we distinguish between bibliography entries and inline citations. A bibliography entry is an item in a paper\u2019s bibliography that refers to another paper. It is represented in a structured format that can be used for paper-identifying features such as title, authors, year, and venue or journal, and for journal articles, the volume, issue, and pages. Also commonly represented are unique document identi\ufb01ers such as the Document Object Identi\ufb01er (DOI), arXiv identi\ufb01er, or PubMed identi\ufb01er. Common formats for bibliography entries are MLA, APA, Vancouver-, and Chicago- style, among others, which are different ways of representing these various features for document identi\ufb01cation. There is often variation in the representation of certain \ufb01elds. For example, Authors can include the \ufb01rst names of each author or only their \ufb01rst initials. In many academic disciplines, journal publications are the norm, whereas conference proceedings dominate in \ufb01elds such as Computer Science; conference proceedings tend to lack journal-related features such as Volume, Issue, and Pages. Bibliography entry demarcation also varies between different formats. In some cases, each entry is preceded by a citation marker (e.g. \u201c[1]\u201d or \u201c[ABC2019]\u201d) that is used throughout the text of the paper to denote inline citations. An inline citation is a mention span within the paper\u2019s abstract or body text that refers to one of the entries in its bibliography.", "metadata": {"source": "data/5_kyle_2020_s2orc.pdf", "page_number": 13, "chunk": 0, "title": "S2ORC: The Semantic Scholar Open Research Corpus"}}, "297": {"idx": 297, "content": "\u201cABC (2019) present model 1, which outperforms model 2 (XYZ (2019)).\u201d In this example, the narrative inline citation ABC (2019) appears as a noun phrase in the sentence while the parenthetical inline citation (XYZ, 2019) is inserted into the sentence as an aside. A sentence remains grammatically correct when parenthetical citations are removed. Other styles of parenthetical citations include, but are not limited to, BRACKET-style numbers (e.g. \u201c[1, 35]\u201d) and OTHER styles such as superscripts (e.g. \u201c1,2\u201d), both of which refer to numbered entries in the bibliography. Bibliography entries without numbered entries or citation markers are typically referenced inline using NAME-YEAR format as ABC (2019) or (XYZ, 2019) in the example above. Additionally, an inline reference is a span in a paper that refers to another part of the paper, for example, references to \ufb01gures, tables, equations, proofs, sections, or appendices. These often take on the form of: \u201cIn Figure 3, we show the relationship between A and B.\u201d where Figure 3 refers to a plot displayed on a separate page. These inline references can be important for understanding the relationship between text and objects within the paper.", "metadata": {"source": "data/5_kyle_2020_s2orc.pdf", "page_number": 13, "chunk": 1, "title": "S2ORC: The Semantic Scholar Open Research Corpus"}}, "298": {"idx": 298, "content": "B PDF \ufb01lters Prior to running GROBID, we \ufb01lter out PDFs that (i) produce an error when processed using the Python library PyPDF2,20 (ii) have greater than 50 pages (more likely to be a dissertation or report), (iii) have page widths greater than page heights (more likely to be slides), and (iv) those which fail to be extracted using pdfalto, the variant of pdftoxml used by GROBID. Numbers of PDFs removed by these \ufb01lters are given in Table 8. Filter Number of PDFs PyPDF2 error 0.54M Over 50 pages 2.27M Page width > height 0.28M PDFAlto error 0.21M Table 8: PDFs \ufb01ltered out before GROBID processing C The paper clustering problem In academic \ufb01elds in which preprint publishing is common (e.g. arXiv), the notion of a \u201cpaper\u201d is somewhat ambiguous. For example, if a published paper differs from its arXiv preprint (as it often does), are the two documents considered separate papers for the purposes of citation? What about different arXiv preprint drafts tagged as different versions but under the same arXiv identi\ufb01er? In this work, each \u201cpaper\u201d of interest is actually a collection (or cluster) of highly-similar (but not necessarily identical) documents. These paper clusters, provided by Semantic Scholar, are constructed to re\ufb02ect how authors tend to view their 20Used to determine PDF page number and page dimensions .", "metadata": {"source": "data/5_kyle_2020_s2orc.pdf", "page_number": 13, "chunk": 2, "title": "S2ORC: The Semantic Scholar Open Research Corpus"}}, "299": {"idx": 299, "content": "4982 own papers; for example, most authors would consider their arXiv preprint and its associated published version to be the same \u201cpaper\u201d. For practical concerns in constructing S2ORC, we further require that one document within the cluster be the canonical document used to represent the paper cluster. There are issues with de\ufb01ning a paper to be a collection of documents. For example, suppose a paper cluster contains both an arXiv preprint and a peer-reviewed draft. And suppose another paper cites the arXiv preprint critiquing content that has been updated in the peer-reviewed draft. If the peer-reviewed draft is chosen as the canonical representation of the paper cluster, then the citation context would not accurately capture the rationale of that reference. While worth noting, we believe such cases are rare and do not affect the vast majority of citation contexts. D S2ORC evaluation criteria Paper cluster quality For each paper cluster, we compare the selected canonical Title and Authors \ufb01elds with the title and authors of the selected canonical PDF. The Title \ufb01eld is labeled correct if it exactly matches the title seen on the PDF, with some allowance for different capitalization and minor differences in special character representation (e.g. \u201c\u03b3\u201d versus \u201cgamma\u201d) and ignoring whitespace. The Authors \ufb01eld is labeled correct if all authors on the PDF are presented in the correct order, with some allowance for variation in the surface form.", "metadata": {"source": "data/5_kyle_2020_s2orc.pdf", "page_number": 14, "chunk": 0, "title": "S2ORC: The Semantic Scholar Open Research Corpus"}}, "300": {"idx": 300, "content": "This is to avoid penalizing publisher metadata for providing a \ufb01rst initial (instead of the \ufb01rst name) or omitting middle names or titles (e.g. \u201cDr.\u201d, \u201cPhD\u201d). Paper-Bibliography linking For each paperbibliography pair, we compare the selected canonical Title and Authors \ufb01elds in the structured bibliography entry to the selected canonical Title and Authors \ufb01elds of the linked paper cluster. The Title \ufb01elds are labeled as a match under the same criteria described above for matching paper cluster Title \ufb01elds and PDF titles. The Authors \ufb01elds are labeled as a match if there is substantial overlap in the names of the authors. For example, if authors A, B and C are in the bibliography entry and the linked paper cluster has authors A and B, then this is still considered a match. We note that in our evaluation, differences in the two sets of author names primarily stems from incorrectly written bibliography entries or mistakes in publisherprovided metadata. E Training corpus sizes for other language models Language model Training data ELMO (Peters et al., 2018a) 1BW (800M) Wikipedia (1.9B) WMT 2008-2012 (3.6B) BERT (Devlin et al., 2019) BooksCorpus (800M) Wikipedia (2.5B) ROBERTA (Liu et al., 2019b) BooksCorpus (800M) CC-News (~3.8B) OpenWebText (~1.9B) Stories (~1.6B) GPT2 (Radford et al., 2019) Web Text Corpus (~2.8B) Table 9: Reported and estimated (several papers report corpus size in terms of bytes) token counts of training data used to train language models.", "metadata": {"source": "data/5_kyle_2020_s2orc.pdf", "page_number": 14, "chunk": 1, "title": "S2ORC: The Semantic Scholar Open Research Corpus"}}, "301": {"idx": 301, "content": "We estimate that all of S2ORC consists of approximately 25B tokens of full body text and 15B tokens of abstract text. As demonstrated for S2ORC-SCIBERT pretraining, aggressivelycleaned body text from the PDF-parsed subset of S2ORC still yields approximately 16.5B tokens. The size of S2ORC makes it more than suf\ufb01cient for pretraining large language models such as ELMO, BERT, ROBERTA, GPT2, and others, whose reported training data sizes are given in Table 9 for comparison. Figure 4: Visualization of contextual representations from layer 9 of S2ORC-SCIBERT on numeric surface forms in a subsample of body text from S2ORC. Labels are heuristics based on token-level patterns. .", "metadata": {"source": "data/5_kyle_2020_s2orc.pdf", "page_number": 14, "chunk": 2, "title": "S2ORC: The Semantic Scholar Open Research Corpus"}}, "302": {"idx": 302, "content": "4983 F Numeric representations in S2ORC-SCIBERT Academic papers contain substantially more diverse uses of numeric surface forms than typical web text, such as experimental results, equations, citation references and section/\ufb01gure markers. To demonstrate this, we cluster contextual word representations involving numbers, heuristically labeling them into one of 8 categories based on surface patterns. Examining the progression of the contextual representations through the layers of BERT reveals an initial focus on sentence position (expected, due to explicit position embeddings) and magnitude, with later layers integrating substantial contextual information, such as the presence of inline LATEX identi\ufb01ers, citation indicators and PDF references. Following Peters et al. (2018b); Liu et al. (2019a), we observe that the \ufb01nal 2-3 BERT layers provide embeddings that excel at predictive language modeling; as such, Figure 4 uses embeddings from layer 9 of S2ORCSCIBERT. .", "metadata": {"source": "data/5_kyle_2020_s2orc.pdf", "page_number": 15, "chunk": 0, "title": "S2ORC: The Semantic Scholar Open Research Corpus"}}, "303": {"idx": 303, "content": "AI for Everyone Presidential Initiative for Artificial Intelligence and  Computing .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 1, "chunk": 0, "title": "AI for Everyone"}}, "304": {"idx": 304, "content": "Objectives of this Course 1. The meaning behind common AI terminology, including neural  networks, machine learning, deep learning, and data science 2. What AI realistically can--and cannot--do 3. How to spot opportunities to apply AI to problems in your own  organization 4. What it feels like to build machine learning and data science projects 5. How to work with an AI team and build an AI strategy in your  company 6. How to navigate ethical and societal discussions surrounding AI .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 2, "chunk": 0, "title": "AI for Everyone"}}, "305": {"idx": 305, "content": "$13 Trillion AI value creation by 2030 Source: McKinsey Global Institute .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 3, "chunk": 0, "title": "AI for Everyone"}}, "306": {"idx": 306, "content": "A lot of the value created by AI will be outside the software industry. AI will have a huge  impact on all the major industries. .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 4, "chunk": 0, "title": "AI for Everyone"}}, "307": {"idx": 307, "content": "There are 2 types of AI ANI Artificial Narrow Intelligence AGI Artificial General Intelligence LOTS OF PROGRESS ALMOST NO PROGRESS .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 5, "chunk": 0, "title": "AI for Everyone"}}, "308": {"idx": 308, "content": "Artificial Narrow Intelligence (ANI) These are AIs that do one thing such as: \u25cf smart speaker \u25cf self-driving car \u25cf AI to do web search \u25cf AI applications in farming or in a factory.  These types of AI are one trick ponies but when you find the appropriate trick,  this can be incredibly valuable. .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 6, "chunk": 0, "title": "AI for Everyone"}}, "309": {"idx": 309, "content": "Artificial General Intelligence (AGI) That is the goal to build AI.  They can do anything a human can do or maybe  even be superintelligent and do even more things  than any human can. .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 7, "chunk": 0, "title": "AI for Everyone"}}, "310": {"idx": 310, "content": "Progress in ANI vs AGI The rapid progress in ANI has caused people  to conclude that there's a lot of progress in  AI, which is true. But that has caused people  to falsely think that there might be a lot of  progress in AGI as well which is leading to  some irrational fears about evil clever robots  coming over to take over humanity anytime  now. .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 8, "chunk": 0, "title": "AI for Everyone"}}, "311": {"idx": 311, "content": "Achieving AGI Will Take Time AGI is an exciting goal for researchers to work on, but it  requires many technological breakthroughs before we  get there and it may be decades or hundreds of years  or even thousands of years away. .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 9, "chunk": 0, "title": "AI for Everyone"}}, "312": {"idx": 312, "content": "Machine Learning The rise of AI has been largely driven by one tool in AI called machine learning. You will start thinking how machine learning might be applied to your  company or to your industry. .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 10, "chunk": 0, "title": "AI for Everyone"}}, "313": {"idx": 313, "content": "Supervised Learning Input (A) Audio If the input is an audio clip, and the AI's job is to output the text transcript,  then this is speech recognition. Output (B) Text (0/1) Application Speech Recognition .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 11, "chunk": 0, "title": "AI for Everyone"}}, "314": {"idx": 314, "content": "Supervised Learning Input (A) English Output (B) Chinese If you want to input English and have it output a different language, Chinese,  Spanish, something else, then this is machine translation. Application Machine Translation .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 12, "chunk": 0, "title": "AI for Everyone"}}, "315": {"idx": 315, "content": "Supervised Learning Input (A) Ad + User Info Output (B) Click? (0/1) All the large online ad platforms have a piece of AI that inputs some  information about an ad, and some information about you, and tries to  predict, will you click on this ad or not? Application Machine Translation .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 13, "chunk": 0, "title": "AI for Everyone"}}, "316": {"idx": 316, "content": "Supervised Learning If you want to build a self-driving car, one of the key pieces of AI is the AI that takes as input an  image, and some information from radar, or from other sensors, and outputs the position of  other cars, so your self-driving car can avoid the other cars. Input(A) Image, radar  info Position of other  cars Self-Driving Car .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 14, "chunk": 0, "title": "AI for Everyone"}}, "317": {"idx": 317, "content": "Supervised Learning In Manufacturing, we take as input a picture of something you've just manufactured, such as a picture  of a cell phone coming off the assembly line., and you want to output, is there a scratch, or is there a  dent, or some other defects on this thing you've just manufactured? This is visual inspection which is  helping manufacturers to reduce or prevent defects in the things that they're making. Input (A) Image of a phone Output (B) Defects (0/1) Visual Inspection .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 15, "chunk": 0, "title": "AI for Everyone"}}, "318": {"idx": 318, "content": "Supervised Learning This set of AI called supervised learning, just learns input to output, or A to B  mappings. On one hand, input to output, A to B it seems quite limiting. But  when you find a right application scenario, this can be incredibly valuable. .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 16, "chunk": 0, "title": "AI for Everyone"}}, "319": {"idx": 319, "content": "Why Now? .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 17, "chunk": 0, "title": "AI for Everyone"}}, "320": {"idx": 320, "content": "Why Now? .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 18, "chunk": 0, "title": "AI for Everyone"}}, "321": {"idx": 321, "content": "Why Now? .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 19, "chunk": 0, "title": "AI for Everyone"}}, "322": {"idx": 322, "content": "Why Now? .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 20, "chunk": 0, "title": "AI for Everyone"}}, "323": {"idx": 323, "content": "Why Now? .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 21, "chunk": 0, "title": "AI for Everyone"}}, "324": {"idx": 324, "content": "Why Now? .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 22, "chunk": 0, "title": "AI for Everyone"}}, "325": {"idx": 325, "content": "The Rise of Fast Computers So, the rise of fast computers with specialized processors such as graphics  processing units or GPUs has enabled many companies, not just giant tech  companies, but many many other companies to be able to train large neural  nets on a large enough amount of data in order to get very good performance  and drive business value. .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 23, "chunk": 0, "title": "AI for Everyone"}}, "326": {"idx": 326, "content": "What is the most important  idea in AI? .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 24, "chunk": 0, "title": "AI for Everyone"}}, "327": {"idx": 327, "content": "Machine Learning .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 25, "chunk": 0, "title": "AI for Everyone"}}, "328": {"idx": 328, "content": "What is Supervised Learning? .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 26, "chunk": 0, "title": "AI for Everyone"}}, "329": {"idx": 329, "content": "A to B mappings  Input to Output mappings .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 27, "chunk": 0, "title": "AI for Everyone"}}, "330": {"idx": 330, "content": "What enables machine learning  to work so well? .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 28, "chunk": 0, "title": "AI for Everyone"}}, "331": {"idx": 331, "content": "What is Data .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 29, "chunk": 0, "title": "AI for Everyone"}}, "332": {"idx": 332, "content": "A Table of Data (Dataset) Size of House (Square Feet) Price ($1000) 523 115 645 150 708 210 1034 280 2290 355 2545 440 A B .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 30, "chunk": 0, "title": "AI for Everyone"}}, "333": {"idx": 333, "content": "A Table of Data (Dataset) Size of House (Square Feet) # of Bedrooms Price ($1000) 523 1 115 645 1 150 708 2 210 1034 3 280 2290 4 355 2545 4 440 A B .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 31, "chunk": 0, "title": "AI for Everyone"}}, "334": {"idx": 334, "content": "Data is often unique to your business Data is often unique to your business, and this is an example of a dataset that  a real estate agency might have that they tried to help price houses.  It's up to you to decide what is A and what is B, and how to choose these  definitions of A and B to make it valuable for your business. .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 32, "chunk": 0, "title": "AI for Everyone"}}, "335": {"idx": 335, "content": "Another example If you have a certain budget and you want to decide what is the size of house  you can afford, then you might decide that the input A is how much does  someone spend and B is just the size of the house in square feet, and that  would be a totally different choice of A and B that tells you, given a certain  budget, what's the size of the house you should be maybe looking at. .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 33, "chunk": 0, "title": "AI for Everyone"}}, "336": {"idx": 336, "content": "A Table of Data (Dataset) Size of House (Square Feet) # of Bedrooms Price ($1000) 523 1 115 645 1 150 708 2 210 1034 3 280 2290 4 355 2545 4 440 B A .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 34, "chunk": 0, "title": "AI for Everyone"}}, "337": {"idx": 337, "content": "Acquiring data \u25cf Manual labeling .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 35, "chunk": 0, "title": "AI for Everyone"}}, "338": {"idx": 338, "content": "Acquiring data \u25cf From observing behaviors of humans User ID Time Price ($) Purchased 4783 Jan 21 08:15.20 7.95 yes 3893 Mar 3 11:30.15 10.00 yes 8384 Jun 11 14:15.05 9.50 no 0931 Aug 2 20:30.55 12.90 yes .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 36, "chunk": 0, "title": "AI for Everyone"}}, "339": {"idx": 339, "content": "Acquiring data \u25cf From observing behaviors of machines Machine Temperature Pressure (psi) Machine Fault 17987 60 7.65 N 34672 100 25.50 N 08542 140 75.50 Y 98536 165 125 Y Input A Input B .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 37, "chunk": 0, "title": "AI for Everyone"}}, "340": {"idx": 340, "content": "\u25cf Download from websites / partnerships \u25cbThanks to the open internet you can find so many datasets available  for free online \u25a0Computer vision or image datasets \u25a0Self driving car datasets \u25a0Speech recognition datasets \u25a0Medical imaging datasets \u25cbKeep in mind licensing and copyright Acquiring data .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 38, "chunk": 0, "title": "AI for Everyone"}}, "341": {"idx": 341, "content": "Use and misuse of  data Give me three years to build up my  IT team, we're collecting so much  data.  Then after three years, I'll have this  perfect dataset.  We'll do AI then. What\u2019s wrong with this  approach? .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 39, "chunk": 0, "title": "AI for Everyone"}}, "342": {"idx": 342, "content": "Use and misuse of  data It turns out that's a really bad  strategy.  Once you've started collecting  some data, go ahead and start  showing it or feeding it to an AI  team.  Then the AI team can give  feedback to your IT team on what  types of data to collect and what  types of IT infrastructure to keep  on building. .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 40, "chunk": 0, "title": "AI for Everyone"}}, "343": {"idx": 343, "content": "Example Maybe an AI team can look at your factory  data and say, \"Hey. You know what? If you  can collect data from this big  manufacturing machine, not just once every  ten minutes, but instead once every one  minute, then we could do a much better job  building a preventative maintenance  systems for you.\u201d Machine Temperatur e Pressure  (psi) Machine  Fault 17987 60 7.65 N 34672 100 25.50 N 08542 140 75.50 Y 98536 165 125 Y Input A Input B .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 41, "chunk": 0, "title": "AI for Everyone"}}, "344": {"idx": 344, "content": "Use and misuse of  data \"Hey, I have so much data. Surely,  an AI team can make it valuable.\" What\u2019s wrong with this  statement? .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 42, "chunk": 0, "title": "AI for Everyone"}}, "345": {"idx": 345, "content": "Unfortunately, this doesn't always  work out.  More data is usually better than  less data, but I wouldn't take it for  granted that just because you have  many terabytes or gigabytes of  data, that an AI team can actually  make that valuable.  Don't throw data at an AI team  and assume it will be valuable. Use and misuse of  data .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 43, "chunk": 0, "title": "AI for Everyone"}}, "346": {"idx": 346, "content": "Not a  cat Not a  cat Cat Cat Data is Messy .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 44, "chunk": 0, "title": "AI for Everyone"}}, "347": {"idx": 347, "content": "If you have bad data, then the AI  will learn inaccurate things. Data problems: \u25cf Incorrect labels \u25cf Missing values Multiple types of data \u25cf Unstructured Data: Images,  audio, text Data is Messy .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 45, "chunk": 0, "title": "AI for Everyone"}}, "348": {"idx": 348, "content": "Example You can have incorrect labels or just  incorrect data. For example, this house is  probably not going to sell for $0.1 just for  one dollar. Or, data can also have missing values such  as we have here a whole bunch of unknown  values. This is structured data. Size of House  (Square Feet) # of  Bedrooms Price  ($1000) 523 1 115 645 1 0.001 708 unknown 210 1034 3 unknown unknown 4 355 2545 unknown 440 .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 46, "chunk": 0, "title": "AI for Everyone"}}, "349": {"idx": 349, "content": "Machine Learning vs Data Science Size of House (Square Feet) # of Bedrooms # of Bathrooms Newly Renovated Price ($1000) 523 1 2 N 115 645 1 3 N 150 708 2 1 N 210 1034 3 3 Y 280 2290 4 4 N 355 2545 4 5 Y 440 A B .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 47, "chunk": 0, "title": "AI for Everyone"}}, "350": {"idx": 350, "content": "Running AI System A software that which automatically returns output B for  input A. If you have an AI system running, serving dozens or hundreds of thousands or  millions of users, that's usually a machine-learning system. .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 48, "chunk": 0, "title": "AI for Everyone"}}, "351": {"idx": 351, "content": "If you want to have a team analyze your dataset in order to gain insights. The  output of a data science project is a set of insights that can help you  make business decisions So, a team might come up with conclusions like: \u25cf \"Hey, did you know if you have two houses of a similar size, they've a similar  square footage, if the house has three bedrooms, then they cost a lot more  than the house of two bedrooms, even if the square for this is the same.\"  Data Science .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 49, "chunk": 0, "title": "AI for Everyone"}}, "352": {"idx": 352, "content": "Data Science \u25cf \"Did you know that newly renovated homes have a 15% premium, and this can  help you make decisions such as, given a similar square footage, do you want  to build a two bedroom or three bedroom size in order to maximize value? \"  \u25cf \"Is it worth an investment to renovate a home in the hope that the  renovation increases the price you can sell a house for?\"  The output of a data science project is a set of insights that can help you  make business decisions, such as what type of house to build or whether to  invest in renovation. .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 50, "chunk": 0, "title": "AI for Everyone"}}, "353": {"idx": 353, "content": "Machine Learning vs Data Science Machine Learning \u201cField of study that gives computers the  ability to learn without being explicitly  programmed.\u201d - Arthur Samuel (1959) A machine learning project will often  result in a piece of software that runs,  that outputs B given A. .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 51, "chunk": 0, "title": "AI for Everyone"}}, "354": {"idx": 354, "content": "Data science is the science of extracting knowledge and insights from data. So, the output of a data science project is often a slide deck, the presentation  summarizes conclusions for executives to take business actions or summarizes  conclusions for a product team to decide how to improve a website. Formal Definition of Data Science .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 52, "chunk": 0, "title": "AI for Everyone"}}, "355": {"idx": 355, "content": "Large platforms have AI that quickly tells them what\u2019s the ad you\u2019re most likely  to click on. This is a machine learning system. It inputs information about the  user and about the ad and outputs whether the user will click on the ad or not. These systems run 24/7 and drive ad revenue for these platforms. Example of ML vs DS in the online ad industry .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 53, "chunk": 0, "title": "AI for Everyone"}}, "356": {"idx": 356, "content": "Example of ML vs DS in the online ad industry If analyzing data tells you, for example, that the travel industry is not buying a  lot of ads, but if you send more salespeople to sell ads to travel companies,  you could convince them to use more advertising, then that would be an  example of a data science project.  The data science conclusion results in the executives deciding to ask a sales  team to spend more time reaching out to the travel industry. .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 54, "chunk": 0, "title": "AI for Everyone"}}, "357": {"idx": 357, "content": "Deep Learning .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 55, "chunk": 0, "title": "AI for Everyone"}}, "358": {"idx": 358, "content": "AI and related disciplines \u25cf Machine Learning \u25cf Data Science \u25cf Deep Learning / Neural Network \u25cf Supervised Learning \u25cf Un supervised learning \u25cf Reinforcement Learning .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 56, "chunk": 0, "title": "AI for Everyone"}}, "359": {"idx": 359, "content": "What makes a company AI company? \u25cfStrategic data acquisition \u25cfUnified datawarehouse \u25cfPervasive automation \u25cfNew roles such as MLE .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 57, "chunk": 0, "title": "AI for Everyone"}}, "360": {"idx": 360, "content": "AI Transformation .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 58, "chunk": 0, "title": "AI for Everyone"}}, "361": {"idx": 361, "content": "Deciding about a new project \u25cfTechnical diligence \u25cf Is it feasible project? \u25cf Can AI do that? \u25cfPretty much any thing you can do with a second of  thought can be automated using supervised learning .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 59, "chunk": 0, "title": "AI for Everyone"}}, "362": {"idx": 362, "content": "Supervised learning tasks .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 60, "chunk": 0, "title": "AI for Everyone"}}, "363": {"idx": 363, "content": ".", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 61, "chunk": 0, "title": "AI for Everyone"}}, "364": {"idx": 364, "content": "Examples of what ML can and can\u2019t do? \u25cf Identifying the intent of the customer - Possible \u25cf Writing an emphatic response to customer\u2019s email \u2013 Not  possible or difficult .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 62, "chunk": 0, "title": "AI for Everyone"}}, "365": {"idx": 365, "content": "Technical diligence rules \u25cf You are learning a simple concept \u25cf Do you have large training data .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 63, "chunk": 0, "title": "AI for Everyone"}}, "366": {"idx": 366, "content": "More examples \u25cf Self driving car \u25cbInput is from sensors, camera \u25cbOutput where are the other cars \u25cf Recognizing gesture of traffic police, construction work,  people\u2013 not possible \u25cf Critical application requires good accuracy .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 64, "chunk": 0, "title": "AI for Everyone"}}, "367": {"idx": 367, "content": "X-ray diagnosis \u25cf Diagnosing a disease from X-ray images\u2013 possible \u25cf Diagnosing a disease after reading a book .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 65, "chunk": 0, "title": "AI for Everyone"}}, "368": {"idx": 368, "content": "Strengths and weakness of ML \u25cf Works when, \u25cf Learning a simple concept \u25cf Lots of data available \u25cf Doesn\u2019t work when, \u25cf Learning a complex concept \u25cf Asked to work on new type of data such as X-ray images in  different conditions and angles .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 66, "chunk": 0, "title": "AI for Everyone"}}, "369": {"idx": 369, "content": "Demand prediction based on price \u25cf Price -> Demand can be modeled using a neural network using a  neuron \u25cf (Perceptron model) \u25cf Network of neurons (ANN) \u25cb Price \u25cb Shipping Cost \u25cb Marketing \u25cb Meterial .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 67, "chunk": 0, "title": "AI for Everyone"}}, "370": {"idx": 370, "content": ".", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 68, "chunk": 0, "title": "AI for Everyone"}}, "371": {"idx": 371, "content": "Face recognition \u25cf Pictures comprise pixels \u25cf Color images and channels \u25cf A neural network corresponds to pixels \u25cf Earlier layers will detect edges, then lobes and then objects .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 69, "chunk": 0, "title": "AI for Everyone"}}, "372": {"idx": 372, "content": ".", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 70, "chunk": 0, "title": "AI for Everyone"}}, "373": {"idx": 373, "content": "Speech Recognition .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 71, "chunk": 0, "title": "AI for Everyone"}}, "374": {"idx": 374, "content": "Key steps of Echo / Alexa \u25cf Collect data \u25cf Labelled voice  \u25cf Train model \u25cf Iterate many times \u25cf Deploy the model \u25a0 Get more data and update model .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 72, "chunk": 0, "title": "AI for Everyone"}}, "375": {"idx": 375, "content": ".", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 73, "chunk": 0, "title": "AI for Everyone"}}, "376": {"idx": 376, "content": ".", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 74, "chunk": 0, "title": "AI for Everyone"}}, "377": {"idx": 377, "content": ".", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 75, "chunk": 0, "title": "AI for Everyone"}}, "378": {"idx": 378, "content": "Machine Learning changing job functions \u25cf Sales \u25a0 Identifying sales opportunities \u25a0 Prioritizing \u25cf Manufacturing line manager \u25a0 Optimize manufacturing \u25a0 Machine learning can spot defects \u25cf Recruiting \u25a0 Identify how people prefer recruitment \u25a0 Spot good candidates .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 76, "chunk": 0, "title": "AI for Everyone"}}, "379": {"idx": 379, "content": "\u25cf Marketing \u25cf Optimize website \u25cf A/B testing \u25cf Recommendation system \u25cb Agriculture \u25cf What to plant? \u25cf Precision agriculture .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 77, "chunk": 0, "title": "AI for Everyone"}}, "380": {"idx": 380, "content": "How to chose an AI project? .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 78, "chunk": 0, "title": "AI for Everyone"}}, "381": {"idx": 381, "content": "Brainstorming framework \u25cf Automate task rather than job \u25cf Automating call center: picking phone, emails, issue refund, call  routing \u25cf Automating radiologist: X-ray, mentoring other doctors,  consulting,  \u25cf Main drivers of business value \u25cf What are  the main pain points in your business? .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 79, "chunk": 0, "title": "AI for Everyone"}}, "382": {"idx": 382, "content": "Is it always necessary to have big data? \u25cf Having more data is good \u25cf With small datasets you can make progress \u25cf 10, 100 or 1000 data points can be a good start .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 80, "chunk": 0, "title": "AI for Everyone"}}, "383": {"idx": 383, "content": ".", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 81, "chunk": 0, "title": "AI for Everyone"}}, "384": {"idx": 384, "content": "Ethical diligence \u25cf Is this going to make society better? .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 82, "chunk": 0, "title": "AI for Everyone"}}, "385": {"idx": 385, "content": "Build Vs Buy \u25cf ML projects can be inhoused or outsourced \u25cf DS projects are generally inhoused \u25cf Buy industry standard, only build specialized products .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 83, "chunk": 0, "title": "AI for Everyone"}}, "386": {"idx": 386, "content": "How to work with AI team \u25cf Specify your acceptance criteria \u25cf 95% accuracy \u25cf Training, validation and Test dataset \u25cf Don\u2019t expect 100% accuracy \u25cf Limitations of ML \u25cf Insufficient data \u25cf Mislabeled data \u25cf Ambiguous labels (human perception) .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 84, "chunk": 0, "title": "AI for Everyone"}}, "387": {"idx": 387, "content": "Machine Learning frameworks .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 85, "chunk": 0, "title": "AI for Everyone"}}, "388": {"idx": 388, "content": "CPU Vs GPU Edge  Deployment .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 86, "chunk": 0, "title": "AI for Everyone"}}, "389": {"idx": 389, "content": "Case Studies .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 87, "chunk": 0, "title": "AI for Everyone"}}, "390": {"idx": 390, "content": ".", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 88, "chunk": 0, "title": "AI for Everyone"}}, "391": {"idx": 391, "content": "Steps or AI pipeline \u25cf Trigger word: Hey Device \u25cf Speech Recognition: Tell me a joke \u25cf Intent Recognition: joke, time, music, weather \u25cf Log of training instances, variation in text \u25cf Execute joke .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 89, "chunk": 0, "title": "AI for Everyone"}}, "392": {"idx": 392, "content": "Activity \u25cf Hey device, set timer for 10 minutes \u25cb What is the intent? \u25cb Extract duration \u25cb What command is to execute .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 90, "chunk": 0, "title": "AI for Everyone"}}, "393": {"idx": 393, "content": "Smart speaker functions \u25cf Play music \u25cf Volume up/ down \u25cf Make call \u25cf Current time \u25cf Units conversion \u25cf Simple question \u25cf These specialized execution routines are written by software engineer .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 91, "chunk": 0, "title": "AI for Everyone"}}, "394": {"idx": 394, "content": "Self driving car .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 92, "chunk": 0, "title": "AI for Everyone"}}, "395": {"idx": 395, "content": ".", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 93, "chunk": 0, "title": "AI for Everyone"}}, "396": {"idx": 396, "content": "AI teams \u25cf AI team may have 100s of engineers \u25cf A small team can have four or five members \u25cf Example roles \u25cf Software Engineers \u25a0 Execute joke, Set timer \u25cf Machine Learning Engineer \u25cf Machine Learning Researcher \u25a0 Extend state-of-the-art \u25a0 Applied ML scientist in between ML researcher and ML Engineer \u25cf Data Scientist \u25a0 Provide insights .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 94, "chunk": 0, "title": "AI for Everyone"}}, "397": {"idx": 397, "content": "\u25a0 Data Engineer \u25cb Organize data \u25cb Data is saved in cost effective way \u25cb We have lot of data, scalability is important \u25cf AI Product Manager \u25cb What to build and feasible .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 95, "chunk": 0, "title": "AI for Everyone"}}, "398": {"idx": 398, "content": "Get started with a small team \u25cf 1 Software engineer \u25cf 1 ML engineer / Data scientist \u25cf No body but your self .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 96, "chunk": 0, "title": "AI for Everyone"}}, "399": {"idx": 399, "content": "AI Transformation playbook \u25cf Execute a pilot project to gain momentum \u25cf Build an in-house AI team \u25cf Provide broad AI training \u25cf Develop an AI strategy \u25cf Develop internal and external communication .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 97, "chunk": 0, "title": "AI for Everyone"}}, "400": {"idx": 400, "content": "Execute pilot project \u25cf Start the fly wheel \u25cf Show traction with in 6-12 months \u25cf Can be in-house or out-sourced .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 98, "chunk": 0, "title": "AI for Everyone"}}, "401": {"idx": 401, "content": "Build an in-house AI team \u25cf Develop tools that could be useful company wide \u25cf Under CIO, CTO,CDO, CAIO .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 99, "chunk": 0, "title": "AI for Everyone"}}, "402": {"idx": 402, "content": "Provide broad AI training .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 100, "chunk": 0, "title": "AI for Everyone"}}, "403": {"idx": 403, "content": "Resources \u25cf Online courses \u25cf Books \u25cf Curate rather than create content .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 101, "chunk": 0, "title": "AI for Everyone"}}, "404": {"idx": 404, "content": "Develop an AI strategy \u25cf Leverage AI to create an advantage specific to your company \u25cf Design strategy that align with virtuous cycle of AI \u25cf Blue River \u2013 precision agriculture .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 102, "chunk": 0, "title": "AI for Everyone"}}, "405": {"idx": 405, "content": "\u25cf AI needs to be specialized or verticalized to your industry sectory \u25cf Don\u2019t compete with giants \u25cf Creating a strategy \u25cf Strategic data acquisition \u25cf Unified data warehouse \u2013 Pull data into single repository, software can  connect the dots \u25cf Create network effect and platform advantages \u25cf Uber, Careem, Facebook .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 103, "chunk": 0, "title": "AI for Everyone"}}, "406": {"idx": 406, "content": "\u25cf Low cost strategy \u25cf High value strategy .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 104, "chunk": 0, "title": "AI for Everyone"}}, "407": {"idx": 407, "content": "Develop internal and external communications \u25cf Investor relations \u25cf Government relations \u25cf Consumer / user education \u25cf Talent / recruitment \u25cf Internal communication .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 105, "chunk": 0, "title": "AI for Everyone"}}, "408": {"idx": 408, "content": "Common pitfalls .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 106, "chunk": 0, "title": "AI for Everyone"}}, "409": {"idx": 409, "content": ".", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 107, "chunk": 0, "title": "AI for Everyone"}}, "410": {"idx": 410, "content": "Take your first step \u25cf Get friends to learn about AI \u25cf Start brainstorming projects \u25cf Hire a few ML / DS to help \u25cf Hire or appoint an AI leader (VP AI, CAIO) \u25cf Discuss with CEO about possibilities of AI transformation .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 108, "chunk": 0, "title": "AI for Everyone"}}, "411": {"idx": 411, "content": "AI Application areas \u25cf Computer Vision \u25cb Image Classification / Object recognition \u25a0 Face recognition \u25cb Object detection \u25cb Image segmentation \u25cb Tracking .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 109, "chunk": 0, "title": "AI for Everyone"}}, "412": {"idx": 412, "content": "Natural language processing \u25cf Text classification  (Spam / Non spam) \u25cf Sentiment recognition \u25cf Information retrieval \u25cf Web search \u25cf Named entity recognition \u25cf Machine translation \u25cf Part of speech tagging .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 110, "chunk": 0, "title": "AI for Everyone"}}, "413": {"idx": 413, "content": "\u25cf Parsing .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 111, "chunk": 0, "title": "AI for Everyone"}}, "414": {"idx": 414, "content": "Speech \u25cf Speech to text \u25cf Trigger / wake word detection \u25cf Speaker ID \u25cf Speech synthesis (text-to-speech / TTS) .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 112, "chunk": 0, "title": "AI for Everyone"}}, "415": {"idx": 415, "content": "Robotics .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 113, "chunk": 0, "title": "AI for Everyone"}}, "416": {"idx": 416, "content": "General machine learning .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 114, "chunk": 0, "title": "AI for Everyone"}}, "417": {"idx": 417, "content": "Unsupervised learning .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 115, "chunk": 0, "title": "AI for Everyone"}}, "418": {"idx": 418, "content": "\u25cf Supervised learning needs lot of data \u25cf 10,000 defected coffee mug, human can easily do that with few  examples .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 116, "chunk": 0, "title": "AI for Everyone"}}, "419": {"idx": 419, "content": "Transfer learning .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 117, "chunk": 0, "title": "AI for Everyone"}}, "420": {"idx": 420, "content": "Reinforcement learning \u25cf Also useful in  Games \u25cf Not as much as  economic value  as supervised  learning .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 118, "chunk": 0, "title": "AI for Everyone"}}, "421": {"idx": 421, "content": "Generative Adversarial Network (GAN) \u25cf Synthesize new images from scratch \u25cf Entertainment industry, film, animation .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 119, "chunk": 0, "title": "AI for Everyone"}}, "422": {"idx": 422, "content": "Knowledge graph .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 120, "chunk": 0, "title": "AI for Everyone"}}, "423": {"idx": 423, "content": "AI & Society \u25cf AI is super power \u25cf Goldilock rule \u25a0 Neither too optimistic nor pessimistic \u25cf Don\u2019t over spend on unnecessary danger \u25cf AI winter \u25cf AI can\u2019t do every thing, but will transform industries .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 121, "chunk": 0, "title": "AI for Everyone"}}, "424": {"idx": 424, "content": "Limitations of AI \u25cf Performance limitations \u25cf With small amount of data \u25cf Explainability is hard (sometimes  doable): How should we trust \u25a0 Humans are also not good at explaining \u25a0 Barrier to acceptance \u25cf Biased through biased data \u25cf Adversarial attacks .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 122, "chunk": 0, "title": "AI for Everyone"}}, "425": {"idx": 425, "content": "AI can learn unhealthy stereotype \u25cf Learn from internet \u25cb Man: Woman as Father: Mother \u25cb Man: Woman as King: Queen \u25cb Man : Computer programmer as women: Home maker \u25cb Man and woman can equally become programmer .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 123, "chunk": 0, "title": "AI for Everyone"}}, "426": {"idx": 426, "content": "Why bias matters \u25cf Hiring tool that discriminates against woman \u25cf Facial recognition working better for specific ehtnicity \u25cf Bank loan approavals \u25cf Toxic effect of reinforcing unhealthy stereotypes .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 124, "chunk": 0, "title": "AI for Everyone"}}, "427": {"idx": 427, "content": "Combating bias \u25cf Technical solution \u25cf Zero out bias \u25cf Use less biased or more inclusive data \u25cf Transparency or auditing process \u25cf Diverse workforce \u25a0 Creates less biased applications .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 125, "chunk": 0, "title": "AI for Everyone"}}, "428": {"idx": 428, "content": "Adversarial attacks \u25cf Spam Filters \u25cf Hate speech filter .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 126, "chunk": 0, "title": "AI for Everyone"}}, "429": {"idx": 429, "content": "Physical attacks .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 127, "chunk": 0, "title": "AI for Everyone"}}, "430": {"idx": 430, "content": "Adversarial defenses \u25cf Cost to defend \u25cf Slow speed \u25cf May not be any incentive to attack, so should we invest in defense? \u25cf Zero-sum against adversaries .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 128, "chunk": 0, "title": "AI for Everyone"}}, "431": {"idx": 431, "content": "Adverse uses of AI \u25cf Deep Fakes \u25a0 Synthesizing videos \u25a0 Video of Obama \u25cb Undermining of democracy and privacy \u25a0 Oppressive surveillance \u25cb Generating fake comments \u25cb Spam Vs anti-spam, Fraud Vs anti-fraud .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 129, "chunk": 0, "title": "AI for Everyone"}}, "432": {"idx": 432, "content": "AI & Developing economy \u25cf Developing economies gradually moved up the ladder \u25cf Lower end ladder are susceptible to automation such as agriculture \u25cf Trampoline to move higher rungs \u25a0 Leapfrog \u25a0 Example of mobile phone \u25a0 Mobile payments \u25cb Online education .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 130, "chunk": 0, "title": "AI for Everyone"}}, "433": {"idx": 433, "content": "How developing economies can build AI? \u25cf US and China leading \u25cf But AI communities are still immature \u25cf Focus on AI to strengthen country\u2019s vertical industries \u25cf Instead of focusing on AI in general, use AI where you are already good at \u25cf Public private partnership \u25cf Invest in education .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 131, "chunk": 0, "title": "AI for Everyone"}}, "434": {"idx": 434, "content": "AI and impact on jobs \u25cf AI is automation on steroids \u25cf Jobs displaced by 2030 \u25cf 400-800 mn \u25cf Jobs created by 2030 \u25cf 555- 890 mn \u25cf Is your job amenable to automation? .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 132, "chunk": 0, "title": "AI for Everyone"}}, "435": {"idx": 435, "content": ".", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 133, "chunk": 0, "title": "AI for Everyone"}}, "436": {"idx": 436, "content": "Some solutions to counter AI impact on jobs \u25cf Conditional basic income: provide a safety net \u25cf Life long learning society \u25cf Political solutions \u25cb Legalization \u25cf Work at intersection of your current joband AI .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 134, "chunk": 0, "title": "AI for Everyone"}}, "437": {"idx": 437, "content": "Summary \u25cf What is AI? \u25cf Building AI projects \u25cf Building AI in your company \u25cf AI & society .", "metadata": {"source": "data/AI for Everyone.pdf", "page_number": 135, "chunk": 0, "title": "AI for Everyone"}}, "438": {"idx": 438, "content": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks Patrick Lewis\u2020\u2021, Ethan Perez\u22c6, Aleksandra Piktus\u2020, Fabio Petroni\u2020, Vladimir Karpukhin\u2020, Naman Goyal\u2020, Heinrich K\u00fcttler\u2020, Mike Lewis\u2020, Wen-tau Yih\u2020, Tim Rockt\u00e4schel\u2020\u2021, Sebastian Riedel\u2020\u2021, Douwe Kiela\u2020 \u2020Facebook AI Research; \u2021University College London; \u22c6New York University; plewis@fb.com Abstract Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when \ufb01ne-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-speci\ufb01c architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pretrained models with a differentiable access mechanism to explicit non-parametric memory have so far been only investigated for extractive downstream tasks. We explore a general-purpose \ufb01ne-tuning recipe for retrieval-augmented generation (RAG) \u2014 models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever.", "metadata": {"source": "data/21_patrick_2020_RAG.pdf", "page_number": 1, "chunk": 0, "title": ""}}, "439": {"idx": 439, "content": "We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, and another which can use different passages per token. We \ufb01ne-tune and evaluate our models on a wide range of knowledgeintensive NLP tasks and set the state of the art on three open domain QA tasks, outperforming parametric seq2seq models and task-speci\ufb01c retrieve-and-extract architectures. For language generation tasks, we \ufb01nd that RAG models generate more speci\ufb01c, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline. 1 Introduction Pre-trained neural language models have been shown to learn a substantial amount of in-depth knowledge from data [47]. They can do so without any access to an external memory, as a parameterized implicit knowledge base [51, 52]. While this development is exciting, such models do have downsides: They cannot easily expand or revise their memory, can\u2019t straightforwardly provide insight into their predictions, and may produce \u201challucinations\u201d [38]. Hybrid models that combine parametric memory with non-parametric (i.e., retrieval-based) memories [20, 26, 48] can address some of these issues because knowledge can be directly revised and expanded, and accessed knowledge can be inspected and interpreted.", "metadata": {"source": "data/21_patrick_2020_RAG.pdf", "page_number": 1, "chunk": 1, "title": ""}}, "440": {"idx": 440, "content": "REALM [20] and ORQA [31], two recently introduced models that combine masked language models [8] with a differentiable retriever, have shown promising results, arXiv:2005.11401v4  [cs.CL]  12 Apr 2021 .", "metadata": {"source": "data/21_patrick_2020_RAG.pdf", "page_number": 1, "chunk": 2, "title": ""}}, "441": {"idx": 441, "content": "The Divine Comedy (x) q Query Encoder q(x) MIPS p\u03b8 Generator\u00a0p\u03b8 (Parametric) Marginalize This 14th century work is divided into 3 sections: \"Inferno\", \"Purgatorio\" & \"Paradiso\"         (y) End-to-End Backprop through q and\u00a0p\u03b8 Barack Obama was born in Hawaii.(x) Fact Veri\ufb01cation: Fact Query supports (y) Question Generation Fact Veri\ufb01cation: Label Generation Document Index Define \"middle ear\"(x) Question Answering: Question Query The middle ear includes the tympanic cavity and the three ossicles.  (y) Question Answering: Answer Generation Retriever p\u03b7 (Non-Parametric) z4 z3 z2 z1 d(z) Jeopardy Question Generation: Answer Query Figure 1: Overview of our approach. We combine a pre-trained retriever (Query Encoder + Document Index) with a pre-trained seq2seq model (Generator) and \ufb01ne-tune end-to-end. For query x, we use Maximum Inner Product Search (MIPS) to \ufb01nd the top-K documents zi. For \ufb01nal prediction y, we treat z as a latent variable and marginalize over seq2seq predictions given different documents. but have only explored open-domain extractive question answering. Here, we bring hybrid parametric and non-parametric memory to the \u201cworkhorse of NLP,\u201d i.e. sequence-to-sequence (seq2seq) models. We endow pre-trained, parametric-memory generation models with a non-parametric memory through a general-purpose \ufb01ne-tuning approach which we refer to as retrieval-augmented generation (RAG).", "metadata": {"source": "data/21_patrick_2020_RAG.pdf", "page_number": 2, "chunk": 0, "title": ""}}, "442": {"idx": 442, "content": "We build RAG models where the parametric memory is a pre-trained seq2seq transformer, and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We combine these components in a probabilistic model trained end-to-end (Fig. 1). The retriever (Dense Passage Retriever [26], henceforth DPR) provides latent documents conditioned on the input, and the seq2seq model (BART [32]) then conditions on these latent documents together with the input to generate the output. We marginalize the latent documents with a top-K approximation, either on a per-output basis (assuming the same document is responsible for all tokens) or a per-token basis (where different documents are responsible for different tokens). Like T5 [51] or BART, RAG can be \ufb01ne-tuned on any seq2seq task, whereby both the generator and retriever are jointly learned. There has been extensive previous work proposing architectures to enrich systems with non-parametric memory which are trained from scratch for speci\ufb01c tasks, e.g. memory networks [64, 55], stackaugmented networks [25] and memory layers [30]. In contrast, we explore a setting where both parametric and non-parametric memory components are pre-trained and pre-loaded with extensive knowledge. Crucially, by using pre-trained access mechanisms, the ability to access knowledge is present without additional training.", "metadata": {"source": "data/21_patrick_2020_RAG.pdf", "page_number": 2, "chunk": 1, "title": ""}}, "443": {"idx": 443, "content": "Our results highlight the bene\ufb01ts of combining parametric and non-parametric memory with generation for knowledge-intensive tasks\u2014tasks that humans could not reasonably be expected to perform without access to an external knowledge source. Our RAG models achieve state-of-the-art results on open Natural Questions [29], WebQuestions [3] and CuratedTrec [2] and strongly outperform recent approaches that use specialised pre-training objectives on TriviaQA [24]. Despite these being extractive tasks, we \ufb01nd that unconstrained generation outperforms previous extractive approaches. For knowledge-intensive generation, we experiment with MS-MARCO [1] and Jeopardy question generation, and we \ufb01nd that our models generate responses that are more factual, speci\ufb01c, and diverse than a BART baseline. For FEVER [56] fact veri\ufb01cation, we achieve results within 4.3% of state-of-the-art pipeline models which use strong retrieval supervision. Finally, we demonstrate that the non-parametric memory can be replaced to update the models\u2019 knowledge as the world changes.1 2 Methods We explore RAG models, which use the input sequence x to retrieve text documents z and use them as additional context when generating the target sequence y.", "metadata": {"source": "data/21_patrick_2020_RAG.pdf", "page_number": 2, "chunk": 2, "title": ""}}, "444": {"idx": 444, "content": "As shown in Figure 1, our models leverage two components: (i) a retriever p\u03b7(z|x) with parameters \u03b7 that returns (top-K truncated) distributions over text passages given a query x and (ii) a generator p\u03b8(yi|x, z, y1:i\u22121) parametrized 1Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transformers Library [66] and can be found at https://github.com/huggingface/transformers/blob/master/ examples/rag/. An interactive demo of RAG models can be found at https://huggingface.co/rag/ 2 .", "metadata": {"source": "data/21_patrick_2020_RAG.pdf", "page_number": 2, "chunk": 3, "title": ""}}, "445": {"idx": 445, "content": "by \u03b8 that generates a current token based on a context of the previous i \u22121 tokens y1:i\u22121, the original input x and a retrieved passage z. To train the retriever and generator end-to-end, we treat the retrieved document as a latent variable. We propose two models that marginalize over the latent documents in different ways to produce a distribution over generated text. In one approach, RAG-Sequence, the model uses the same document to predict each target token. The second approach, RAG-Token, can predict each target token based on a different document. In the following, we formally introduce both models and then describe the p\u03b7 and p\u03b8 components, as well as the training and decoding procedure. 2.1 Models RAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate the complete sequence. Technically, it treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized, pRAG-Sequence(y|x) \u2248 X z\u2208top-k(p(\u00b7|x)) p\u03b7(z|x)p\u03b8(y|x, z) = X z\u2208top-k(p(\u00b7|x)) p\u03b7(z|x) N Y i p\u03b8(yi|x, z, y1:i\u22121) RAG-Token Model In the RAG-Token model we can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer.", "metadata": {"source": "data/21_patrick_2020_RAG.pdf", "page_number": 3, "chunk": 0, "title": ""}}, "446": {"idx": 446, "content": "Concretely, the top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token, Formally, we de\ufb01ne: pRAG-Token(y|x) \u2248 N Y i X z\u2208top-k(p(\u00b7|x)) p\u03b7(z|x)p\u03b8(yi|x, z, y1:i\u22121) Finally, we note that RAG can be used for sequence classi\ufb01cation tasks by considering the target class as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent. 2.2 Retriever: DPR The retrieval component p\u03b7(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture: p\u03b7(z|x) \u221dexp \u0000d(z)\u22a4q(x) \u0001 d(z) = BERTd(z), q(x) = BERTq(x) where d(z) is a dense representation of a document produced by a BERTBASE document encoder [8], and q(x) a query representation produced by a query encoder, also based on BERTBASE. Calculating top-k(p\u03b7(\u00b7|x)), the list of k documents z with highest prior probability p\u03b7(z|x), is a Maximum Inner Product Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This retriever was trained to retrieve documents which contain answers to TriviaQA [24] questions and Natural Questions [29]. We refer to the document index as the non-parametric memory. 2.3 Generator: BART The generator component p\u03b8(yi|x, z, y1:i\u22121) could be modelled using any encoder-decoder.", "metadata": {"source": "data/21_patrick_2020_RAG.pdf", "page_number": 3, "chunk": 1, "title": ""}}, "447": {"idx": 447, "content": "We use BART-large [32], a pre-trained seq2seq transformer [58] with 400M parameters. To combine the input x with the retrieved content z when generating from BART, we simply concatenate them. BART was pre-trained using a denoising objective and a variety of different noising functions. It has obtained state-of-the-art results on a diverse set of generation tasks and outperforms comparably-sized T5 models [32]. We refer to the BART generator parameters \u03b8 as the parametric memory henceforth. 2.4 Training We jointly train the retriever and generator components without any direct supervision on what document should be retrieved. Given a \ufb01ne-tuning training corpus of input/output pairs (xj, yj), we 3 .", "metadata": {"source": "data/21_patrick_2020_RAG.pdf", "page_number": 3, "chunk": 2, "title": ""}}, "448": {"idx": 448, "content": "minimize the negative marginal log-likelihood of each target, P j \u2212log p(yj|xj) using stochastic gradient descent with Adam [28]. Updating the document encoder BERTd during training is costly as it requires the document index to be periodically updated as REALM does during pre-training [20]. We do not \ufb01nd this step necessary for strong performance, and keep the document encoder (and index) \ufb01xed, only \ufb01ne-tuning the query encoder BERTq and the BART generator. 2.5 Decoding At test time, RAG-Sequence and RAG-Token require different ways to approximate arg maxy p(y|x). RAG-Token The RAG-Token model can be seen as a standard, autoregressive seq2seq generator with transition probability: p\u2032 \u03b8(yi|x, y1:i\u22121) = P z\u2208top-k(p(\u00b7|x)) p\u03b7(zi|x)p\u03b8(yi|x, zi, y1:i\u22121) To decode, we can plug p\u2032 \u03b8(yi|x, y1:i\u22121) into a standard beam decoder. RAG-Sequence For RAG-Sequence, the likelihood p(y|x) does not break into a conventional pertoken likelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for each document z, scoring each hypothesis using p\u03b8(yi|x, z, y1:i\u22121). This yields a set of hypotheses Y , some of which may not have appeared in the beams of all documents. To estimate the probability of an hypothesis y we run an additional forward pass for each document z for which y does not appear in the beam, multiply generator probability with p\u03b7(z|x) and then sum the probabilities across beams for the marginals.", "metadata": {"source": "data/21_patrick_2020_RAG.pdf", "page_number": 4, "chunk": 0, "title": ""}}, "449": {"idx": 449, "content": "We refer to this decoding procedure as \u201cThorough Decoding.\u201d For longer output sequences, |Y | can become large, requiring many forward passes. For more ef\ufb01cient decoding, we can make a further approximation that p\u03b8(y|x, zi) \u22480 where y was not generated during beam search from x, zi. This avoids the need to run additional forward passes once the candidate set Y has been generated. We refer to this decoding procedure as \u201cFast Decoding.\u201d 3 Experiments We experiment with RAG in a wide range of knowledge-intensive tasks. For all experiments, we use a single Wikipedia dump for our non-parametric knowledge source. Following Lee et al. [31] and Karpukhin et al. [26], we use the December 2018 dump. Each Wikipedia article is split into disjoint 100-word chunks, to make a total of 21M documents. We use the document encoder to compute an embedding for each document, and build a single MIPS index using FAISS [23] with a Hierarchical Navigable Small World approximation for fast retrieval [37]. During training, we retrieve the top k documents for each query. We consider k \u2208{5, 10} for training and set k for test time using dev data. We now discuss experimental details for each task. 3.1 Open-domain Question Answering Open-domain question answering (QA) is an important real-world application and common testbed for knowledge-intensive tasks [20]. We treat questions and answers as input-output text pairs (x, y) and train RAG by directly minimizing the negative log-likelihood of answers.", "metadata": {"source": "data/21_patrick_2020_RAG.pdf", "page_number": 4, "chunk": 1, "title": ""}}, "450": {"idx": 450, "content": "We compare RAG to the popular extractive QA paradigm [5, 7, 31, 26], where answers are extracted spans from retrieved documents, relying primarily on non-parametric knowledge. We also compare to \u201cClosed-Book QA\u201d approaches [52], which, like RAG, generate answers, but which do not exploit retrieval, instead relying purely on parametric knowledge. We consider four popular open-domain QA datasets: Natural Questions (NQ) [29], TriviaQA (TQA) [24]. WebQuestions (WQ) [3] and CuratedTrec (CT) [2]. As CT and WQ are small, we follow DPR [26] by initializing CT and WQ models with our NQ RAG model. We use the same train/dev/test splits as prior work [31, 26] and report Exact Match (EM) scores. For TQA, to compare with T5 [52], we also evaluate on the TQA Wiki test set. 3.2 Abstractive Question Answering RAG models can go beyond simple extractive QA and answer questions with free-form, abstractive text generation. To test RAG\u2019s natural language generation (NLG) in a knowledge-intensive setting, we use the MSMARCO NLG task v2.1 [43]. The task consists of questions, ten gold passages retrieved from a search engine for each question, and a full sentence answer annotated from the retrieved passages. We do not use the supplied passages, only the questions and answers, to treat 4 .", "metadata": {"source": "data/21_patrick_2020_RAG.pdf", "page_number": 4, "chunk": 2, "title": ""}}, "451": {"idx": 451, "content": "MSMARCO as an open-domain abstractive QA task. MSMARCO has some questions that cannot be answered in a way that matches the reference answer without access to the gold passages, such as \u201cWhat is the weather in Volcano, CA?\u201d so performance will be lower without using gold passages. We also note that some MSMARCO questions cannot be answered using Wikipedia alone. Here, RAG can rely on parametric knowledge to generate reasonable responses. 3.3 Jeopardy Question Generation To evaluate RAG\u2019s generation abilities in a non-QA setting, we study open-domain question generation. Rather than use questions from standard open-domain QA tasks, which typically consist of short, simple questions, we propose the more demanding task of generating Jeopardy questions. Jeopardy is an unusual format that consists of trying to guess an entity from a fact about that entity. For example, \u201cThe World Cup\u201d is the answer to the question \u201cIn 1986 Mexico scored as the \ufb01rst country to host this international sports competition twice.\u201d As Jeopardy questions are precise, factual statements, generating Jeopardy questions conditioned on their answer entities constitutes a challenging knowledge-intensive generation task. We use the splits from SearchQA [10], with 100K train, 14K dev, and 27K test examples. As this is a new task, we train a BART model for comparison. Following [67], we evaluate using the SQuAD-tuned Q-BLEU-1 metric [42].", "metadata": {"source": "data/21_patrick_2020_RAG.pdf", "page_number": 5, "chunk": 0, "title": ""}}, "452": {"idx": 452, "content": "Q-BLEU is a variant of BLEU with a higher weight for matching entities and has higher correlation with human judgment for question generation than standard metrics. We also perform two human evaluations, one to assess generation factuality, and one for speci\ufb01city. We de\ufb01ne factuality as whether a statement can be corroborated by trusted external sources, and speci\ufb01city as high mutual dependence between the input and output [33]. We follow best practice and use pairwise comparative evaluation [34]. Evaluators are shown an answer and two generated questions, one from BART and one from RAG. They are then asked to pick one of four options\u2014quuestion A is better, question B is better, both are good, or neither is good. 3.4 Fact Veri\ufb01cation FEVER [56] requires classifying whether a natural language claim is supported or refuted by Wikipedia, or whether there is not enough information to decide. The task requires retrieving evidence from Wikipedia relating to the claim and then reasoning over this evidence to classify whether the claim is true, false, or unveri\ufb01able from Wikipedia alone. FEVER is a retrieval problem coupled with an challenging entailment reasoning task. It also provides an appropriate testbed for exploring the RAG models\u2019 ability to handle classi\ufb01cation rather than generation. We map FEVER class labels (supports, refutes, or not enough info) to single output tokens and directly train with claim-class pairs.", "metadata": {"source": "data/21_patrick_2020_RAG.pdf", "page_number": 5, "chunk": 1, "title": ""}}, "453": {"idx": 453, "content": "Crucially, unlike most other approaches to FEVER, we do not use supervision on retrieved evidence. In many real-world applications, retrieval supervision signals aren\u2019t available, and models that do not require such supervision will be applicable to a wider range of tasks. We explore two variants: the standard 3-way classi\ufb01cation task (supports/refutes/not enough info) and the 2-way (supports/refutes) task studied in Thorne and Vlachos [57]. In both cases we report label accuracy. 4 Results 4.1 Open-domain Question Answering Table 1 shows results for RAG along with state-of-the-art models. On all four open-domain QA tasks, RAG sets a new state of the art (only on the T5-comparable split for TQA). RAG combines the generation \ufb02exibility of the \u201cclosed-book\u201d (parametric only) approaches and the performance of \"open-book\" retrieval-based approaches. Unlike REALM and T5+SSM, RAG enjoys strong results without expensive, specialized \u201csalient span masking\u201d pre-training [20]. It is worth noting that RAG\u2019s retriever is initialized using DPR\u2019s retriever, which uses retrieval supervision on Natural Questions and TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based \u201ccrossencoder\u201d to re-rank documents, along with an extractive reader. RAG demonstrates that neither a re-ranker nor extractive reader is necessary for state-of-the-art performance. There are several advantages to generating answers even when it is possible to extract them.", "metadata": {"source": "data/21_patrick_2020_RAG.pdf", "page_number": 5, "chunk": 2, "title": ""}}, "454": {"idx": 454, "content": "Documents with clues about the answer but do not contain the answer verbatim can still contribute towards a correct answer being generated, which is not possible with standard extractive approaches, leading 5 .", "metadata": {"source": "data/21_patrick_2020_RAG.pdf", "page_number": 5, "chunk": 3, "title": ""}}, "455": {"idx": 455, "content": "Table 1: Open-Domain QA Test Scores. For TQA, left column uses the standard test set for OpenDomain QA, right column uses the TQA-Wiki test set. See Appendix D for further details. Model NQ TQA WQ CT Closed Book T5-11B [52] 34.5 - /50.1 37.4 - T5-11B+SSM[52] 36.6 - /60.5 44.7 - Open Book REALM [20] 40.4 - / - 40.7 46.8 DPR [26] 41.5 57.9/ - 41.1 50.6 RAG-Token 44.1 55.2/66.1 45.5 50.0 RAG-Seq. 44.5 56.8/68.0 45.2 52.2 Table 2: Generation and classi\ufb01cation Test Scores. MS-MARCO SotA is [4], FEVER-3 is [68] and FEVER-2 is [57] *Uses gold context/evidence. Best model without gold access underlined. Model Jeopardy MSMARCO FVR3 FVR2 B-1 QB-1 R-L B-1 Label Acc. SotA - - 49.8* 49.9* 76.8 92.2* BART 15.1 19.7 38.2 41.6 64.0 81.1 RAG-Tok. 17.3 22.2 40.1 41.5 72.5 89.5 RAG-Seq. 14.7 21.4 40.8 44.2 to more effective marginalization over documents. Furthermore, RAG can generate correct answers even when the correct answer is not in any retrieved document, achieving 11.8% accuracy in such cases for NQ, where an extractive model would score 0%. 4.2 Abstractive Question Answering As shown in Table 2, RAG-Sequence outperforms BART on Open MS-MARCO NLG by 2.6 Bleu points and 2.6 Rouge-L points.", "metadata": {"source": "data/21_patrick_2020_RAG.pdf", "page_number": 6, "chunk": 0, "title": ""}}, "456": {"idx": 456, "content": "RAG approaches state-of-the-art model performance, which is impressive given that (i) those models access gold passages with speci\ufb01c information required to generate the reference answer , (ii) many questions are unanswerable without the gold passages, and (iii) not all questions are answerable from Wikipedia alone. Table 3 shows some generated answers from our models. Qualitatively, we \ufb01nd that RAG models hallucinate less and generate factually correct text more often than BART. Later, we also show that RAG generations are more diverse than BART generations (see \u00a74.5). 4.3 Jeopardy Question Generation Table 2 shows that RAG-Token performs better than RAG-Sequence on Jeopardy question generation, with both models outperforming BART on Q-BLEU-1. 4 shows human evaluation results, over 452 pairs of generations from BART and RAG-Token. Evaluators indicated that BART was more factual than RAG in only 7.1% of cases, while RAG was more factual in 42.7% of cases, and both RAG and BART were factual in a further 17% of cases, clearly demonstrating the effectiveness of RAG on the task over a state-of-the-art generation model. Evaluators also \ufb01nd RAG generations to be more speci\ufb01c by a large margin. Table 3 shows typical generations from each model. Jeopardy questions often contain two separate pieces of information, and RAG-Token may perform best because it can generate responses that combine content from several documents. Figure 2 shows an example.", "metadata": {"source": "data/21_patrick_2020_RAG.pdf", "page_number": 6, "chunk": 1, "title": ""}}, "457": {"idx": 457, "content": "When generating \u201cSun\u201d, the posterior is high for document 2 which mentions \u201cThe Sun Also Rises\u201d. Similarly, document 1 dominates the posterior when \u201cA Farewell to Arms\u201d is generated. Intriguingly, after the \ufb01rst token of each book is generated, the document posterior \ufb02attens. This observation suggests that the generator can complete the titles without depending on speci\ufb01c documents. In other words, the model\u2019s parametric knowledge is suf\ufb01cient to complete the titles. We \ufb01nd evidence for this hypothesis by feeding the BART-only baseline with the partial decoding \"The Sun. BART completes the generation \"The Sun Also Rises\" is a novel by this author of \"The Sun Also Rises\" indicating the title \"The Sun Also Rises\" is stored in BART\u2019s parameters. Similarly, BART will complete the partial decoding \"The Sun Also Rises\" is a novel by this author of \"A with \"The Sun Also Rises\" is a novel by this author of \"A Farewell to Arms\". This example shows how parametric and non-parametric memories work together\u2014the non-parametric component helps to guide the generation, drawing out speci\ufb01c knowledge stored in the parametric memory. 4.4 Fact Veri\ufb01cation Table 2 shows our results on FEVER. For 3-way classi\ufb01cation, RAG scores are within 4.3% of state-of-the-art models, which are complex pipeline systems with domain-speci\ufb01c architectures and substantial engineering, trained using intermediate retrieval supervision, which RAG does not require. 6 .", "metadata": {"source": "data/21_patrick_2020_RAG.pdf", "page_number": 6, "chunk": 2, "title": ""}}, "458": {"idx": 458, "content": "Document 1: his works are considered classics of American literature ... His wartime experiences formed the basis for his novel \u201dA Farewell to Arms\u201d (1929) ... Document 2: ... artists of the 1920s \u201dLost Generation\u201d expatriate community. His debut novel, \u201dThe Sun Also Rises\u201d, was published in 1926. BOS \u201d The Sun Also R ises \u201d is a novel by this authorof \u201d A Fare well to Arms\u201d Doc 1 Doc 2 Doc 3 Doc 4 Doc 5 Figure 2: RAG-Token document posterior p(zi|x, yi, y\u2212i) for each generated token for input \u201cHemingway\" for Jeopardy generation with 5 retrieved documents. The posterior for document 1 is high when generating \u201cA Farewell to Arms\" and for document 2 when generating \u201cThe Sun Also Rises\". Table 3: Examples from generation tasks. RAG models generate more speci\ufb01c and factually accurate responses. \u2018?\u2019 indicates factually incorrect responses, * indicates partially correct responses. Task Input Model Generation MSMARCO de\ufb01ne middle ear BART ?The middle ear is the part of the ear between the middle ear and the nose. RAG-T The middle ear is the portion of the ear internal to the eardrum. RAG-S The middle ear includes the tympanic cavity and the three ossicles. what currency needed in scotland BART The currency needed in Scotland is Pound sterling. RAG-T Pound is the currency needed in Scotland. RAG-S The currency needed in Scotland is the pound sterling. Jeopardy Question Gener -ation Washington BART ?This state has the largest number of counties in the U.S. RAG-T It\u2019s the only U.S.", "metadata": {"source": "data/21_patrick_2020_RAG.pdf", "page_number": 7, "chunk": 0, "title": ""}}, "459": {"idx": 459, "content": "state named for a U.S. president RAG-S It\u2019s the state where you\u2019ll \ufb01nd Mount Rainier National Park The Divine Comedy BART *This epic poem by Dante is divided into 3 parts: the Inferno, the Purgatorio & the Purgatorio RAG-T Dante\u2019s \"Inferno\" is the \ufb01rst part of this epic poem RAG-S This 14th century work is divided into 3 sections: \"Inferno\", \"Purgatorio\" & \"Paradiso\" For 2-way classi\ufb01cation, we compare against Thorne and Vlachos [57], who train RoBERTa [35] to classify the claim as true or false given the gold evidence sentence. RAG achieves an accuracy within 2.7% of this model, despite being supplied with only the claim and retrieving its own evidence. We also analyze whether documents retrieved by RAG correspond to documents annotated as gold evidence in FEVER. We calculate the overlap in article titles between the top k documents retrieved by RAG and gold evidence annotations. We \ufb01nd that the top retrieved document is from a gold article in 71% of cases, and a gold article is present in the top 10 retrieved articles in 90% of cases. 4.5 Additional Results Generation Diversity Section 4.3 shows that RAG models are more factual and speci\ufb01c than BART for Jeopardy question generation. Following recent work on diversity-promoting decoding [33, 59, 39], we also investigate generation diversity by calculating the ratio of distinct ngrams to total ngrams generated by different models.", "metadata": {"source": "data/21_patrick_2020_RAG.pdf", "page_number": 7, "chunk": 1, "title": ""}}, "460": {"idx": 460, "content": "Table 5 shows that RAG-Sequence\u2019s generations are more diverse than RAG-Token\u2019s, and both are signi\ufb01cantly more diverse than BART without needing any diversity-promoting decoding. Retrieval Ablations A key feature of RAG is learning to retrieve relevant information for the task. To assess the effectiveness of the retrieval mechanism, we run ablations where we freeze the retriever during training. As shown in Table 6, learned retrieval improves results for all tasks. We compare RAG\u2019s dense retriever to a word overlap-based BM25 retriever [53]. Here, we replace RAG\u2019s retriever with a \ufb01xed BM25 system, and use BM25 retrieval scores as logits when calculating p(z|x). Table 6 shows the results. For FEVER, BM25 performs best, perhaps since FEVER claims are heavily entity-centric and thus well-suited for word overlap-based retrieval. Differentiable retrieval improves results on all other tasks, especially for Open-Domain QA, where it is crucial. Index hot-swapping An advantage of non-parametric memory models like RAG is that knowledge can be easily updated at test time. Parametric-only models like T5 or BART need further training to update their behavior as the world changes. To demonstrate, we build an index using the DrQA [5] Wikipedia dump from December 2016 and compare outputs from RAG using this index to the newer index from our main results (December 2018). We prepare a list of 82 world leaders who had changed 7 .", "metadata": {"source": "data/21_patrick_2020_RAG.pdf", "page_number": 7, "chunk": 2, "title": ""}}, "461": {"idx": 461, "content": "Table 4: Human assessments for the Jeopardy Question Generation Task. Factuality Speci\ufb01city BART better 7.1% 16.8% RAG better 42.7% 37.4% Both good 11.7% 11.8% Both poor 17.7% 6.9% No majority 20.8% 20.1% Table 5: Ratio of distinct to total tri-grams for generation tasks. MSMARCO Jeopardy QGen Gold 89.6% 90.0% BART 70.7% 32.4% RAG-Token 77.8% 46.8% RAG-Seq. 83.5% 53.8% Table 6: Ablations on the dev set. As FEVER is a classi\ufb01cation task, both RAG models are equivalent. Model NQ TQA WQ CT Jeopardy-QGen MSMarco FVR-3 FVR-2 Exact Match B-1 QB-1 R-L B-1 Label Accuracy RAG-Token-BM25 29.7 41.5 32.1 33.1 17.5 22.3 55.5 48.4 75.1 91.6 RAG-Sequence-BM25 31.8 44.1 36.6 33.8 11.1 19.5 56.5 46.9 RAG-Token-Frozen 37.8 50.1 37.1 51.1 16.7 21.7 55.9 49.4 72.9 89.4 RAG-Sequence-Frozen 41.2 52.1 41.8 52.6 11.8 19.6 56.7 47.3 RAG-Token 43.5 54.8 46.5 51.9 17.9 22.6 56.2 49.4 74.5 90.6 RAG-Sequence 44.0 55.8 44.9 53.4 15.3 21.5 57.2 47.5 between these dates and use a template \u201cWho is {position}?\u201d (e.g. \u201cWho is the President of Peru?\u201d) to query our NQ RAG model with each index. RAG answers 70% correctly using the 2016 index for 2016 world leaders and 68% using the 2018 index for 2018 world leaders. Accuracy with mismatched indices is low (12% with the 2018 index and 2016 leaders, 4% with the 2016 index and 2018 leaders). This shows we can update RAG\u2019s world knowledge by simply replacing its non-parametric memory.", "metadata": {"source": "data/21_patrick_2020_RAG.pdf", "page_number": 8, "chunk": 0, "title": ""}}, "462": {"idx": 462, "content": "Effect of Retrieving more documents Models are trained with either 5 or 10 retrieved latent documents, and we do not observe signi\ufb01cant differences in performance between them. We have the \ufb02exibility to adjust the number of retrieved documents at test time, which can affect performance and runtime. Figure 3 (left) shows that retrieving more documents at test time monotonically improves Open-domain QA results for RAG-Sequence, but performance peaks for RAG-Token at 10 retrieved documents. Figure 3 (right) shows that retrieving more documents leads to higher Rouge-L for RAG-Token at the expense of Bleu-1, but the effect is less pronounced for RAG-Sequence. 10 20 30 40 50 K Retrieved Docs 39 40 41 42 43 44 NQ Exact Match RAG-Tok RAG-Seq 10 20 30 40 50 K Retrieved Docs 40 50 60 70 80 NQ Answer Recall @ K RAG-Tok RAG-Seq Fixed DPR BM25 10 20 30 40 50 K Retrieved Docs 48 50 52 54 56 Bleu-1 / Rouge-L score RAG-Tok R-L RAG-Tok B-1 RAG-Seq R-L RAG-Seq B-1 Figure 3: Left: NQ performance as more documents are retrieved. Center: Retrieval recall performance in NQ. Right: MS-MARCO Bleu-1 and Rouge-L as more documents are retrieved. 5 Related Work Single-Task Retrieval Prior work has shown that retrieval improves performance across a variety of NLP tasks when considered in isolation.", "metadata": {"source": "data/21_patrick_2020_RAG.pdf", "page_number": 8, "chunk": 1, "title": ""}}, "463": {"idx": 463, "content": "Such tasks include open-domain question answering [5, 29], fact checking [56], fact completion [48], long-form question answering [12], Wikipedia article generation [36], dialogue [41, 65, 9, 13], translation [17], and language modeling [19, 27]. Our work uni\ufb01es previous successes in incorporating retrieval into individual tasks, showing that a single retrieval-based architecture is capable of achieving strong performance across several tasks. 8 .", "metadata": {"source": "data/21_patrick_2020_RAG.pdf", "page_number": 8, "chunk": 2, "title": ""}}, "464": {"idx": 464, "content": "General-Purpose Architectures for NLP Prior work on general-purpose architectures for NLP tasks has shown great success without the use of retrieval. A single, pre-trained language model has been shown to achieve strong performance on various classi\ufb01cation tasks in the GLUE benchmarks [60, 61] after \ufb01ne-tuning [49, 8]. GPT-2 [50] later showed that a single, left-to-right, pre-trained language model could achieve strong performance across both discriminative and generative tasks. For further improvement, BART [32] and T5 [51, 52] propose a single, pre-trained encoder-decoder model that leverages bi-directional attention to achieve stronger performance on discriminative and generative tasks. Our work aims to expand the space of possible tasks with a single, uni\ufb01ed architecture, by learning a retrieval module to augment pre-trained, generative language models. Learned Retrieval There is signi\ufb01cant work on learning to retrieve documents in information retrieval, more recently with pre-trained, neural language models [44, 26] similar to ours. Some work optimizes the retrieval module to aid in a speci\ufb01c, downstream task such as question answering, using search [46], reinforcement learning [6, 63, 62], or a latent variable approach [31, 20] as in our work.", "metadata": {"source": "data/21_patrick_2020_RAG.pdf", "page_number": 9, "chunk": 0, "title": ""}}, "465": {"idx": 465, "content": "These successes leverage different retrieval-based architectures and optimization techniques to achieve strong performance on a single task, while we show that a single retrieval-based architecture can be \ufb01ne-tuned for strong performance on a variety of tasks. Memory-based Architectures Our document index can be seen as a large external memory for neural networks to attend to, analogous to memory networks [64, 55]. Concurrent work [14] learns to retrieve a trained embedding for each entity in the input, rather than to retrieve raw text as in our work. Other work improves the ability of dialog models to generate factual text by attending over fact embeddings [15, 13]. A key feature of our memory is that it is comprised of raw text rather distributed representations, which makes the memory both (i) human-readable, lending a form of interpretability to our model, and (ii) human-writable, enabling us to dynamically update the model\u2019s memory by editing the document index. This approach has also been used in knowledge-intensive dialog, where generators have been conditioned on retrieved text directly, albeit obtained via TF-IDF rather than end-to-end learnt retrieval [9]. Retrieve-and-Edit approaches Our method shares some similarities with retrieve-and-edit style approaches, where a similar training input-output pair is retrieved for a given input, and then edited to provide a \ufb01nal output.", "metadata": {"source": "data/21_patrick_2020_RAG.pdf", "page_number": 9, "chunk": 1, "title": ""}}, "466": {"idx": 466, "content": "These approaches have proved successful in a number of domains including Machine Translation [18, 22] and Semantic Parsing [21]. Our approach does have several differences, including less of emphasis on lightly editing a retrieved item, but on aggregating content from several pieces of retrieved content, as well as learning latent retrieval, and retrieving evidence documents rather than related training pairs. This said, RAG techniques may work well in these settings, and could represent promising future work. 6 Discussion In this work, we presented hybrid generation models with access to parametric and non-parametric memory. We showed that our RAG models obtain state of the art results on open-domain QA. We found that people prefer RAG\u2019s generation over purely parametric BART, \ufb01nding RAG more factual and speci\ufb01c. We conducted an thorough investigation of the learned retrieval component, validating its effectiveness, and we illustrated how the retrieval index can be hot-swapped to update the model without requiring any retraining. In future work, it may be fruitful to investigate if the two components can be jointly pre-trained from scratch, either with a denoising objective similar to BART or some another objective. Our work opens up new research directions on how parametric and non-parametric memories interact and how to most effectively combine them, showing promise in being applied to a wide variety of NLP tasks. 9 .", "metadata": {"source": "data/21_patrick_2020_RAG.pdf", "page_number": 9, "chunk": 2, "title": ""}}, "467": {"idx": 467, "content": "Broader Impact This work offers several positive societal bene\ufb01ts over previous work: the fact that it is more strongly grounded in real factual knowledge (in this case Wikipedia) makes it \u201challucinate\u201d less with generations that are more factual, and offers more control and interpretability. RAG could be employed in a wide variety of scenarios with direct bene\ufb01t to society, for example by endowing it with a medical index and asking it open-domain questions on that topic, or by helping people be more effective at their jobs. With these advantages also come potential downsides: Wikipedia, or any potential external knowledge source, will probably never be entirely factual and completely devoid of bias. Since RAG can be employed as a language model, similar concerns as for GPT-2 [50] are valid here, although arguably to a lesser extent, including that it might be used to generate abuse, faked or misleading content in the news or on social media; to impersonate others; or to automate the production of spam/phishing content [54]. Advanced language models may also lead to the automation of various jobs in the coming decades [16]. In order to mitigate these risks, AI systems could be employed to \ufb01ght against misleading content and automated spam/phishing. Acknowledgments The authors would like to thank the reviewers for their thoughtful and constructive feedback on this paper, as well as HuggingFace for their help in open-sourcing code to run RAG models.", "metadata": {"source": "data/21_patrick_2020_RAG.pdf", "page_number": 10, "chunk": 0, "title": ""}}, "468": {"idx": 468, "content": "The authors would also like to thank Kyunghyun Cho and Sewon Min for productive discussions and advice. EP thanks supports from the NSF Graduate Research Fellowship. PL is supported by the FAIR PhD program. References [1] Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina Stoica, Saurabh Tiwary, and Tong Wang. MS MARCO: A Human Generated MAchine Reading COmprehension Dataset. arXiv:1611.09268 [cs], November 2016. URL http: //arxiv.org/abs/1611.09268. arXiv: 1611.09268. [2] Petr Baudi\u0161 and Jan \u0160ediv`y. Modeling of the question answering task in the yodaqa system. In International Conference of the Cross-Language Evaluation Forum for European Languages, pages 222\u2013228. Springer, 2015. URL https://link.springer.com/chapter/10.1007% 2F978-3-319-24027-5_20. [3] Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic Parsing on Freebase from Question-Answer Pairs. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1533\u20131544, Seattle, Washington, USA, October 2013. Association for Computational Linguistics. URL http://www.aclweb.org/anthology/ D13-1160. [4] Bin Bi, Chenliang Li, Chen Wu, Ming Yan, and Wei Wang. Palm: Pre-training an autoencoding&autoregressive language model for context-conditioned generation. ArXiv, abs/2004.07159, 2020. URL https://arxiv.org/abs/2004.07159.", "metadata": {"source": "data/21_patrick_2020_RAG.pdf", "page_number": 10, "chunk": 1, "title": ""}}, "469": {"idx": 469, "content": "[5] Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading Wikipedia to Answer Open-Domain Questions. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1870\u20131879, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1171. URL https://www.aclweb.org/anthology/P17-1171. [6] Eunsol Choi, Daniel Hewlett, Jakob Uszkoreit, Illia Polosukhin, Alexandre Lacoste, and Jonathan Berant. Coarse-to-\ufb01ne question answering for long documents. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 209\u2013220, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1020. URL https://www.aclweb.org/anthology/P17-1020. 10 .", "metadata": {"source": "data/21_patrick_2020_RAG.pdf", "page_number": 10, "chunk": 2, "title": ""}}, "470": {"idx": 470, "content": "[7] Christopher Clark and Matt Gardner. Simple and Effective Multi-Paragraph Reading Comprehension. arXiv:1710.10723 [cs], October 2017. URL http://arxiv.org/abs/1710.10723. arXiv: 1710.10723. [8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://www.aclweb.org/anthology/N19-1423. [9] Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. Wizard of wikipedia: Knowledge-powered conversational agents. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=r1l73iRqKm. [10] Matthew Dunn, Levent Sagun, Mike Higgins, V. Ugur Guney, Volkan Cirik, and Kyunghyun Cho. SearchQA: A New Q&A Dataset Augmented with Context from a Search Engine. arXiv:1704.05179 [cs], April 2017. URL http://arxiv.org/abs/1704.05179. arXiv: 1704.05179. [11] Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 889\u2013898, Melbourne, Australia, July 2018.", "metadata": {"source": "data/21_patrick_2020_RAG.pdf", "page_number": 11, "chunk": 0, "title": ""}}, "471": {"idx": 471, "content": "Association for Computational Linguistics. doi: 10.18653/v1/P18-1082. URL https://www.aclweb.org/anthology/ P18-1082. [12] Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5: Long form question answering. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3558\u20133567, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1346. URL https://www.aclweb.org/ anthology/P19-1346. [13] Angela Fan, Claire Gardent, Chloe Braud, and Antoine Bordes. Augmenting transformers with KNN-based composite memory, 2020. URL https://openreview.net/forum?id= H1gx1CNKPH. [14] Thibault F\u00e9vry, Livio Baldini Soares, Nicholas FitzGerald, Eunsol Choi, and Tom Kwiatkowski. Entities as experts: Sparse memory access with entity supervision. ArXiv, abs/2004.07202, 2020. URL https://arxiv.org/abs/2004.07202. [15] Marjan Ghazvininejad, Chris Brockett, Ming-Wei Chang, Bill Dolan, Jianfeng Gao, Wen tau Yih, and Michel Galley. A knowledge-grounded neural conversation model. In AAAI Conference on Arti\ufb01cial Intelligence, 2018. URL https://www.aaai.org/ocs/index.php/ AAAI/AAAI18/paper/view/16710. [16] Katja Grace, John Salvatier, Allan Dafoe, Baobao Zhang, and Owain Evans. When will AI exceed human performance? evidence from AI experts. CoRR, abs/1705.08807, 2017. URL http://arxiv.org/abs/1705.08807. [17] Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O.K. Li.", "metadata": {"source": "data/21_patrick_2020_RAG.pdf", "page_number": 11, "chunk": 1, "title": ""}}, "472": {"idx": 472, "content": "Search engine guided neural machine translation. In AAAI Conference on Arti\ufb01cial Intelligence, 2018. URL https: //www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17282. [18] Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O.K. Li. Search engine guided neural machine translation. In 32nd AAAI Conference on Arti\ufb01cial Intelligence, AAAI 2018, 32nd AAAI Conference on Arti\ufb01cial Intelligence, AAAI 2018, pages 5133\u20135140. AAAI press, 2018. 32nd AAAI Conference on Arti\ufb01cial Intelligence, AAAI 2018 ; Conference date: 02-02-2018 Through 07-02-2018. [19] Kelvin Guu, Tatsunori B. Hashimoto, Yonatan Oren, and Percy Liang. Generating sentences by editing prototypes. Transactions of the Association for Computational Linguistics, 6:437\u2013450, 2018. doi: 10.1162/tacl_a_00030. URL https://www.aclweb.org/anthology/Q18-1031. 11 .", "metadata": {"source": "data/21_patrick_2020_RAG.pdf", "page_number": 11, "chunk": 2, "title": ""}}, "473": {"idx": 473, "content": "[20] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. REALM: Retrieval-augmented language model pre-training. ArXiv, abs/2002.08909, 2020. URL https: //arxiv.org/abs/2002.08909. [21] Tatsunori B Hashimoto, Kelvin Guu, Yonatan Oren, and Percy S Liang. A retrieve-and-edit framework for predicting structured outputs. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems 31, pages 10052\u2013 10062. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/ 8209-a-retrieve-and-edit-framework-for-predicting-structured-outputs. pdf. [22] Nabil Hossain, Marjan Ghazvininejad, and Luke Zettlemoyer. Simple and effective retrieveedit-rerank text generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2532\u20132538, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.228. URL https://www.aclweb.org/ anthology/2020.acl-main.228. [23] Jeff Johnson, Matthijs Douze, and Herv\u00e9 J\u00e9gou. Billion-scale similarity search with gpus. arXiv preprint arXiv:1702.08734, 2017. URL https://arxiv.org/abs/1702.08734. [24] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension.", "metadata": {"source": "data/21_patrick_2020_RAG.pdf", "page_number": 12, "chunk": 0, "title": ""}}, "474": {"idx": 474, "content": "In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1601\u20131611, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1147. URL https://www.aclweb.org/anthology/P17-1147. [25] Armand Joulin and Tomas Mikolov. Inferring algorithmic patterns with stackaugmented recurrent nets. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1, NIPS\u201915, page 190\u2013198, Cambridge, MA, USA, 2015. MIT Press. URL https://papers.nips.cc/paper/ 5857-inferring-algorithmic-patterns-with-stack-augmented-recurrent-nets. [26] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. arXiv preprint arXiv:2004.04906, 2020. URL https://arxiv.org/abs/2004.04906. [27] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generalization through memorization: Nearest neighbor language models. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=HklBjCEKvH. [28] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1412.6980.", "metadata": {"source": "data/21_patrick_2020_RAG.pdf", "page_number": 12, "chunk": 1, "title": ""}}, "475": {"idx": 475, "content": "[29] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red\ufb01eld, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural Questions: a Benchmark for Question Answering Research. Transactions of the Association of Computational Linguistics, 2019. URL https://tomkwiat.users.x20web.corp.google.com/papers/ natural-questions/main-1455-kwiatkowski.pdf. [30] Guillaume Lample, Alexandre Sablayrolles, Marc\u2019 Aurelio Ranzato, Ludovic Denoyer, and Herve Jegou. Large memory layers with product keys. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d\u2019 Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 8548\u20138559. Curran Associates, Inc., 2019. URL http: //papers.nips.cc/paper/9061-large-memory-layers-with-product-keys.pdf. [31] Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. Latent retrieval for weakly supervised open domain question answering. In Proceedings of the 57th Annual Meeting of the Association 12 .", "metadata": {"source": "data/21_patrick_2020_RAG.pdf", "page_number": 12, "chunk": 2, "title": ""}}, "476": {"idx": 476, "content": "for Computational Linguistics, pages 6086\u20136096, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1612. URL https://www.aclweb.org/ anthology/P19-1612. [32] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019. URL https://arxiv.org/abs/1910.13461. [33] Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. A diversity-promoting objective function for neural conversation models. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 110\u2013119, San Diego, California, June 2016. Association for Computational Linguistics. doi: 10.18653/v1/N16-1014. URL https://www.aclweb.org/anthology/ N16-1014. [34] Margaret Li, Jason Weston, and Stephen Roller. Acute-eval: Improved dialogue evaluation with optimized questions and multi-turn comparisons. ArXiv, abs/1909.03087, 2019. URL https://arxiv.org/abs/1909.03087. [35] Hairong Liu, Mingbo Ma, Liang Huang, Hao Xiong, and Zhongjun He. Robust neural machine translation with joint textual and phonetic embedding. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3044\u20133049, Florence, Italy, July 2019.", "metadata": {"source": "data/21_patrick_2020_RAG.pdf", "page_number": 13, "chunk": 0, "title": ""}}, "477": {"idx": 477, "content": "Association for Computational Linguistics. doi: 10.18653/v1/P19-1291. URL https://www.aclweb.org/anthology/P19-1291. [36] Peter J. Liu*, Mohammad Saleh*, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam Shazeer. Generating wikipedia by summarizing long sequences. In International Conference on Learning Representations, 2018. URL https://openreview.net/forum? id=Hyg0vbWC-. [37] Yury A. Malkov and D. A. Yashunin. Ef\ufb01cient and robust approximate nearest neighbor search using hierarchical navigable small world graphs. IEEE Transactions on Pattern Analysis and Machine Intelligence, 42:824\u2013836, 2016. URL https://arxiv.org/abs/1603.09320. [38] Gary Marcus. The next decade in ai: four steps towards robust arti\ufb01cial intelligence. arXiv preprint arXiv:2002.06177, 2020. URL https://arxiv.org/abs/2002.06177. [39] Luca Massarelli, Fabio Petroni, Aleksandra Piktus, Myle Ott, Tim Rockt\u00e4schel, Vassilis Plachouras, Fabrizio Silvestri, and Sebastian Riedel. How decoding strategies affect the veri\ufb01ability of generated text. arXiv preprint arXiv:1911.03587, 2019. URL https: //arxiv.org/abs/1911.03587. [40] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed precision training. In ICLR, 2018. URL https://openreview.net/forum?id=r1gs9JgRZ. [41] Nikita Moghe, Siddhartha Arora, Suman Banerjee, and Mitesh M. Khapra.", "metadata": {"source": "data/21_patrick_2020_RAG.pdf", "page_number": 13, "chunk": 1, "title": ""}}, "478": {"idx": 478, "content": "Towards exploiting background knowledge for building conversation systems. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2322\u20132332, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1255. URL https://www.aclweb.org/anthology/D18-1255. [42] Preksha Nema and Mitesh M. Khapra. Towards a better metric for evaluating question generation systems. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3950\u20133959, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1429. URL https://www.aclweb.org/ anthology/D18-1429. [43] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. MS MARCO: A human generated machine reading comprehension dataset. In Tarek Richard Besold, Antoine Bordes, Artur S. d\u2019Avila Garcez, and Greg Wayne, editors, Proceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic 13 .", "metadata": {"source": "data/21_patrick_2020_RAG.pdf", "page_number": 13, "chunk": 2, "title": ""}}, "479": {"idx": 479, "content": "approaches 2016 co-located with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain, December 9, 2016, volume 1773 of CEUR Workshop Proceedings. CEUR-WS.org, 2016. URL http://ceur-ws.org/Vol-1773/CoCoNIPS_ 2016_paper9.pdf. [44] Rodrigo Nogueira and Kyunghyun Cho. Passage re-ranking with BERT. arXiv preprint arXiv:1901.04085, 2019. URL https://arxiv.org/abs/1901.04085. [45] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), pages 48\u201353, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-4009. URL https://www.aclweb. org/anthology/N19-4009. [46] Ethan Perez, Siddharth Karamcheti, Rob Fergus, Jason Weston, Douwe Kiela, and Kyunghyun Cho. Finding generalizable evidence by learning to convince q&a models. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2402\u20132411, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1244. URL https://www.aclweb.org/anthology/D19-1244.", "metadata": {"source": "data/21_patrick_2020_RAG.pdf", "page_number": 14, "chunk": 0, "title": ""}}, "480": {"idx": 480, "content": "[47] Fabio Petroni, Tim Rockt\u00e4schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. Language models as knowledge bases? In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2463\u20132473, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/ D19-1250. URL https://www.aclweb.org/anthology/D19-1250. [48] Fabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim Rockt\u00e4schel, Yuxiang Wu, Alexander H. Miller, and Sebastian Riedel. How context affects language models\u2019 factual predictions. In Automated Knowledge Base Construction, 2020. URL https://openreview.net/forum? id=025X0zPfn. [49] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving Language Understanding by Generative Pre-Training, 2018. URL https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/ language-unsupervised/language_understanding_paper.pdf. [50] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners, 2019. URL https://d4mucfpksywv.cloudfront.net/better-language-models/language_ models_are_unsupervised_multitask_learners.pdf. [51] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu.", "metadata": {"source": "data/21_patrick_2020_RAG.pdf", "page_number": 14, "chunk": 1, "title": ""}}, "481": {"idx": 481, "content": "Exploring the limits of transfer learning with a uni\ufb01ed text-to-text transformer. arXiv e-prints, 2019. URL https://arxiv.org/abs/1910.10683. [52] Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the parameters of a language model? arXiv e-prints, 2020. URL https://arxiv.org/abs/ 2002.08910. [53] Stephen Robertson and Hugo Zaragoza. The probabilistic relevance framework: Bm25 and beyond. Found. Trends Inf. Retr., 3(4):333\u2013389, April 2009. ISSN 1554-0669. doi: 10.1561/ 1500000019. URL https://doi.org/10.1561/1500000019. [54] Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu, Alec Radford, and Jian-Bing Wang. Release strategies and the social impacts of language models. ArXiv, abs/1908.09203, 2019. [55] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 2440\u20132448. Curran Associates, Inc., 2015. URL http://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf. 14 .", "metadata": {"source": "data/21_patrick_2020_RAG.pdf", "page_number": 14, "chunk": 2, "title": ""}}, "482": {"idx": 482, "content": "[56] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER: a large-scale dataset for fact extraction and VERi\ufb01cation. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 809\u2013819, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1074. URL https://www.aclweb.org/anthology/N18-1074. [57] James H. Thorne and Andreas Vlachos. Avoiding catastrophic forgetting in mitigating model biases in sentence-pair classi\ufb01cation with elastic weight consolidation. ArXiv, abs/2004.14366, 2020. URL https://arxiv.org/abs/2004.14366. [58] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 5998\u20136008. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf. [59] Ashwin Vijayakumar, Michael Cogswell, Ramprasaath Selvaraju, Qing Sun, Stefan Lee, David Crandall, and Dhruv Batra. Diverse beam search for improved description of complex scenes. AAAI Conference on Arti\ufb01cial Intelligence, 2018. URL https://www.aaai.org/ocs/index. php/AAAI/AAAI18/paper/view/17329.", "metadata": {"source": "data/21_patrick_2020_RAG.pdf", "page_number": 15, "chunk": 0, "title": ""}}, "483": {"idx": 483, "content": "[60] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353\u2013355, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5446. URL https://www.aclweb.org/ anthology/W18-5446. [61] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. SuperGLUE: A Stickier Benchmark for GeneralPurpose Language Understanding Systems. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d\\textquotesingle Alch\u00e9-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 3261\u20133275. Curran Associates, Inc., 2019. URL https:// arxiv.org/abs/1905.00537. [62] Shuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang, Tim Klinger, Wei Zhang, Shiyu Chang, Gerry Tesauro, Bowen Zhou, and Jing Jiang. R3: Reinforced ranker-reader for open-domain question answering. In Sheila A. McIlraith and Kilian Q. Weinberger, editors, Proceedings of the Thirty-Second AAAI Conference on Arti\ufb01cial Intelligence, (AAAI-18), the 30th innovative Applications of Arti\ufb01cial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Arti\ufb01cial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018, pages 5981\u20135988. AAAI Press, 2018.", "metadata": {"source": "data/21_patrick_2020_RAG.pdf", "page_number": 15, "chunk": 1, "title": ""}}, "484": {"idx": 484, "content": "URL https://www.aaai.org/ocs/index. php/AAAI/AAAI18/paper/view/16712. [63] Shuohang Wang, Mo Yu, Jing Jiang, Wei Zhang, Xiaoxiao Guo, Shiyu Chang, Zhiguo Wang, Tim Klinger, Gerald Tesauro, and Murray Campbell. Evidence aggregation for answer reranking in open-domain question answering. In ICLR, 2018. URL https://openreview. net/forum?id=rJl3yM-Ab. [64] Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1410.3916. [65] Jason Weston, Emily Dinan, and Alexander Miller. Retrieve and re\ufb01ne: Improved sequence generation models for dialogue. In Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd International Workshop on Search-Oriented Conversational AI, pages 87\u201392, Brussels, Belgium, October 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5713. URL https://www.aclweb.org/anthology/W18-5713. 15 .", "metadata": {"source": "data/21_patrick_2020_RAG.pdf", "page_number": 15, "chunk": 2, "title": ""}}, "485": {"idx": 485, "content": "[66] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Huggingface\u2019s transformers: State-of-the-art natural language processing. ArXiv, abs/1910.03771, 2019. [67] Shiyue Zhang and Mohit Bansal. Addressing semantic drift in question generation for semisupervised question answering. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2495\u20132509, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1253. URL https://www.aclweb.org/anthology/D19-1253. [68] Wanjun Zhong, Jingjing Xu, Duyu Tang, Zenan Xu, Nan Duan, Ming Zhou, Jiahai Wang, and Jian Yin. Reasoning over semantic-level graph for fact checking. ArXiv, abs/1909.03745, 2019. URL https://arxiv.org/abs/1909.03745. 16 .", "metadata": {"source": "data/21_patrick_2020_RAG.pdf", "page_number": 16, "chunk": 0, "title": ""}}, "486": {"idx": 486, "content": "Appendices for Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks A Implementation Details For Open-domain QA we report test numbers using 15 retrieved documents for RAG-Token models. For RAG-Sequence models, we report test results using 50 retrieved documents, and we use the Thorough Decoding approach since answers are generally short. We use greedy decoding for QA as we did not \ufb01nd beam search improved results. For Open-MSMarco and Jeopardy question generation, we report test numbers using ten retrieved documents for both RAG-Token and RAG-Sequence, and we also train a BART-large model as a baseline. We use a beam size of four, and use the Fast Decoding approach for RAG-Sequence models, as Thorough Decoding did not improve performance. B Human Evaluation Figure 4: Annotation interface for human evaluation of factuality. A pop-out for detailed instructions and a worked example appear when clicking \"view tool guide\". Figure 4 shows the user interface for human evaluation. To avoid any biases for screen position, which model corresponded to sentence A and sentence B was randomly selected for each example. Annotators were encouraged to research the topic using the internet, and were given detailed instructions and worked examples in a full instructions tab. We included some gold sentences in order to assess the accuracy of the annotators. Two annotators did not perform well on these examples and their annotations were removed from the results.", "metadata": {"source": "data/21_patrick_2020_RAG.pdf", "page_number": 17, "chunk": 0, "title": ""}}, "487": {"idx": 487, "content": "C Training setup Details We train all RAG models and BART baselines using Fairseq [45].2 We train with mixed precision \ufb02oating point arithmetic [40], distributing training across 8, 32GB NVIDIA V100 GPUs, though training and inference can be run on one GPU. We \ufb01nd that doing Maximum Inner Product Search with FAISS is suf\ufb01ciently fast on CPU, so we store document index vectors on CPU, requiring \u223c100 GB of CPU memory for all of Wikipedia. After submission, We have ported our code to HuggingFace Transformers [66]3, which achieves equivalent performance to the previous version but is a cleaner and easier to use implementation. This version is also open-sourced. We also compress the document index using FAISS\u2019s compression tools, reducing the CPU memory requirement to 36GB. Scripts to run experiments with RAG can be found at https://github.com/huggingface/transformers/ blob/master/examples/rag/README.md and an interactive demo of a RAG model can be found at https://huggingface.co/rag/ 2https://github.com/pytorch/fairseq 3https://github.com/huggingface/transformers 17 .", "metadata": {"source": "data/21_patrick_2020_RAG.pdf", "page_number": 17, "chunk": 1, "title": ""}}, "488": {"idx": 488, "content": "D Further Details on Open-Domain QA For open-domain QA, multiple answer annotations are often available for a given question. These answer annotations are exploited by extractive models during training as typically all the answer annotations are used to \ufb01nd matches within documents when preparing training data. For RAG, we also make use of multiple annotation examples for Natural Questions and WebQuestions by training the model with each (q, a) pair separately, leading to a small increase in accuracy. For TriviaQA, there are often many valid answers to a given question, some of which are not suitable training targets, such as emoji or spelling variants. For TriviaQA, we \ufb01lter out answer candidates if they do not occur in top 1000 documents for the query. CuratedTrec preprocessing The answers for CuratedTrec are given in the form of regular expressions, which has been suggested as a reason why it is unsuitable for answer-generation models [20]. To overcome this, we use a pre-processing step where we \ufb01rst retrieve the top 1000 documents for each query, and use the answer that most frequently matches the regex pattern as the supervision target. If no matches are found, we resort to a simple heuristic: generate all possible permutations for each regex, replacing non-deterministic symbols in the regex nested tree structure with a whitespace.", "metadata": {"source": "data/21_patrick_2020_RAG.pdf", "page_number": 18, "chunk": 0, "title": ""}}, "489": {"idx": 489, "content": "TriviaQA Evaluation setups The open-domain QA community customarily uses public development datasets as test datasets, as test data for QA datasets is often restricted and dedicated to reading compehension purposes. We report our results using the datasets splits used in DPR [26], which are consistent with common practice in Open-domain QA. For TriviaQA, this test dataset is the public TriviaQA Web Development split. Roberts et al. [52] used the TriviaQA of\ufb01cial Wikipedia test set instead. F\u00e9vry et al. [14] follow this convention in order to compare with Roberts et al. [52] (See appendix of [14]). We report results on both test sets to enable fair comparison to both approaches. We \ufb01nd that our performance is much higher using the of\ufb01cial Wiki test set, rather than the more conventional open-domain test set, which we attribute to the of\ufb01cial Wiki test set questions being simpler to answer from Wikipedia. E Further Details on FEVER For FEVER classi\ufb01cation, we follow the practice from [32], and \ufb01rst re-generate the claim, and then classify using the representation of the \ufb01nal hidden state, before \ufb01nally marginalizing across documents to obtain the class probabilities. The FEVER task traditionally has two sub-tasks. The \ufb01rst is to classify the claim as either \"Supported\", \"Refuted\" or \"Not Enough Info\", which is the task we explore in the main paper. FEVER\u2019s other sub-task involves extracting sentences from Wikipedia as evidence supporting the classi\ufb01cation prediction.", "metadata": {"source": "data/21_patrick_2020_RAG.pdf", "page_number": 18, "chunk": 1, "title": ""}}, "490": {"idx": 490, "content": "As FEVER uses a different Wikipedia dump to us, directly tackling this task is not straightforward. We hope to address this in future work. F Null Document Probabilities We experimented with adding \"Null document\" mechanism to RAG, similar to REALM [20] in order to model cases where no useful information could be retrieved for a given input. Here, if k documents were retrieved, we would additionally \"retrieve\" an empty document and predict a logit for the null document, before marginalizing over k + 1 predictions. We explored modelling this null document logit by learning (i) a document embedding for the null document, (ii) a static learnt bias term, or (iii) a neural network to predict the logit. We did not \ufb01nd that these improved performance, so in the interests of simplicity, we omit them. For Open MS-MARCO, where useful retrieved documents cannot always be retrieved, we observe that the model learns to always retrieve a particular set of documents for questions that are less likely to bene\ufb01t from retrieval, suggesting that null document mechanisms may not be necessary for RAG. G Parameters Our RAG models contain the trainable parameters for the BERT-base query and document encoder of DPR, with 110M parameters each (although we do not train the document encoder ourselves) and 406M trainable parameters from BART-large, 406M parameters, making a total of 626M trainable 18 .", "metadata": {"source": "data/21_patrick_2020_RAG.pdf", "page_number": 18, "chunk": 2, "title": ""}}, "491": {"idx": 491, "content": "Table 7: Number of instances in the datasets used. *A hidden subset of this data is used for evaluation Task Train Development Test Natural Questions 79169 8758 3611 TriviaQA 78786 8838 11314 WebQuestions 3418 362 2033 CuratedTrec 635 134 635 Jeopardy Question Generation 97392 13714 26849 MS-MARCO 153726 12468 101093* FEVER-3-way 145450 10000 10000 FEVER-2-way 96966 6666 6666 parameters. The best performing \"closed-book\" (parametric only) open-domain QA model is T5-11B with 11 Billion trainable parameters. The T5 model with the closest number of parameters to our models is T5-large (770M parameters), which achieves a score of 28.9 EM on Natural Questions [52], substantially below the 44.5 that RAG-Sequence achieves, indicating that hybrid parametric/nonparametric models require far fewer trainable parameters for strong open-domain QA performance. The non-parametric memory index does not consist of trainable parameters, but does consists of 21M 728 dimensional vectors, consisting of 15.3B values. These can be easily be stored at 8-bit \ufb02oating point precision to manage memory and disk footprints. H Retrieval Collapse In preliminary experiments, we observed that for some tasks such as story generation [11], the retrieval component would \u201ccollapse\u201d and learn to retrieve the same documents regardless of the input. In these cases, once retrieval had collapsed, the generator would learn to ignore the documents, and the RAG model would perform equivalently to BART.", "metadata": {"source": "data/21_patrick_2020_RAG.pdf", "page_number": 19, "chunk": 0, "title": ""}}, "492": {"idx": 492, "content": "The collapse could be due to a less-explicit requirement for factual knowledge in some tasks, or the longer target sequences, which could result in less informative gradients for the retriever. Perez et al. [46] also found spurious retrieval results when optimizing a retrieval component in order to improve performance on downstream tasks. I Number of instances per dataset The number of training, development and test datapoints in each of our datasets is shown in Table 7. 19 .", "metadata": {"source": "data/21_patrick_2020_RAG.pdf", "page_number": 19, "chunk": 1, "title": ""}}, "493": {"idx": 493, "content": "Corpus Conversion Service: A Machine Learning Platform to Ingest Documents at Scale. Peter W J Staar, Michele Dolfi, Christoph Auer, Costas Bekas taa,dol,cau,bek@zurich.ibm.com IBM Research Rueschlikon, Switzerland ABSTRACT Over the past few decades, the amount of scientific articles and technical literature has increased exponentially in size. Consequently, there is a great need for systems that can ingest these documents at scale and make the contained knowledge discoverable. Unfortunately, both the format of these documents (e.g. the PDF format or bitmap images) as well as the presentation of the data (e.g. complex tables) make the extraction of qualitative and quantitive data extremely challenging. In this paper, we present a modular, cloud-based platform to ingest documents at scale. This platform, called the Corpus Conversion Service (CCS), implements a pipeline which allows users to parse and annotate documents (i.e. collect ground-truth), train machine-learning classification algorithms and ultimately convert any type of PDF or bitmap-documents to a structured content representation format. We will show that each of the modules is scalable due to an asynchronous microservice architecture and can therefore handle massive amounts of documents. Furthermore, we will show that our capability to gather groundtruth is accelerated by machine-learning algorithms by at least one order of magnitude.", "metadata": {"source": "data/28_PWJ_2018_amachinelearning.pdf", "page_number": 1, "chunk": 0, "title": "Corpus Conversion Service: A Machine Learning Platform to Ingest Documents at Scale."}}, "494": {"idx": 494, "content": "This allows us to both gather large amounts of ground-truth in very little time and obtain very good precision/recall metrics in the range of 99% with regard to content conversion to structured output. The CCS platform is currently deployed on IBM internal infrastructure and serving more than 250 active users for knowledge-engineering project engagements. ACM Reference Format: Peter W J Staar, Michele Dolfi, Christoph Auer, Costas Bekas. 2018. Corpus Conversion Service: A Machine Learning Platform to Ingest Documents at Scale.. In KDD \u201918: The 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, August 19\u201323, 2018, London, United Kingdom. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/3219819.3219834 1 INTRODUCTION It is estimated that there are roughly 2.5 trillion PDF documents currently in circulation1. These documents range from manuals for 1This number originates from a keynote talk by Phil Ydens, Adobe\u2019s VP Engineering for Document Cloud. A video of the presentation can be found here: https://youtu.be/ 5Axw6OGPYHw Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted.", "metadata": {"source": "data/28_PWJ_2018_amachinelearning.pdf", "page_number": 1, "chunk": 1, "title": "Corpus Conversion Service: A Machine Learning Platform to Ingest Documents at Scale."}}, "495": {"idx": 495, "content": "To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. KDD \u201918, August 19\u201323, 2018, London, United Kingdom \u00a9 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-5552-0/18/08...$15.00 https://doi.org/10.1145/3219819.3219834 appliances, annual reports of companies, all the way to research papers, detailing a specific scientific discovery. It is needless to say that valuable qualitative and quantitative information is contained in many of them. However, content encoded in PDF is by its nature reduced to streams of printing instructions purposed to faithfully present a pleasing visual layout. Both the data representation and the enormous variability of layouts across these documents make it extremely challenging to access content and transform it into a representation that enables knowledge discovery. In addition to the sheer current quantity of documents, the submission rate of published documents in the scientific domain is also growing exponentially2. This poses a real problem, since more and more information published in the PDF documents is going dark. In order to make the content of these documents searchable (e.g. find me a phase-diagram of material XYZ), one needs essentially two components.", "metadata": {"source": "data/28_PWJ_2018_amachinelearning.pdf", "page_number": 1, "chunk": 2, "title": "Corpus Conversion Service: A Machine Learning Platform to Ingest Documents at Scale."}}, "496": {"idx": 496, "content": "First, you need to ingest documents from a variety of formats (with the PDF format being the most prevalent one) and convert these documents to structured data files with a structured format such as JSON or XML. Second, you need a query engine that is able to deal with a large variety of concepts (documents, images, authors, tables, etc) extracted from these documents and put these into context. In this paper, we focus entirely on the first component, the ingestion of documents and their conversion into structured data files. The solution we propose is thought of as a platform, which at its core has trainable machine learning algorithms. This platform, called Corpus Conversion Service (CCS), consists out of a set of microservices organized in five main components. Each of these microservices can be consumed by its own REST API. This approach not only allows us to build complex pipelines to process documents automatically, but also allows us to develop new microservices against the platform. In order to make this platform scalable, all microservices are integrated through asynchronous communication protocols, which gives us many benefits: It allows to do proper resource management, eliminates strong dependencies and makes the platform robust against single task failures.", "metadata": {"source": "data/28_PWJ_2018_amachinelearning.pdf", "page_number": 1, "chunk": 3, "title": "Corpus Conversion Service: A Machine Learning Platform to Ingest Documents at Scale."}}, "497": {"idx": 497, "content": "To obtain a thorough understanding of what our platform can do and how well it performs, we have structured this paper as follows: In Section 2, we briefly review the current state-of-the-art document processing solutions. In Section 3, we present the design of the platform and its components. In Section 4, we discuss the architecture, the deployment methods, and how well the platform scales with regard to volume (both in users and content) and compute resources, respectively. Finally, in Section 5, we discuss the open 2This is clearly the case on the popular arXiv scientific online repository: https://arxiv. org/help/stats/2012_by_area/index Applied Data Science Track Paper KDD 2018, August 19-23, 2018, London, United Kingdom 774 .", "metadata": {"source": "data/28_PWJ_2018_amachinelearning.pdf", "page_number": 1, "chunk": 4, "title": "Corpus Conversion Service: A Machine Learning Platform to Ingest Documents at Scale."}}, "498": {"idx": 498, "content": "JSON PDF Parse Extract all text-cells,  bitmap images &  linepaths Apply model Apply the ML model on each of  the cells of the parsed page to  determine its semantic label Assemble the document(s) by  combining the parsed pages and their  associated predicted labels for the  cell Assemble Annotate Train Annotate the cells in the  parsed pages  with layout  semantic labels. Train default and/or layout  specific models  for semantic label  prediction  Model available No model available Update model Use new model for annotation Parse bitmap images  to classify image and  extract text (via OCR) Found bitmap  images form  Scanned document Figure 1: A diagram of the conversion pipeline in the Corpus Conversion Service platform. It consists of 5 components: (1) Parsing of the document and its contained bitmap images, (2) Annotating the text of the parsed documents with layout semantic labels, (3) Training models based on the ground-truth acquired by the annotations, (4) Applying machine learned models on the parsed documents to determine the layout semantic label of each cell and finally (5) Assembling the document into a structured data format (e.g. JSON). The main conversion pipeline is depicted in blue and allows you to process and convert documents at scale into a structured data format. The green and orange sections can be used optionally, in order to process scanned documents (green) or train new models based on human annotation (orange). questions w.r.t.", "metadata": {"source": "data/28_PWJ_2018_amachinelearning.pdf", "page_number": 2, "chunk": 0, "title": "Corpus Conversion Service: A Machine Learning Platform to Ingest Documents at Scale."}}, "499": {"idx": 499, "content": "research and possible next steps in the development of the platform. 2 STATE OF THE ART The task of converting PDF documents and automatic content reconstruction has been an outstanding problem for over three decades [3, 4]. Broadly speaking, there are two types of approaches to this problem. In the first approach, documents are converted with the goal to represent the content as close as possible to the original visual layout of the document. This can be done through a conversion from PDF towards HTML or MS Word for example. The second approach attempts to convert the document into a format that can be easily processed programmatically, i.e. a representation of the document which is not preserving the layout, yet contains all the content from the original document in a structured format. For example, this could be a JSON/XML file with a particular schema. Since our Corpus Conversion Service is thought of as a first step towards a knowledge discovery platform for documents, we have opted for the second approach in our solution. Many solutions have already been developed that tackle the problem of document conversion. There are well known open-source programs such as Xpdf3 and Tabula4. There are also proprietary solutions, such as Abby5, Nuance6 or DataCap7. In contrast to the open-source solutions, all three proprietary solutions support also extraction from scanned documents.", "metadata": {"source": "data/28_PWJ_2018_amachinelearning.pdf", "page_number": 2, "chunk": 1, "title": "Corpus Conversion Service: A Machine Learning Platform to Ingest Documents at Scale."}}, "500": {"idx": 500, "content": "Besides the well known opensource and proprietary solutions, there are also countless academic solutions as well as libraries. For example, the challenge of segmenting complex page layouts is actively addressed by recurring competitions posed by ICDAR, as in Ref. [1] and previous editions. 3https://www.xpdfreader.com 4http://tabula.technology/ 5https://www.abbyy.com/ 6https://www.nuance.com/ 7https://www.ibm.com/us-en/marketplace/data-capture-and-imaging 3 PLATFORM DESIGN Given the plethora of existing solutions, we would like to point out how our solution differs from these, and thus approaches the problem of document conversion in a new way. The key idea is that we do not write any rule-based conversion algorithms, but rather utilize generic machine learning algorithms which produce models that can be easily and quickly trained on ground-truth acquired via human annotation. This flexible mechanism allows us to adapt very quickly to certain templates of documents, achieve very accurate results and ultimately eliminates the time-consuming and costly tuning of traditional rule-based conversion algorithms. This approach is in stark contrast to the previously mentioned state of the art conversion systems, which are all rule-based. While the approach of swapping rule based solutions with machine learning solutions might appear very natural in the current era of artificial intelligence, it has some serious consequences with regard to its design.", "metadata": {"source": "data/28_PWJ_2018_amachinelearning.pdf", "page_number": 2, "chunk": 2, "title": "Corpus Conversion Service: A Machine Learning Platform to Ingest Documents at Scale."}}, "501": {"idx": 501, "content": "First of all, one can not think anymore at the level of a single document. Rather, one should think at the level of a collection of documents (or a corpus of documents). A machine learned model for a single document is not very useful, but a machine learned model for a certain type of documents (e.g. scientific articles, patents, regulations, contracts, etc.) obviously is. This is the first big distinction between the current existing solutions and ours: Existing solutions take one document at a time (no matter its origin) and convert it to a desired output format. Our solution can ingest an entire collection of documents and build machine learned models on top of that. Of course, once the the model is trained, one can convert documents one at a time, too. A second discriminator between the existing solutions and ours is that we need to provide the tools to gather ground-truth, since no model can be trained without it. Hence, not only do we need the ability to manage collections of documents, we also need the ability for people to annotate documents and store these annotations in an efficient way. These annotations are then used as ground-truth data Applied Data Science Track Paper KDD 2018, August 19-23, 2018, London, United Kingdom 775 .", "metadata": {"source": "data/28_PWJ_2018_amachinelearning.pdf", "page_number": 2, "chunk": 3, "title": "Corpus Conversion Service: A Machine Learning Platform to Ingest Documents at Scale."}}, "502": {"idx": 502, "content": "Figure 2: The cells obtained for the title page of a poster abstract about the CCS [11] after the parsing stage. During the parsing, we extract all bounding boxes of the text (or cells) in such a way that they all have: (1) a maximum width, (2) are only single line and (3) split into multiple cells in case of list-identifiers, multi-columns or crossing vertical lines (such as in tables). to train models. It is clear then that ML models add an extra level of complexity: One has to provide the ability to store a collection of documents, annotate these documents, store the annotations, train a model and ultimately apply this model on unseen documents. For the authors of this paper, it was therefore evident that our solution cannot be a monolithic application. It fits much better the concept of a cloud-based platform that can execute the previously mentioned tasks in an efficient and scalable way. 3.1 Components Our platform implements a processing pipeline to ingest, manage, parse, annotate, train and eventually convert the data contained in any type of format (scanned or programmatically created PDF, bitmap images, Word documents, etc.) into a structured data format (e.g. JSON or XML).", "metadata": {"source": "data/28_PWJ_2018_amachinelearning.pdf", "page_number": 3, "chunk": 0, "title": "Corpus Conversion Service: A Machine Learning Platform to Ingest Documents at Scale."}}, "503": {"idx": 503, "content": "This processing pipeline is formed by five components as depicted in Figure 1: (1) parsing of documents into an internal format optimised for ML, (2) Annotation of the label ground-truth in parsed documents (3) training ML models from the acquired annotations, (4) applying the custom ML model(s), (5) assembling the document(s) into a structured data format. If a trained model is available, only components 1, 4 and 5 are needed to convert the documents. If no template-specific machine learned model is available yet, we provide two additional components 2 and 3, that allow users to gather ground-truth and train custom models. It is important to note that the platform comes with default models, so annotation and training are advised to retrieve the best quality output, yet they are optional. Let us now elaborate on what each of the five components deliver in the rest of this section. Figure 3: The labelled cells annotated on the title page of a poster abstract about the CCS [11]. Here, the title, authors, affiliation, subtitle, main-text, caption and picture labels are represented respectively as red, green, purple, dark-red, yellow, orange and ivory. 3.2 Parsing of Documents In the parsing component of the pipeline, we solve the following straightforward but non-trivial task: Find the bounding boxes of all text-snippets that appear on each PDF page. For simplicity, we will refer to the bounding boxes of the text-snippets as cells in the remainder of the paper.", "metadata": {"source": "data/28_PWJ_2018_amachinelearning.pdf", "page_number": 3, "chunk": 1, "title": "Corpus Conversion Service: A Machine Learning Platform to Ingest Documents at Scale."}}, "504": {"idx": 504, "content": "There are two reasons why we are interested in these cells. First, they provide us with the crucial geometric features which are later used in the machine learning models to determine the layout semantic label. Second, the concept of a cell can be easily transferred to scanned documents. In Figure 2, we show the cells obtained from an example PDF page after the parsing stage. While the task of finding the cells might appear intuitive from a conceptual point of view, it is not in practice, since there does not exist a unique, precise definition of the cells. This lack of a precise definition has its origins not only in the ISO-standard8 detailing the PDF document code but also in the variability of the quality of PDFs. Older PDFs which were created from scanned images using OCR typically return cells for each word, while more recent PDFs allow us to create cells for full text-lines. This variability in the geometric features of the cell (e.g. the width of the cell) can negatively impact the performance of later machine learning models. As a consequence, we reduce the variability of the geometric features as much as possible. The more consistent and homogeneous the geometric features of a cell are, the better the machine learning algorithms can do predictions. For programmatic PDFs, the text cells are contructed from raw streams of symbols and transforms defined in the PDF document. This operation relies on the iterators provided by the QPDF library9.", "metadata": {"source": "data/28_PWJ_2018_amachinelearning.pdf", "page_number": 3, "chunk": 2, "title": "Corpus Conversion Service: A Machine Learning Platform to Ingest Documents at Scale."}}, "505": {"idx": 505, "content": "For scanned PDFs, we use a two step approach to find the cells by first running all bitmap resources in the PDF through an OCR engine and then merging the extracted text-snippets from the images with 8a line of text might be printed character-by-character, word-by-word or the entire text snippet. 9http://qpdf.sourceforge.net/ Applied Data Science Track Paper KDD 2018, August 19-23, 2018, London, United Kingdom 776 .", "metadata": {"source": "data/28_PWJ_2018_amachinelearning.pdf", "page_number": 3, "chunk": 3, "title": "Corpus Conversion Service: A Machine Learning Platform to Ingest Documents at Scale."}}, "506": {"idx": 506, "content": "the remaining cells from the programmatically created content. Eventually, all the created cells and line paths are stored in an internal JSON format, which also keeps references to the bitmap resources embedded in the PDF document. From this point, all further processing does not need to distinguish between scanned or programmatic sources. 3.3 Ground-truth gathering through human-annotation In this component, we collect ground-truth for the custom machine learning models to be trained on. Representative ground-truth data is of paramount importance to obtain machine learned models with excellent recall and precision. Unfortunately, it is often very hard to obtain lots of representative ground-truth data, primarily due the the enormous variability across the layout of documents. As a consequence, the concept of annotators for documents were incorporated into the platform from the very beginning. The purpose of these annotators is two-fold. First and foremost, the annotators on the platform allow us to gather ground-truth at scale using a crowd-sourcing approach. In each annotation task, we retrieve the original PDF page and its associated parsed components, containing the cells (see Figure 2). We then ask the (human) annotator to assign each cell a layout semantic label. Examples of semantic labels are: Title, Abstract, Authors, Subtitle, Text, Table, Figure, List, etc10. In the annotator tool, each layout semantic label is visually represented by a colour.", "metadata": {"source": "data/28_PWJ_2018_amachinelearning.pdf", "page_number": 4, "chunk": 0, "title": "Corpus Conversion Service: A Machine Learning Platform to Ingest Documents at Scale."}}, "507": {"idx": 507, "content": "By assigning a colour to each semantic label, the task of semantic annotation is translated into a colouring-task, as can be seen in Figure 3. Since humans are very efficient in visual recognition, this task comes very natural to us. The required time spent to annotate a single page starting from the parsing output has shown to average at 30 seconds over various annotation campaigns. The second purpose of the annotators is to visually inspect the quality of our machine learned models. The goal of the models is to emulate the action of the annotators, i.e. to assign a layout semantic label to each cell. Clearly, the result of a prediction for each page can therefore be displayed as if it were an annotated page. This allows the users to directly inspect the results of the models on unseen pages. A direct consequence of this inspection capability in the annotators is that the annotation task can be transformed easily into a correction task, i.e. the human annotators only need to correct the incorrectly predicted labels. Of course, as the models become better over time, the number of corrections needed to be made become less and less. This allows us to significantly reduce the annotation time per document. Since annotations are typically created by professionals with a high hourly rate, the colouring technique allowed us to significantly reduce the cost of groundtruth gathering. In Figure 3, we show the annotation-rate in number-of-annotatedpages per minute.", "metadata": {"source": "data/28_PWJ_2018_amachinelearning.pdf", "page_number": 4, "chunk": 1, "title": "Corpus Conversion Service: A Machine Learning Platform to Ingest Documents at Scale."}}, "508": {"idx": 508, "content": "The vertical red lines indicate that a training was performed on the annotated pages, and a new, improved model is used from that point to predict the labels. Since the corrections become less and less, the rate of annotation goes up. It is needless to 10It is important to notice that there is no restriction on the number of labels nor the semantic meaning of these labels. The only limitation one has is that the set of semantic labels needs to be consistent across the dataset, but this is evidently true for any type of ML algorithm. 0 10 20 30 40 50 # annotated pages 5 10 15 20 25 30 pages / min train train train train train PhysRev Elsevier mean Figure 4: The annotation rate of pages for two different collections (Physical Review B and Elsevier papers) as a function of the number of annotated pages. As one can observe, the mean annotation rate is increasing after each training (depicted by a vertical dashed red line). After the first training, the human annotator is presented a pre-annotated page, using the predictions from the latest model. As the predictions become better with increasing size of the ground-truth, less corrections need to be made and hence more pages can be annotated in similar time intervals.", "metadata": {"source": "data/28_PWJ_2018_amachinelearning.pdf", "page_number": 4, "chunk": 2, "title": "Corpus Conversion Service: A Machine Learning Platform to Ingest Documents at Scale."}}, "509": {"idx": 509, "content": "say that this inter-leaving of training models (based on annotated ground-truth) and annotation benefits directly from our platform approach, since each task (submitting page-annotations, training the model, applying the model for predicting the labels) comes down to an asynchronous call to a microservice. The accelerated annotation leads to a speed-up of a factor of 10 for ground-truth collection. 3.4 Machine Learning: Training models & Applying models In the CCS, there are essentially two types of machine-learning models. On the one hand, we have default models, which are designed to be layout independent. They take a raster image of the page to identify and locate basic objects, such as tables, figures, formulas, etc. On the other hand, we also support the training of custom, template-specific models, which are designed to specialize on a particular layout template and allow us to convert and extract the data out of documents with very high precision and recall. They will classify each cell in the page with regard to their layout semantic label. 3.4.1 Metrics. Before discussing the performance of the models, let us first define the precision and recall metrics used to evaluate the results. The first observation is that the output of a machine learned model is exactly the same of what a human annotator would produce, i.e. it will assign a text cell a semantic label. The correctness of this label is what we aim to measure with the recall and precision metrics.", "metadata": {"source": "data/28_PWJ_2018_amachinelearning.pdf", "page_number": 4, "chunk": 3, "title": "Corpus Conversion Service: A Machine Learning Platform to Ingest Documents at Scale."}}, "510": {"idx": 510, "content": "The second observation is that we deal with Applied Data Science Track Paper KDD 2018, August 19-23, 2018, London, United Kingdom 777 .", "metadata": {"source": "data/28_PWJ_2018_amachinelearning.pdf", "page_number": 4, "chunk": 4, "title": "Corpus Conversion Service: A Machine Learning Platform to Ingest Documents at Scale."}}, "511": {"idx": 511, "content": "Figure 5: A typical image of a parsed PDF page that is fed to the default models. In red, we show the detection of the tables combined with the confidence of the model. The results displayed here originate from the YOLOv2 model. a multi-class classification problem, i.e. we don\u2019t have only two labels, but many possible semantic labels, hence the performance result will be the average of the recall and precision for each label. The recall (=R) and precision (=P) for a given label on a page is defined by the standard formulas R = tp tp + fp , P = tp tp + fn , (1) wheretp, fp and fn represent respectively true positive, false positive and false negative predicted labels. 3.4.2 Default Models. The aim of the default models is to identify specific, ubiquitous objects in documents. Examples of such objects are tables, figures with their captions, mathematical formulas, etc. Due to the high variability in both the document layout as well as in the representation of these objects, we need very robust object detection methods. Currently, the most robust methods for detecting objects are deep neural networks such as R-CNNs (and their derivatives Fast- and Faster-R-CNN) [5, 6, 10], the YOLO architecture [8, 9] and the SSD networks [7]. On our platform, we have the Faster-R-CNN [10] and the YOLOv2 [9] networks available as individual microservices, both for training and predictions.", "metadata": {"source": "data/28_PWJ_2018_amachinelearning.pdf", "page_number": 5, "chunk": 0, "title": "Corpus Conversion Service: A Machine Learning Platform to Ingest Documents at Scale."}}, "512": {"idx": 512, "content": "In this paper, we will focus only on the detection of table objects, but the same principles described in the following analysis are also applied for other type of objects. Table 1: Time-to-solution and performance results for the Faster RCNN and YOLOv2 models. The training of the models was done on 25000 PDF pages. The prediction (per page) and performance numbers (Recall=R and Precision=P) were obtained on 5000 page images, where the prediction confidence cutoff was tuned to yield the maximum F1 score for each. All time-to-solution measurements for training were obtained on a POWER8 node with a single Pascal P100 GPU. Time to solution Performance Training Prediction P R Faster-RCNN 72 hours 4 sec 0.97 0.98 YOLOv2 9 hours 0.1 sec 0.99 0.98 The networks available on our platform have been trained on arXiv data11. We have annotated 30000 PDF pages and know the location of at least one table on each page. From these 30000 pages, we have used 25000 pages as training data and kept the other 5000 pages for evaluation. Due to the large size of the dataset, we did not need to employ any data-augmentation technique, which is usually necessary for object-detection or image-classification algorithms. We do not locate the table directly on the image of the original PDF page but rather on an image representation of the parsed PDF page with cell boxes.", "metadata": {"source": "data/28_PWJ_2018_amachinelearning.pdf", "page_number": 5, "chunk": 1, "title": "Corpus Conversion Service: A Machine Learning Platform to Ingest Documents at Scale."}}, "513": {"idx": 513, "content": "The reasoning behind this is to reduce the variability between all input PDF pages as much as possible and thus increase the effectiveness of the deep neural networks. An example of such an image can be seen in Figure 5. The red bounding boxes around the tables are a result of the prediction using YOLOv2 and are absent in the image on which the model predicts. Note that the visualisation of the text cells visible in Figure 5 does not include any text of the original document, but only its geometrical definition. This is important when one compares for example Asian documents with Japanese, Chinese or Korean characters versus European languages with the roman alphabet. We do not want the deep neural network to focus on the specific characters, but rather on the layout of the cells in the page. Let us now discuss both deep neural network training microservices on the platform. In Table 1, we show the time-to-solution for training and predicting a single page as well as the performance in terms of recall and precision. In the training phase, we ensure that both algorithms ran each 100 epochs, i.e. all 25000 page images were fed to the network 100 times. We observe that the out-ofthe-box Faster R-CNN from Tensorflow does not implement any batching during the training phase, while YOLOv2 batches 8 images at a time, thanks to an image resizing which is automatically applied. We believe that this is the main origin for the discrepancy of time-to-solution for the training phase.", "metadata": {"source": "data/28_PWJ_2018_amachinelearning.pdf", "page_number": 5, "chunk": 2, "title": "Corpus Conversion Service: A Machine Learning Platform to Ingest Documents at Scale."}}, "514": {"idx": 514, "content": "The same holds true for the prediction. Therefore, from the point of view of the platform, the YOLOv2 architecture seems better suited for deployment, as it allows to have a much higher throughput (\u224810 pages/sec/node). For the performance analysis, let us outline one pre-processing stage which is needed before computing the metrics described 11All the data is coming from the bulk data download https://arxiv.org/help/bulk_ data_s3 Applied Data Science Track Paper KDD 2018, August 19-23, 2018, London, United Kingdom 778 .", "metadata": {"source": "data/28_PWJ_2018_amachinelearning.pdf", "page_number": 5, "chunk": 3, "title": "Corpus Conversion Service: A Machine Learning Platform to Ingest Documents at Scale."}}, "515": {"idx": 515, "content": "previously. The object-detection networks predict a set of bounding boxes with a confidence level between 0 and 1. We use these bounding boxes to associate with each cell a label, which is in this particular case either Table or Not-Table, depending on whether they overlap with the predicted bounding box. The corresponding recall and precision are then computed for this dual-class classification problem. In order to do a fair comparison of the two networks, we optimise the precision and recall metrics with regard to the predicted confidence. For YOLOv2 we observe that the recall goes down and the precision goes up as the confidence is increased, obtaining a maximum F1 score of 98.7% at a confidence level of 0.5. The Faster R-CNN method is also performing quite well, but has slightly lower precision and recall numbers. We believe this originates from the selective search algorithm which is used to determine regions of interest. The images we feed it are not typical photographic images (made with a camera) but layout visualisations. The selective search algorithm in Faster R-CNN might not be optimal for such type of objects. 3.4.3 Template specific Models. The goal of template specific models is to obtain a better extraction quality by specializing the model on a specific template. This is necessary in many technical fields, where the accuracy of the extracted data is of paramount importance.", "metadata": {"source": "data/28_PWJ_2018_amachinelearning.pdf", "page_number": 6, "chunk": 0, "title": "Corpus Conversion Service: A Machine Learning Platform to Ingest Documents at Scale."}}, "516": {"idx": 516, "content": "Furthermore, many technical documents in a specific field typically appear in a certain template and it often makes sense to take advantage of this template to improve extraction quality. For an algorithm to fit in the interactive platform design we identified a few key requirements. First, it is crucial that the model can generate good results with a limited set of pages. In practice this means the algorithm needs to perform well for 100-400 annotated pages, or the equivalent of a couple of man-hours for annotation. Second it must be robust against extreme imbalance of the labeled data. It is clear that cells of the label Title will be much more uncommon than cells with the label of Text. Last, the model needs to be very quick in training and predicting, since it will support the interactive annotation process. For these reasons, we chose random forest [2] as a machine learning algorithm for template specific models. Random forest algorithms are known to be trained fast and can produce very accurate results on limited, but relatively structured data. In our case, this structure originates of course from the template. Furthermore, random forest is an ensemble method, meaning that they learn on the distribution function of the features, and not individual dataelements. As a consequence, they are typically more robust against imbalance of the labeled data, since the distribution functions are renormalised.", "metadata": {"source": "data/28_PWJ_2018_amachinelearning.pdf", "page_number": 6, "chunk": 1, "title": "Corpus Conversion Service: A Machine Learning Platform to Ingest Documents at Scale."}}, "517": {"idx": 517, "content": "The random forest method is applied to each cell of the page based on a feature vector representing all of its properties. For example, the feature vector contains information as the page number, the size of the text cell, its position, as well as the distance from the neighbouring cells. Additionally to pure geometrical information we include the text style (normal, italic, or bold) and some text statistics, as the fraction of numeric characters. We then improve the obtained results by performing subsequent iterations with other random forest methods, which operate on an enlarged feature space including the previously predicted labels of the neighbourhood around the current cell. Table 2: Performance results for the template specific model of the Physical Review B journals. The confusion matrix highlights the huge imbalance between the number of text cells with different labels. The usage of ensemble machine learning methods allows to achieve a very high accuracy over all label types. predicted label true label Title Author Subtitle Text Picture Table Title 75 0 0 0 0 0 Author 1 670 0 0 0 0 Subtitle 0 0 325 0 0 0 Text 1 17 0 56460 14 0 Picture 0 0 0 4 4223 26 Table 0 0 0 0 1 3418 Recall 100 99.85 100 99.94 99.24 99.97 Precision 97.40 97.52 100 99.99 99.64 99.24 Table 3: Comparison for two different journal templates showing the aggregated precision and recall averaged over all labels. Each model has been independently trained on a dataset of 400 pages each.", "metadata": {"source": "data/28_PWJ_2018_amachinelearning.pdf", "page_number": 6, "chunk": 2, "title": "Corpus Conversion Service: A Machine Learning Platform to Ingest Documents at Scale."}}, "518": {"idx": 518, "content": "The results show that the ML algorithm proves to perform very well for the multiple document templates, simply by providing a different dataset to train on. Journal template P R Physical Review B 98.96 99.83 Elsevier 99.46 99.58 It is important to realize that almost all of these features are purely geometrical. This allows us to apply exactly the same machine learning methods on both scanned and programmatic PDF documents. In Table 2, we illustrate the performance results of the models for a particular scientific journal, Physical Review B12. We randomly chose 100 open-access papers and annotated 400 pages of them with 6 semantic labels. Tables 2 shows the confusion matrix between the true and the predicted labels as well as the derived recall and precision metrics for each label. We observe that the recall and precision numbers are excellent, with most of them above 99%. This is not surprising, since we are building models that specialise for a particular template. Moreover, the same ML algorithm proves to perform very well on different document templates, as is evident from the numbers shown in Table 3, simply by providing it with different datasets to train on. The latter is the power of our platform: we can re-use the same machine-learning algorithm to generate different models solely based on the data gathered by the annotation on the platform.", "metadata": {"source": "data/28_PWJ_2018_amachinelearning.pdf", "page_number": 6, "chunk": 3, "title": "Corpus Conversion Service: A Machine Learning Platform to Ingest Documents at Scale."}}, "519": {"idx": 519, "content": "We do not need to define rules and heuristics or update code in 12https://journals.aps.org/prb Applied Data Science Track Paper KDD 2018, August 19-23, 2018, London, United Kingdom 779 .", "metadata": {"source": "data/28_PWJ_2018_amachinelearning.pdf", "page_number": 6, "chunk": 4, "title": "Corpus Conversion Service: A Machine Learning Platform to Ingest Documents at Scale."}}, "520": {"idx": 520, "content": "Listing 1: Excerpt from the JSON output of the Corpus Conversion Service after conversion of this paper. { \"description\": { \"title\": \"Corpus Conversion Service: A machine learning platform to ingest documents at scale.\", ,\u2192 \"abstract\": \"Over the past few decades, the amount of scientific articles [...]\" , ,\u2192 \"affiliations\": \"IBM Research Rueschlikon, Switzerland \", \"authors\": \"Peter W J Staar, Michele Dolfi, Christoph Auer, Costas Bekas \" }, \"main-text\": [ { \"prov\": [ { \"bbox\": [ 52.304, 509.750, 168.099, 523.980 ], \"page\": 1 } ], \"type\": \"subtitle-level-1\", \"text\": \"1 INTRODUCTION\" }, { \"prov\": [ { \"bbox\": [ 52.304, 337.678, 286.067, 380.475], \"page\": 1 } ], \"type\": \"paragraph\", \"text\": \"It is estimated that [...] put these into context.\" }, ... ], \"tables\": [ {...}, ... ], \"images\": [ {...}, ... ] } order to deal with new types of documents. We only need to gather more data. 3.5 Assembly In this component, we build a structured data file in JSON or XML format, which contains all the text and objects (e.g. tables) from the original document, retaining the layout semantics. This structured data file is constructed by assembling all the cells from the parsed file in combination with their associated predicted (or human-annotated) layout semantic labels. It should be noted that no machine learning is used in this component. It is purely rule based and therefore completely deterministic. The assembly phase is a two step process.", "metadata": {"source": "data/28_PWJ_2018_amachinelearning.pdf", "page_number": 7, "chunk": 0, "title": "Corpus Conversion Service: A Machine Learning Platform to Ingest Documents at Scale."}}, "521": {"idx": 521, "content": "First, one gathers all the cells with their associated layout semantic label and sorts them according to reading order. Then, the text of all cells that have the same label is contracted into a temporary document objects. Third, we build the internal structure of the temporary document objects, based on the information provided by the models. The latter is only applicable for internally structured objects, such as tables. An example of the generated JSON output is shown in Listing 1. 4 ARCHITECTURE AND ORCHESTRATION OF CLOUD BASED MICROSERVICES In this section, we describe how the microservices in each of the components of the platform are deployed and orchestrated. Before discussing the technical details, we would like to point out our requirements for the architecture of the platform. These requirements are all related to scaling. Specifically, we would like the platform to scale with the number of documents, the number of users and last but not least the number of cloud based compute resources. In other words, we want a service that can ingest millions of documents, serve potentially thousands of users and scale its compute Storage Compute Orchestration Frontend REST API NoSQL Database Object Store Async. Worker 1 Async. Worker 2 Async. Worker N \u2026 Message Broker Results Backend User Interface Figure 6: Diagram of the architecture of our platform.", "metadata": {"source": "data/28_PWJ_2018_amachinelearning.pdf", "page_number": 7, "chunk": 1, "title": "Corpus Conversion Service: A Machine Learning Platform to Ingest Documents at Scale."}}, "522": {"idx": 522, "content": "The architecture is composed from 4 layers: an interface layer with REST-API and frontend, an orchestration layer with a message broker and results backend, a compute layer consisting out of a variable number of asynchronous workers and finally a storage layer providing a NoSQL database and an object store. The NoSQL database stores the queryable meta-data of each file that is stored in the object store. resources such that the time-to-solution is reasonable at all times for any operation. It is clear that the architecture of such a service is heavily influenced by these requirements. 4.1 Platform layers In Figure 1, we have shown a diagram of our pipeline on the platform to process documents. In Figure 6, we show a sketch of its architecture. As one can observe, we have grouped the service into four layers. These layers are: (1) An interface layer which implements a REST-API and a user frontend: The user frontend is an AngularJS application build on top of the REST-API and implements the annotators for ground-truth gathering. The REST-API is built and documented using the OpenAPI specifications13 and is implemented in Python. (2) An orchestration layer that schedules the tasks for the microservices, stores their execution status and final result. The task scheduling is done with the Message Broker RabbitMQ14. The results are stored in the in-memory data store Redis15. In order to perform certain consecutive tasks (e.g.", "metadata": {"source": "data/28_PWJ_2018_amachinelearning.pdf", "page_number": 7, "chunk": 2, "title": "Corpus Conversion Service: A Machine Learning Platform to Ingest Documents at Scale."}}, "523": {"idx": 523, "content": "parsing a PDF page with embedded scanned images requires first a parsing of the programmatic PDF page to extract the images and then an OCR service to extract the cells from these images) we can directly chain tasks, such that subsequent steps are only executed if the previous terminated successfully. This approach allows for a very robust, fault-tolerant service with very little downtime. (3) A compute layer that implements the microservices detailed in section 3: Each of the workers in this layer executes the available microservices (e.g. parsing, training, predictions, assembly, etc). In order to scale with regard to resources, we have encapsulated each microservice into a distributed 13https://www.openapis.org/ 14https://www.rabbitmq.com/ 15https://www.redis.io/ Applied Data Science Track Paper KDD 2018, August 19-23, 2018, London, United Kingdom 780 .", "metadata": {"source": "data/28_PWJ_2018_amachinelearning.pdf", "page_number": 7, "chunk": 3, "title": "Corpus Conversion Service: A Machine Learning Platform to Ingest Documents at Scale."}}, "524": {"idx": 524, "content": "2017-04 2017-05 2017-06 2017-07 2017-08 2017-09 2017-10 2017-11 2017-12 2018-01 0 50 100 150 200 250 users 0 200000 400000 600000 800000 1000000 pages users pages Figure 7: Evolution of number of users and number of PDF pages on the platform. The jumps in the number of pages originates from big ingestions of documents performed by some users. This proves that the CCS platform is also able to accomodate these short burst of extreme activity. task queue using the Celery library16. This allows us to dynamically scale the compute resources, since each worker can be spawned automatically on the cluster and register itself to the broker. The workers are not only consumers of tasks, but may also produce new ones. This is the case for the requests operating on the whole corpus. Whenever possible we parallelise the compute-heavy operations at the page (or document) level. (4) A storage layer that stores all documents as well as the results from the microservices: The storage layer is composed out of two services: an object-store that stores all documents and processed stages (e.g. the parsed PDF pages, trained models, etc) and a queryable NoSQL database that stores the metadata of each file in the object-store. The object-store allows us to easily scale the storage with regard to the number of processed documents.", "metadata": {"source": "data/28_PWJ_2018_amachinelearning.pdf", "page_number": 8, "chunk": 0, "title": "Corpus Conversion Service: A Machine Learning Platform to Ingest Documents at Scale."}}, "525": {"idx": 525, "content": "However, it is not build to be queried efficiently, which is why we put a NoSQL database (in our case we use MongoDB17) on top to manage the storage and act as an access-layer. By design, all the microservices in the compute layer are stateless, i.e. they don\u2019t manage any data, but only operate on it. This allows us to trust the additional stability and data safety concerns to the state-of-the-art tools that we have chosen, such as MongoDB, Redis and RabbitMQ. Being a cloud-based platform, our solution allows for these software assets to be detached from the main deployment and to be served by specialised vendors services which are certified to the latest industry requirements such as data-at-rest encryption, high availability, etc. The choice of the services plays also a crucial role in addressing the scaling requirements for the platform. From the sketch (Fig. 6), 16http://www.celeryproject.org/ 17https://www.mongodb.com/ 1 2 3 4 5 # of worker nodes 1 2 3 4 5 6 Speedup parse apply model assemble Figure 8: Speedup in the pipeline components as a function of the number of worker nodes (each with four cores, running four local worker processes). it is clear that the compute layer has a considerable amount of communication with these external services. During the development we evaluated multiple options and, e.g. we had to replace some services because of inadequate performance or scaling bottlenecks.", "metadata": {"source": "data/28_PWJ_2018_amachinelearning.pdf", "page_number": 8, "chunk": 1, "title": "Corpus Conversion Service: A Machine Learning Platform to Ingest Documents at Scale."}}, "526": {"idx": 526, "content": "For example other result-backends didn\u2019t offer the auto-cleaning functionality offered by Redis and, before opting for a custom solution mixing MongoDB with an object storage, we evaluated other solutions as the GridFS storage, but it didn\u2019t fit to the constraints of typical cloud environments. 4.2 Deployment Our platform is deployable on Kubernetes clusters18 available on many cloud providers or even on-premise installations, e.g. using the IBM Cloud Private 19 distribution. Depending on the requirements, the storage services are launched inside the same cluster or linked to externally hosted endpoints. The common parts of all deployments are the interface and the compute layer. The compute layer is designed for dynamically adapt the number of resources on the current load. For example, more parsing-microservice instances could be spawned when a large document is uploaded and they can automatically scaled down at the end of the task, such that the resources are free for other components, like training and assembling the processed documents. The components running in the compute layer are further organized in different queues, such that we can control the fraction of resources allocated for each different component depending on their computational requirements. The parse component is indeed more demanding than the simple annotation components.", "metadata": {"source": "data/28_PWJ_2018_amachinelearning.pdf", "page_number": 8, "chunk": 2, "title": "Corpus Conversion Service: A Machine Learning Platform to Ingest Documents at Scale."}}, "527": {"idx": 527, "content": "Currently, our main system operates on 5 Kubernetes nodes with 4 CPU cores and 8 GB of main memory each, and additionally one POWER 8 node with four GPUs is dedicated to the deep learning training and prediction tasks. Here, the flexible binding of microservices to specific nodes is a great advantage of the Kubernetes deployment. Moreover, 5 other virtual machines are employed to host the services in the orchestration and store layer. Applied Data Science Track Paper KDD 2018, August 19-23, 2018, London, United Kingdom 781 .", "metadata": {"source": "data/28_PWJ_2018_amachinelearning.pdf", "page_number": 8, "chunk": 3, "title": "Corpus Conversion Service: A Machine Learning Platform to Ingest Documents at Scale."}}, "528": {"idx": 528, "content": "4.3 Scaling benchmarks Let us now discuss some scaling results on our platform. As we pointed out in the beginning of the section, our requirements for the platform were scaling with regard to the number of users, the number of processed documents and compute resources. In Figure 7, we show the number of users and the number of processed PDF pages20 as a function of time. As one can see, the number of users and processed PDF pages has been increasing steadily over time since the launch of our service in April 2017. It is however interesting to see that there are sharp steps, indicating that some users have been uploading massive amounts of documents into the service in a very small amount of time. Due to our design, it was not a problem to accommodate these peaks and our service was able to handle these short burst of extreme activity. In Figure 8, we show the scaling of the three main pipeline microservices (i.e. the parsing of PDF documents, applying machine learned models and conversion of documents to JSON) on the platform with regard to compute resources. We show this scaling by displaying the speedup versus the number of worker nodes available. Here, we chose to have four workers serving each pipeline microservice, since each worker is running on a node with four cores. As one can observe, the speedup in the parse and ML apply tasks scales linearly with the the number of workers, and thus the nodes.", "metadata": {"source": "data/28_PWJ_2018_amachinelearning.pdf", "page_number": 9, "chunk": 0, "title": "Corpus Conversion Service: A Machine Learning Platform to Ingest Documents at Scale."}}, "529": {"idx": 529, "content": "Notably, we can even observe a slightly better-thanlinear speedup, which appears due to bandwidth constraints on the baseline with one worker. The speedup on the assemble tasks, in comparison, flattens off sooner, as this task can only be parallelised on the document and not on the page level. The variability in the length of documents is reflected in a load imbalance between the worker nodes, however this averages out with sufficiently large corpus sizes. Consequently, we are able to scale the compute resources in order to keep the time-to-solution constant for any job-size. 5 CONCLUSION We have presented a scalable, cloud based platform, which can ingest, parse and annotate documents, and particularly, train & apply advanced machine learning models in order to extract the content of the ingested documents and convert it into a structured data representation. The fundamental design choices in our solution have proven to enable scaling in three elementary ways. First, it can service multiple users concurrently. Second, it can ingest, parse and apply machine learned models on many documents at the same time. Third, it can scale its compute resources for different tasks on the platform according to their respective load so the conversion of documents on the platform is at all times bounded in time, given enough resources. In the future, we plan to extend the platform in two major areas.", "metadata": {"source": "data/28_PWJ_2018_amachinelearning.pdf", "page_number": 9, "chunk": 1, "title": "Corpus Conversion Service: A Machine Learning Platform to Ingest Documents at Scale."}}, "530": {"idx": 530, "content": "First, we would like to extend the number of microservices, especially with regard to image understanding. The number of types of images is enormous (e.g. line & scatterplot, histograms, pie-charts, geographic maps, etc). The goal here would be to extract 18https://kubernetes.io/ 19ibm.biz/privatecloud 20We don\u2019t show the number of documents, since the number of pages in a document can range from 1 to well above 1000. Consequently, the number of pages is a more robust metric to measure the scaling with regard to the corpus size. the data out of these individual type of images after a successful identification with an image-classifier. Second, we would like to improve the quality and performance of our default models. We strongly believe that the results can be greatly improved since the neural networks we currently use are optimised for photographic images, and not images of parsed document pages (as is shown in Figure 5). To leverage this growing use of deep learning models, we will additionally introduce specialised data-parallelism in order to speed up the training and provide interactive user-customisation capabilities. ACKNOWLEDGMENTS The authors would like to thank Roxana Istrate and Matthieu Mottet for their contribution to the development of the CCS system. This work was supported by the NCCR MARVEL (http://nccr-marvel. ch), funded by the Swiss National Science Foundation.", "metadata": {"source": "data/28_PWJ_2018_amachinelearning.pdf", "page_number": 9, "chunk": 2, "title": "Corpus Conversion Service: A Machine Learning Platform to Ingest Documents at Scale."}}, "531": {"idx": 531, "content": "MD was supported by the FORCE project, funded by Horizon 2020 under NMBP-23-2016 call with Grant agreement number 721027 (http: //the-force-project.eu). REFERENCES [1] A. Antonacopoulos, C. Clausner, C. Papadopoulos, and S. Pletschacher. 2015. ICDAR2015 Competition on Recognition of Documents with Complex Layouts \u2013 RDCL2015. In Proceedings of the 13th International Conference on Document Analysis and Recognition (ICDAR2015). Nancy, 1151\u20131155. [2] Leo Breiman. 2001. Random Forests. Machine Learning 45, 1 (01 Oct 2001), 5\u201332. https://doi.org/10.1023/A:1010933404324 [3] R. Cattoni, T. Coianiz, S. Messelodi, and C. M. Modena. 1998. Geometric layout analysis techniques for document image understanding: a review. Technical Report. [4] Jean-Pierre Chanod, Boris Chidlovskii, Herv\u00e9 Dejean, Olivier Fambon, J\u00e9r\u00f4me Fuselier, Thierry Jacquin, and Jean-Luc Meunier. 2005. From Legacy Documents to XML: A Conversion Framework. Springer Berlin Heidelberg, Berlin, Heidelberg, 92\u2013103. https://doi.org/10.1007/11551362_9 [5] Ross Girshick. 2015. Fast R-CNN. In Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV) (ICCV \u201915). IEEE Computer Society, Washington, DC, USA, 1440\u20131448. https://doi.org/10.1109/ICCV.2015.169 [6] Ross B. Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. 2013. Rich feature hierarchies for accurate object detection and semantic segmentation. CoRR abs/1311.2524 (2013).", "metadata": {"source": "data/28_PWJ_2018_amachinelearning.pdf", "page_number": 9, "chunk": 3, "title": "Corpus Conversion Service: A Machine Learning Platform to Ingest Documents at Scale."}}, "532": {"idx": 532, "content": "arXiv:1311.2524 http://arxiv.org/abs/1311.2524 [7] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C. Berg. 2016. SSD: Single Shot MultiBox Detector. Springer International Publishing, Cham, 21\u201337. https://doi.org/10.1007/ 978-3-319-46448-0_2 [8] Joseph Redmon, Santosh Kumar Divvala, Ross B. Girshick, and Ali Farhadi. 2016. You Only Look Once: Unified, Real-Time Object Detection. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2016), 779\u2013788. [9] Joseph Redmon and Ali Farhadi. 2016. YOLO9000: Better, Faster, Stronger. arXiv preprint arXiv:1612.08242 (2016). [10] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. 2015. Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks. In Advances in Neural Information Processing Systems 28, C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett (Eds.). Curran Associates, Inc., 91\u201399. http://papers.nips.cc/paper/ 5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks. pdf [11] Peter W J Staar, Michele Dolfi, Christoph Auer, and Costas Bekas. 2018. Corpus Conversion Service poster at the SysML conference. http://www.sysml.cc/doc/ 76.pdf Applied Data Science Track Paper KDD 2018, August 19-23, 2018, London, United Kingdom 782 .", "metadata": {"source": "data/28_PWJ_2018_amachinelearning.pdf", "page_number": 9, "chunk": 4, "title": "Corpus Conversion Service: A Machine Learning Platform to Ingest Documents at Scale."}}, "533": {"idx": 533, "content": "GROBID: Combining Automatic Bibliographic Data Recognition and Term Extraction for Scholarship Publications Patrice Lopez European Patent O\ufb03ce, D-10969 Berlin, Germany plopez@epo.org Abstract. Based on state of the art machine learning techniques, GROBID (GeneRation Of BIbliographic Data) performs reliable bibliographic data extractions from scholar articles combined with multi-level term extractions. These two types of extraction present synergies and correspond to complementary descriptions of an article. This tool is viewed as a component for enhancing the existing and the future large repositories of technical and scienti\ufb01c publications. 1 Objectives The purpose of this demonstration is to show to the digital library community a practical example of the accuracy of current state of the art machine learning techniques applied to information extraction in scholarship articles. The demonstration is based on the web application at the following addresse: http://grobid.no-ip.org. 2 Bibliographical Data Extraction After the selection of a PDF document, GROBID extracts the bibliographical data corresponding to the header information (title, authors, abstract, etc.) and to each reference (title, authors, journal title, issue, number, etc.). The references are associated to their respective citation contexts. The result of the citation extraction can be exported as a whole or per reference following di\ufb00erent formats (BibTeX and TEI) and as COInS1.", "metadata": {"source": "data/25_lopez_2009_grobid.pdf", "page_number": 1, "chunk": 0, "title": ""}}, "534": {"idx": 534, "content": "The automatic extraction of bibliographical data is a challenging task because of the high variability of the bibliographical formats and presentations. We have applied Conditional Random Fields to this task following the approach of [1] implemented with the Mallet toolkit [2], based on approx. 1000 training examples for header information, and 1200 training examples for cited references. An evaluation with the reference CORA dataset showed a reliable level of accuracy of 98,6% per header \ufb01eld and 74.9% per complete header instance, 95,7% per citation \ufb01eld and 78.9% per citation instance. 1 See http://ocoins.info M. Agosti et al. (Eds.): ECDL 2009, LNCS 5714, pp. 473\u2013474, 2009. c \u20ddSpringer-Verlag Berlin Heidelberg 2009 .", "metadata": {"source": "data/25_lopez_2009_grobid.pdf", "page_number": 1, "chunk": 1, "title": ""}}, "535": {"idx": 535, "content": "474 P. Lopez By selecting the consolidation option, GROBID sends a request to Crossref web service for each extracted citation. If a core of metadata (such as the main title and the \ufb01rst author) is correctly identi\ufb01ed, the system will possibly retrieve the full publishers metadata. These metadata are then used for correcting the extracted \ufb01elds and for enriching the results. Interestingly, the instance accuracy for citations goes up to 83.2% on the CORA dataset with this option. 3 Multi-level Term Extraction If the option term extraction is selected, the header will be enriched with two lists of terms; a list of disciplinary domains for the purpose of a general categorization of the article and a list of the most signi\ufb01cant terms extracted from the whole body of text. In addition, each citation is enriched with a third list of terms extracted from the di\ufb00erent citation contexts in order to capture the important discriminant aspects for which the reference is used. The usage of terms of domain-speci\ufb01c terminologies is admittedly viewed as one of the most distinguishing features of scienti\ufb01c and technical documents. Term extraction in GROBID follows the approach of [3]. A term is characterized by two scores; one representing its phraseness (or lexical cohesion), i.e. the degree to which a sequence of words can be considered a phrase, and one representing its informativeness , i.e. the degree to which the phrase is representative of a document given a collection of documents.", "metadata": {"source": "data/25_lopez_2009_grobid.pdf", "page_number": 2, "chunk": 0, "title": ""}}, "536": {"idx": 536, "content": "A linguistic chain comprising language identi\ufb01cation, sentence segmentation, word tokenization, POS tagging and lemmatization is \ufb01rst applied. Noun phrases are extracted as term candidates. The Dice coe\ufb03cient is computed for evaluating the phraseness of a term. The informativeness is evaluated based on the estimation of the deviation between the document and a backgroung HMM language model based on a 18 millions corpus of English Wikipedia articles. 4 Application to Digital Libraries We believe that the text processing modules will be a central component of the future digital libraries. The goal of GROBID is to support various user tasks in relation to large article repositories, in particular the assistance for self archiving of articles, the pre-processing of documents for information retrieval, the generation of reliable OpenURL links or automatic citation suggestions. References 1. Peng, F., McCallum, A.: Accurate Information Extraction from Research Papers using Conditional Random Fields. In: Proceedings of HLT-NAACL (2004) 2. McCallum, A., Kachites, A.: MALLET: A Machine Learning for Language Toolkit (2002) 3. Tomokiyo, T., Hurst, M.: A language model approach to keyphrase extraction. In: Proceedings of ACL Workshop on Multiword Expressions (2003) .", "metadata": {"source": "data/25_lopez_2009_grobid.pdf", "page_number": 2, "chunk": 1, "title": ""}}, "537": {"idx": 537, "content": ".", "metadata": {"source": "data/1_grother_1995_handprit.pdf", "page_number": 1, "chunk": 0, "title": "FWRITR"}}, "538": {"idx": 538, "content": ".", "metadata": {"source": "data/1_grother_1995_handprit.pdf", "page_number": 2, "chunk": 0, "title": "FWRITR"}}, "539": {"idx": 539, "content": ".", "metadata": {"source": "data/1_grother_1995_handprit.pdf", "page_number": 3, "chunk": 0, "title": "FWRITR"}}, "540": {"idx": 540, "content": ".", "metadata": {"source": "data/1_grother_1995_handprit.pdf", "page_number": 4, "chunk": 0, "title": "FWRITR"}}, "541": {"idx": 541, "content": ".", "metadata": {"source": "data/1_grother_1995_handprit.pdf", "page_number": 5, "chunk": 0, "title": "FWRITR"}}, "542": {"idx": 542, "content": ".", "metadata": {"source": "data/1_grother_1995_handprit.pdf", "page_number": 6, "chunk": 0, "title": "FWRITR"}}, "543": {"idx": 543, "content": ".", "metadata": {"source": "data/1_grother_1995_handprit.pdf", "page_number": 7, "chunk": 0, "title": "FWRITR"}}, "544": {"idx": 544, "content": ".", "metadata": {"source": "data/1_grother_1995_handprit.pdf", "page_number": 8, "chunk": 0, "title": "FWRITR"}}, "545": {"idx": 545, "content": ".", "metadata": {"source": "data/1_grother_1995_handprit.pdf", "page_number": 9, "chunk": 0, "title": "FWRITR"}}, "546": {"idx": 546, "content": ".", "metadata": {"source": "data/1_grother_1995_handprit.pdf", "page_number": 10, "chunk": 0, "title": "FWRITR"}}, "547": {"idx": 547, "content": ".", "metadata": {"source": "data/1_grother_1995_handprit.pdf", "page_number": 11, "chunk": 0, "title": "FWRITR"}}, "548": {"idx": 548, "content": ".", "metadata": {"source": "data/1_grother_1995_handprit.pdf", "page_number": 12, "chunk": 0, "title": "FWRITR"}}, "549": {"idx": 549, "content": ".", "metadata": {"source": "data/1_grother_1995_handprit.pdf", "page_number": 13, "chunk": 0, "title": "FWRITR"}}, "550": {"idx": 550, "content": ".", "metadata": {"source": "data/1_grother_1995_handprit.pdf", "page_number": 14, "chunk": 0, "title": "FWRITR"}}, "551": {"idx": 551, "content": ".", "metadata": {"source": "data/1_grother_1995_handprit.pdf", "page_number": 15, "chunk": 0, "title": "FWRITR"}}, "552": {"idx": 552, "content": ".", "metadata": {"source": "data/1_grother_1995_handprit.pdf", "page_number": 16, "chunk": 0, "title": "FWRITR"}}, "553": {"idx": 553, "content": ".", "metadata": {"source": "data/1_grother_1995_handprit.pdf", "page_number": 17, "chunk": 0, "title": "FWRITR"}}, "554": {"idx": 554, "content": ".", "metadata": {"source": "data/1_grother_1995_handprit.pdf", "page_number": 18, "chunk": 0, "title": "FWRITR"}}, "555": {"idx": 555, "content": ".", "metadata": {"source": "data/1_grother_1995_handprit.pdf", "page_number": 19, "chunk": 0, "title": "FWRITR"}}, "556": {"idx": 556, "content": ".", "metadata": {"source": "data/1_grother_1995_handprit.pdf", "page_number": 20, "chunk": 0, "title": "FWRITR"}}, "557": {"idx": 557, "content": ".", "metadata": {"source": "data/1_grother_1995_handprit.pdf", "page_number": 21, "chunk": 0, "title": "FWRITR"}}, "558": {"idx": 558, "content": ".", "metadata": {"source": "data/1_grother_1995_handprit.pdf", "page_number": 22, "chunk": 0, "title": "FWRITR"}}, "559": {"idx": 559, "content": ".", "metadata": {"source": "data/1_grother_1995_handprit.pdf", "page_number": 23, "chunk": 0, "title": "FWRITR"}}, "560": {"idx": 560, "content": ".", "metadata": {"source": "data/1_grother_1995_handprit.pdf", "page_number": 24, "chunk": 0, "title": "FWRITR"}}, "561": {"idx": 561, "content": ".", "metadata": {"source": "data/1_grother_1995_handprit.pdf", "page_number": 25, "chunk": 0, "title": "FWRITR"}}, "562": {"idx": 562, "content": ".", "metadata": {"source": "data/1_grother_1995_handprit.pdf", "page_number": 26, "chunk": 0, "title": "FWRITR"}}, "563": {"idx": 563, "content": ".", "metadata": {"source": "data/1_grother_1995_handprit.pdf", "page_number": 27, "chunk": 0, "title": "FWRITR"}}, "564": {"idx": 564, "content": ".", "metadata": {"source": "data/1_grother_1995_handprit.pdf", "page_number": 28, "chunk": 0, "title": "FWRITR"}}, "565": {"idx": 565, "content": "Algorithms for the Longest Common Subsequence Problem  DANIEL S. HIRSCHBERG  Princeton Untverslty, Princeton, New Jersey  AaS~ACT Two algorithms are presented that solve the longest common subsequence problem The first  algorithm is applicable in the general case and requires O(pn + n log n) time where p is the length of the  longest common subsequence The second algorithm requires time bounded by O(p(m + 1 - p)log n) In the  common speoal case where p is close to m, this algorithm takes much less time than n ~  KEY WORDS AND PHRASES' subsequence, common subsequence, algorithm  CR CATEOORIES 3 73, 3 79, 5 25, 5 39  Introduction  We start by defining conventions and terminology that will be used throughout this  paper.  String C = clc~ ... cp is a subsequence of string A = aja2 \"'\" am if there is a mapping  F: {1, 2 .... , p} ~ {1, 2, ... , m} such that F(i) = k only if c~ = ak and F is a monotone  strictly increasing function (i.e. F(i) = u, F(]) = v, and i < j imply that u < v). C can be  formed by deleting m - p (not necessarily adjacent) symbols from A. For example,  \"course\" is a subsequence of \"computer science.\"  String C is a common subsequence of strings A and B if C is a subsequence of A and  also a subsequence of B.  String C is a longest common subsequence (abbreviated LCS) of string A and B if C is  a common subsequence of A and B of maximal length, i.e. there is no common subse-  quence of A and B that has greater length.", "metadata": {"source": "data/17_daniel_1977_algorithm.pdf", "page_number": 1, "chunk": 0, "title": "Algorithms for the Longest Common Subsequence Problem"}}, "566": {"idx": 566, "content": "Throughout this paper, we assume that A and B are strings of lengths m and n, m _< n,  that have an LCS C of (unknown) length p.  We assume that the symbols that may appear in these strings come from some alphabet  of size t. A symbol can be stored in memory by using log t bits, which we assume will fit in  one word of memory. Symbols can be compared (a -< b?) in one time unit.  The number of different symbols that actually appear in string B is defined to be s  (which must be less than n and t).  The longest common subsequence problem has been solved by using a recursion  relationship on the length of the solution [7, 12, 16, 21]. These are generally applicable  algorithms that take O(mn) time for any input strings of lengths m and n even though  the lower bound on time of O(mn) need not apply to all inputs [2]. We present  algorithms that, depending on the nature of the Input, may not require quadratic time  to recover an LCS. The first algorithm is applicable in the general case and requires  O(pn + n log n) time. The second algorithm requires time bounded by O((m + 1 - p)p  log n).", "metadata": {"source": "data/17_daniel_1977_algorithm.pdf", "page_number": 1, "chunk": 1, "title": "Algorithms for the Longest Common Subsequence Problem"}}, "567": {"idx": 567, "content": "In the common special case where p is close to m, this algorithm takes time  Copyright \u00a9 1977, Associatton for Computing Machinery, Inc General permission to republish, but not for  profit, all or part of this material ts granted provtded that ACM's copyright notice is gl~,en and that reference is  made to the publication, to Its date of ~ssue, and to the fact that reprinting privileges were granted by  permission of the Association for Computing Machinery.  This research was supported by a National Science Foundation graduate fellowship and by the National Science  Foundation under Grant GJ-35570.  Author's present address' Department of Electrical Engineering, Rice Umverslty, Houston, TX 77001  Journal of the Assoclauon for Computing Machinery. Vol 24. No 4. October 1977. pp 664-675  .", "metadata": {"source": "data/17_daniel_1977_algorithm.pdf", "page_number": 1, "chunk": 2, "title": "Algorithms for the Longest Common Subsequence Problem"}}, "568": {"idx": 568, "content": "Algorithms for the Longest Common Subsequence Problem  665  much less than n z. We conclude with references to other algorithms for the LCS  problem that may be of interest.  pn Algorithm  We present in this section algorithm ALGD,  which will find an LCS in time O(pn +  n log n) where p is the length of the LCS. Thus this algorithm may be preferred for  applications where the expected length of an LCS is small relative to the lengths of the  input strings.  Some preliminary definitions are as follows:  We represent the concatenation of strings X and Y by XIIY.  Ai, represents the string ala2 \"\" a, (elements 1 through i of string A). Similarly, the  prefix of length j of string B is represented by Bu.  We define L(i,j) to be the length of the LCS of prefixes of lengths i andj of strings A  and B, i e. the length of the LCS orAl, and Bo.  (t, j) represents the positions of a, and b, the ith element of string A and the jth  element of string B. We refer to i (/) as the t-value (j-value) of (1, j).  We define {(0, 0)} to be the set of O-candidates, and we define (i, j} to be a k\u00b0  candtdate (for k -> 1) if a, = b e and there exist i' and j' such that i' < i, j' < j, and  (i',j') is a (k - 1)-candidate. We say that (i',j')generates (i,j).  Define a0 = b0 = $ where $ is some symbol that does not appear in strmgsA or B.  LEM~A 1.  For k -> 1, (t, j) is a k-candidate iff L(i, j) >- k and a~ = bj. Thus there is a  common subsequence of length k of A l~ and B w  PROOF.", "metadata": {"source": "data/17_daniel_1977_algorithm.pdf", "page_number": 2, "chunk": 0, "title": "Algorithms for the Longest Common Subsequence Problem"}}, "569": {"idx": 569, "content": "By induction on k. (i, j) Is a 1-candidate iff a, = be (by definition), in which  case L(i, j) necessarily is at least 1. Thus the lemma is true for k = 1. Assume it is true  for k - 1. Consider k. If (t, j) is a k-candidate then there exist i' < i and j' < j such  that (i',j') is a (k - 1)-candidate. By assumption, there is a common subsequence D' =  dldz \"\" dk-l of Ale, and B~j,. Since a~ = bj ((i, j) is a k-candidate), D = D' Ila~ Is a  common subsequence of length k of Aim and B~. Thus L(i, 1) -> k.  Conversely, if L(i, j) >- k and a~ = b j, then there exist i' < i andj' < j such that ae =  bj, and L(i', 1') = L(i, j) -  1 -> k -  1.  (i', j') is a (k - D-candidate (by inductive  hypothesis) and thus (i, j) is a k-candidate.  []  The length of an LCS is p, the maximum value of k such that there exists a k-  candidate. As we shall see, to recover an LCS, it suffices to maintain the sequence of a 0-  candidate, 1-candidate .... , (p -  1)-candidate, and a p-candidate such that in this  sequence each/-candidate can generate the (i + 1)-candidate for 0 --< i < p.  Rule.  Letx = (x~,x2) andy = (ya, y~) be two k-candidates. Ifx~ >- y~ andx2 ~- Y2,  then we say that y rules out x (x ~s a superfluous k-candidate) since any (k + 1)-  candidate that could be generated by x can also be generated by y. Thus, from the set  of k-candidates, we need consider only those that are minimal under the usual vector  ordering. Note that lfx and y are minimal elements then x~ < y~ iffx2 > Y2.", "metadata": {"source": "data/17_daniel_1977_algorithm.pdf", "page_number": 2, "chunk": 1, "title": "Algorithms for the Longest Common Subsequence Problem"}}, "570": {"idx": 570, "content": "LEMraA 2.  Let the set off k-candtdates be {(i~, 1~)} (r = 1, 2 .... ). We can rule out  candidates so that (after renumbering) t~ < i2 < \"'\" and j~ > j2 > \"\".  PRoov.  Any two k-candidates (l, j) and (t', j') satisfy one of the following (without  loss of generality, i -< i'):  (1) i <t', j-<j'.  (2) i<i'.j  >j'.  (3) t =i', j--<j'.  (4) i =t', j >j'.  In cases (1) and (3) (t',j') can be ruled out; m case (4) (i,j) can be ruled out; and case (2)  satisfies the statement of the lemma. Thus any set of k-candidates whtch cannot be  reduced by further application of the rule will satisfy the condition stated in the  lemma'.  []  The set of k-candidates, reduced by apphcatmn of the rule so as to satisfy the  statement of Lemma 2, are the minimal elements of the set of k-candidates (since no  .", "metadata": {"source": "data/17_daniel_1977_algorithm.pdf", "page_number": 2, "chunk": 2, "title": "Algorithms for the Longest Common Subsequence Problem"}}, "571": {"idx": 571, "content": "666  DANIEL $. HIISCHltEIIG  element can rule out a minimal element) and will be called the set of minimal k-  candidates. By Lemma 2, there is at most one minimal k-candidate for each i-value.  We note that if (i, j) is a minimal k-candidate then L(/, j) = k and (i, ]) is the k-  candidate with/-value i having smallest j-value] such that L(i, j) = k.  LEMMA 3.  For k -~ 1, (i, ]) is a mintmal k.candtdate iff ] is the minimum value such  that b~ ffi a, and low < ] < high, where high is the minimum ].value of all k-candidates  whose i-value is less than i (no upper limit if there are no such k-candidates) and low is the  mmimum ]-value o fall (k - 1)-candidates whose i-value is less than i.  PROOF. Assume that (i, ]) is a minimal k-candidate. If] -> high then there is a k-  candidate ~/', j') such that i' < i and]' = high _< j. <i, j) would be ruled out by ~/', j')  and thus would not be minimal.  If] _< low, then there is no (k - 1)-candidate that can generate (i, j). (i, j) would not  be a k-candidate.  bj = at is required by the definition of k-candidate and low < j < high has just been  shown. If] and ]' both satisfy these constraints, j < j', then (i, j'> is ruled out by (i, j).  Thus, for a particular i, j must be the minimum #value of all k-candidates satisfying  these constraints.  The if of the lemma has thus been shown.  The converse is easily shown: If (i, j) is not a k-candidate, then either at ~ b~ or there is  no (k - 1)-candidate that can generate <i,]).", "metadata": {"source": "data/17_daniel_1977_algorithm.pdf", "page_number": 3, "chunk": 0, "title": "Algorithms for the Longest Common Subsequence Problem"}}, "572": {"idx": 572, "content": "That is, thej-value of all (k - 1)-candidates  with/-value less than i is greater than or equal to ]. This is equivalent to ] _< low.  If {i,/) is a k-candidate but is not minimal, say (i',]') rules out (i,]), then i' -< i and]' _<  ]. If/' < i, then clearly] < high is violated. Otherwise, i' = i. In this case]' > low since  (i', j'> must be generated from a (k - 1)-candidate and b~, = a, since <i', j) is a k-  candidate. Also# <j < high. Thusj' satisfies all the constraints and] is not the minimum  value that does so, a contradiction.  []  We present algorithm ALGD, which, using the results of Lemma 3, obtains an LCS  C of length p of input strings A and B in time O(pn + n log n).  The algorithm is based on an efficient representation of the L matrix. Since L is  nondecreasing in both arguments, we may draw contours in its matrix as shown in the  following example:  B  c  b o c  b o  a b o  A  0  ,  ,  ,  ,  ,  b  O0 ~,,.L) I  ~. ~ 2 2 2 2  c  i~11  1  2~1  I  2 2 2 2 2  d  22  222  b Ill I 2~2 2 ~ 3 3 3 3  b  2  3 3 ~4~-  The entire matrix is specified by its contours. The contours are described by sets of  minimal k-candidates. The contour between L-values ofk - 1 and k is defined by the set  of minimal k-candidates whose elements are positioned at the convex corners of the  contour.  To keep track of the minimal k-candidates, we use the matrix D. D[k, i] is the j-value  of the unique minimal k-candidate having/-value of i or 0 if there is no such minimal k-  candidate.", "metadata": {"source": "data/17_daniel_1977_algorithm.pdf", "page_number": 3, "chunk": 1, "title": "Algorithms for the Longest Common Subsequence Problem"}}, "573": {"idx": 573, "content": "Thus D[k, i] describes the contours by giving the number of the first column  of row i that is in region k (if that number is different from D[k, ~ - 1]).  .", "metadata": {"source": "data/17_daniel_1977_algorithm.pdf", "page_number": 3, "chunk": 2, "title": "Algorithms for the Longest Common Subsequence Problem"}}, "574": {"idx": 574, "content": "Algorithms for the Longest Common  Subsequence Problem  667  lowcheck is the smallest/-value of a (k - 1)-candidate. FLAG has value 1 iff there are  any k-candidates.  NB[O] is the number of times symbol 0 occurs in string B.  PB[O,  1] .....  PB [0, NB [0 ]] is the ordered list, smallest first, of positions in B in which symbol 0 occurs.  If t, the size of the symbol alphabet, is not large compared to n, then we may index an  array by the bit representation of a symbol. Otherwise, if t >> n, then we construct a  balanced binary search tree which provides a mapping from symbols that appear in string  B to the integers 1 through s (there are s different symbols that appear in B). Whenever  string element a, appears as an array subscript (as in N[a,]), it should be understood that  we are indexing N by the integer s, which has been obtained (during initialization for  ALGD)  from traversing the search tree just described. If a, does not appear in B, then  the integer s, is zero. An equivalent assumption is followed for subscript b~ in step 1.  ALGD(m, n, A, B, C, p)  1. NB[O],~-OforO = l,.  ,s  PB[O,O]~--OforO= l,.  ,s  en[0, 0] ~ 0; PB[0, 1] ~ 0  for j *- 1 step 1 until n do  ~in  NB[bj] ~ NB[bj] + 1  eB[bj, NB[b~]] ~ j  end  2 D[O,i]~.-Oforz=O,..  ,m  lowcheck ~-- 0  3. fork ~lstepldo  ~gin  4.  N[O] ~ NB[O] for 0 = 1,  , s  N[0] ~ 1  FLAG ~ 0  low ~ D[k - 1, lowcheck]  high ~ n + 1  5.  for t ~ lowcheck + 1 step 1 until m do  begin  6.", "metadata": {"source": "data/17_daniel_1977_algorithm.pdf", "page_number": 4, "chunk": 0, "title": "Algorithms for the Longest Common Subsequence Problem"}}, "575": {"idx": 575, "content": "while PB[ai, N[at] - 1] > low do N[a~] ~ N[a,] -  1  7  if htgh > PB[a~, N[a~]] > low  then begin  htgh ~ PB[a,, N[a,]]  D[k, t] <--high  if FLAG = 0 then {lowcheck ~-- t, FLAG ~-- 1}  end  else D[k, ,] *- 0  8  if D[k - 1, t] > 0 then low ~ D[k -  l, t]  end loop of step 5  9. if FLAG = 0 then go to step 10  end loop of step 3  10 pc--k-1  k~-.-p  fort~m  + 1 step-  luntilOdo  if D[k, ~] > 0 then  I~gin  Ck ~- ai  k,,--k-1  end  The loop of step 3 evaluates the set of minimal k-candidates for k = 1, 2, .... The loop  of step 5 evaluates the set of minimal k-candidates, smallest/-value first, and fills in the  D array accordingly (in the example given previously this is left-to-right) while scanning  the chains of occurrences of a given character in B with largestj-value first (right-to-left).  For each i, i can be the/-value of a minimal k-candidate if there is a ] satisfying the  constraints of Lemma 3. This is tested by determining the minimum]-value of symbol a,  that is greater than low, If that value is less than htgh, then (i,]) is a minimal k-candidate.  .", "metadata": {"source": "data/17_daniel_1977_algorithm.pdf", "page_number": 4, "chunk": 1, "title": "Algorithms for the Longest Common Subsequence Problem"}}, "576": {"idx": 576, "content": "668  DANIEL S. HIRSCHBERG  There can be no k-candidate with/-value less than or equal to lowcheck, so the loop of  step 5 begins at lowcheck + 1. lowcheck is set, in step 7, when the first minimal k-  candidate (that having smallest/-value of all k-candidates) is determined.  LEMMA 4. ALGD evaluates the correct values of high and low (as defined m  Lemma 3) for determining whether each k-candidate (i, j) is minimal.  PRoov. high is supposed to be the minimum ]-value of all k-candidates with/-value  less than i high is imtialized at n + 1 (i.e. does not limit) in step 4, before any k-  candidates have been generated. Thereafter, if any k-candidates are found to be minimal  (in step 7), then, since the ]-values of minimal k-candidates decrease as the t-values  increase, the mimmumj-value of all minimal k-candidates with t-value less than i will be  the ]-value of the minimal k-candidate with greatest/-value less than i (i.e. the last one  found, since we generate minimal k-candidates in order of increasing/-value). The j-  values of ruled-out (nonminimal) k-candidates cannot be smaller than the ]-value of the  last minimal k-can&date high is updated to the most recent/-value each time a new  minimal k-candidate is found in step 7. Thus high has value as defined in Lemma 3.  low is supposed to be the minimum ]-value of all (k - 1)-candidates whose/-value is  less than i.", "metadata": {"source": "data/17_daniel_1977_algorithm.pdf", "page_number": 5, "chunk": 0, "title": "Algorithms for the Longest Common Subsequence Problem"}}, "577": {"idx": 577, "content": "Again, sincej-values decrease as/-values increase, low should be thej-value  of the (k - 1)-candidate whose/-value is as great as possible but less than i. low is  initialized in step 4 to be the ]-value of the first (lowest/-value) (k - 1)-candidate. As i  increases, if there was a minimal (k - 1)-candidate with/-value of i, then the mimmum  permissible j-value will decrease and low is updated (in step 8) for the next iteration.  []  LEMMA 5. ALGD correctly determines the set of minimal k-candtdates.  PaooF.  By Lemma 4, high and low are computed correctly. We must show that in  the loop of steps 5-8 D[k, i] gets the mlnimumj-value (0 if none) such that b~ = a~ and  low < j < high.  The ]-values of successive minimal k-candidates decrease in value since their/-values  increase. In looking for D[k, i] we look for a match for symbol a, in string B, and we  can restrict our attention to occurrences (j-values) of symbol as in string B that are  before (less than) the last occurrence (]-value) that was examined. Step 6 does that.  PB[a, o] is the ordered list of j-values of symbol a, and N[a,] points to the smallest  ]-values (in PB) of symbol at that has been examined. Initially, m step 4, N[a,] points to  the last occurrence of symbol at. If the last-examined ]-value of a, is greater than low,  step 6 sets N[a,] to point to the lowestj-value of at that is greater than low.", "metadata": {"source": "data/17_daniel_1977_algorithm.pdf", "page_number": 5, "chunk": 1, "title": "Algorithms for the Longest Common Subsequence Problem"}}, "578": {"idx": 578, "content": "If the last-  examined ]-value of a, is not greater than low, then there can be no minimal k-  candidate for this value of i since the minimum ]-value that is greater than low either  violates the high constraint or results in a candidate that can be ruled out. In this case  step 6 does nothing, the test in step 7 fails, and D[k, i] is set to zero.  []  THEOREm 1. ALGD correctly computes the LCS of strings A and B.  PROOF.  By Lemma 5, ALGD correctly determines the set of minimal k-candidates.  Thus, if there are any k-candidates, at least one is minimal. If (t, j) is the pth match in  an LCS which is of length p, then, by Lemma 1, (/, j) is a p-candidate. Thus there is at  least one minimal p-can&date (and there are no (p + D-candidates). Step 10 of  ALGD recovers a common subsequence of length p by recovering a sequence of  (/-values o0 minimal candidates such that the minimal k-candidate generated the  minimal (k + D-candidate.  []  THEOREI~ 2. Assuming that symbols can be compared in one time unit, ALGD  requires time of O(pn + n log s), where s is the number of different symbols that appear  in string B.  PROOF. Step 1 can be done in time O(n log s). Step 2 can be done in time O(m).  Step 3 executes steps 4-9 p times. Step 4 takes time O(s) per execution, s -< n, for  total time less than or equal to O(pn). Step 5 executes steps 6-8 at most m times, a  total of at most pm times.", "metadata": {"source": "data/17_daniel_1977_algorithm.pdf", "page_number": 5, "chunk": 2, "title": "Algorithms for the Longest Common Subsequence Problem"}}, "579": {"idx": 579, "content": "The while loop in step 6 is executed at most n times within  the loop of step 5 since the N[O] are not increased within this loop (each position of B is  examined at most once for each value of k). The total time in step 6 is therefore O(pn)  .", "metadata": {"source": "data/17_daniel_1977_algorithm.pdf", "page_number": 5, "chunk": 3, "title": "Algorithms for the Longest Common Subsequence Problem"}}, "580": {"idx": 580, "content": "Algorithms for the Longest Common Subsequence Problem  669  Steps 7 and 8 are done in constant time. Total time is O(pm). Step 9 is done in  constant time. Total time is O(p). Step 10 is done in time O(m). Total execution time is  thus as stated above.  []  Note that for p _> O(log s), ALGD requires time O(pn).  pe log n Algorithm  We now consider a special case that often occurs in applications such as determining  the discrepancies between two files, one of which was obtained by making minor  alterations to the other (and we wish to recover those alterations). We assume that  there is an LCS of length at least m - ~ (for some given ~).  If C is an LCS of A and B, there will be at most ~ elements of A that do not appear in  C. The position of each such element will be called a skipped position. Thus there are at  most E skipped positions. We define e to be ~ + 1.  If (t,j) is a minimal k-candidate that can be an element in an LCS (that is, a, = bj is the  kth element of an LCS), then k -< i -< k + \u00a2 (otherwise more than E positions inA would  be skipped). We shall call such candidates feasible k-candidates. Let h = i - k. Then 0 -<  h <- ~ and h is the number of positions in A that have been skipped thus far (through  ak+h). By Lemma 2, there is at most one feasible k-candidate with/-value of i.  Let the feasible k-candidate pairs (/-value andj-value) be held in arrays F and G, e.g.  (h + k,/) would be described by F[h] = h + k, G[h] = j.", "metadata": {"source": "data/17_daniel_1977_algorithm.pdf", "page_number": 6, "chunk": 0, "title": "Algorithms for the Longest Common Subsequence Problem"}}, "581": {"idx": 581, "content": "If there is no feasible k-  candidate with/-value h + k, let F[h] = F[h - 1], G[h] = G[h - 1], and define F[-1]  = 0, G[-1] = n + 1. By this construction and by Lemma 2, F is a nondecreasing  sequence and G is a nonincreasmg sequence.  Define NEXTB(O, j) to be the minimum r > 1 such that br = 0. If there is no such r,  then NEXTB(O, j) is defined to be n + 1.  LEMMA 6.  If (i, j) is a feasible k-candidate, then j = NEXTB(a, G[h]), where h = i -  k and where G[h] is the value assoctated with the set or feasible (k - 1)-candidates.  PROOF. Let (i, j) be a feasible k-candidate. By definition of k-candidate, there must  exist i' < i and f < j such that <i', j') is a feasible (k - 1)-candidate. By Lemma  3, j is the minimum (over possible j') of NEXTB(a,, j'). But j\" < j' implies that  NEXTB(O, j\") _< NEXTB(O, ]'). Therefore I = NEXTB(a,, mm possible j'). Since j-  values of minimal k-candidates decrease as their /-values increase, the minimum  possible j' is the/-value of the feasible (k - 1)-candidate whose t-value is as large as  possible but less than i = h + k, i.e. not more than h + (k - 1). G[h] is precisely that  /-value. So we conclude thatj = NEXTB(a, G[h]).  []  In order to be able to recover an LCS, we shall keep track (for each feasible k-  candidate) of which h positions in A have been skipped. A straightforward method,  keeping values of F[h] for all h and k, requires space of O(pc).", "metadata": {"source": "data/17_daniel_1977_algorithm.pdf", "page_number": 6, "chunk": 1, "title": "Algorithms for the Longest Common Subsequence Problem"}}, "582": {"idx": 582, "content": "We shall use a data  structure that requires only O(e 2 + n) space without changing the order of magnitude of  time requirements.  Let there be an array KEEP whose elewents are trtples such that  KEEP[x] = (aa[x], nskip[x], pt Ix]).  P is an array of size e such that, after the set of feasible k-candidates has been  determined, x = P[h] will be the index of the element of KEEP that has information  enabhng recovery of a common subsequence that has aFtn] = bGtnj as its kth element. F[h]  = h + k, and thus precisely h of the elements a~, ..., aFthl will not appear in the common  subsequence. To recover the common subsequence, it is sufficient to recover these h  skipped positions. Ifx = 0, then no positions were skipped, and ifx < 0, then there is no  common subsequence to be recovered.  The method of recovery is as follows:  If x is zero, there are no more skipped positions to be recovered.  Otherwise, aa[x] is the largest index of a skipped position in string A. nskip[x] is the  number of consecutive positions ending in aa[x], all of which are skipped positions.  .", "metadata": {"source": "data/17_daniel_1977_algorithm.pdf", "page_number": 6, "chunk": 2, "title": "Algorithms for the Longest Common Subsequence Problem"}}, "583": {"idx": 583, "content": "670  DANIEL S. HIRSCHBERG  If all of the skipped A-positions have been recovered, then pt[x] is zero.  Otherwise, pt[x] is the index of KEEP  that has information enabling recovery of the  skipped A-positions having indices smaller than aa[x] - nskip[x] + 1.  Example.  If positions 2, 5, 6, 7, 9, 10 in string A correspond to a common  subsequence of length 6 (of A1, ~0), then h = 4 and KEEP[ P[4]] will enable recovery of  positions 1, 3, 4, 8: aa[P[4]] = 8, nskip[P[4]] = 1, pt[P[4]] = y (another index of  KEEP).  an[y] = 4, nskip[y]  = 2 (positions 3 and 4 have been skipped), pt[y] =  z. an[z] = 1, nsklp[z] = 1, pt[z] = 0 (all skipped positions have been recovered).  Reference counts are kept for each element of KEEP.  Spaces in the KEEP  array are  maintained by garbage collection functions GETSPACE  which provides an available  space and PUTSPACE  which places a newly available space (i.e. one whose reference  count drops to zero) on the garbage linked list. See [10] for implementation techniques.  We now present ALGE,  which uses Lemma 6 in order to solve the LCS problem in  time O(pe log n):  ALGE (m, n, A, B, C, p, e)  1 F[h], G[h] ~ 0 for h = 0 .... \u2022  P[O] ~-- O; P[h]~---lforh  = 1,  ,\u2022  2  for k ~ 1 step 1 while there were candidates found m the last pass do  begin  3  lmax ~-- 0  jrmn ~ n + 1  4  for h ~ 0 step 1 until e do  begin  5  t~--h+k  J ~ NEXTB(a.", "metadata": {"source": "data/17_daniel_1977_algorithm.pdf", "page_number": 7, "chunk": 0, "title": "Algorithms for the Longest Common Subsequence Problem"}}, "584": {"idx": 584, "content": "G[h])  ff l -> I mm  6  then begin  F[h ] ~ lmax  G[h] ~ Imm  NEWP[h ] <--- - 1  end  7  else begin  nsktp ~ (, - 1) -F[h]  if nsklp = 0  then NEWP[h] ~ P[h]  else begin  NEWP[h] ~ GETSPACE  KEEP[NEWP[h]] ~ (l - 1, nsktp, P[h - nsktp])  end  8  lmax ~ l  I mm *-1  F[h ] ~ l  G[h] ~'-1  end  9  end loop of step 4  10.  if no k-candidates were found then gotn step 13  for : *- 0 step l until \u2022 do  begin  11  REMOVE(PIt])  Pit] ~ NEWP[t]  end loop of step 10  12 end loop of step 2  13 x ~ mm h such that P[h] _> 0, -1 if none such  pc--k-1  ifx < 00Rp < m - \u2022 then {print \"NO\", gain step 15}  14. RECOVER  15 END of ALGE  SUBROUTINE RECOVER  1 SKIP[x + 1] <-- 0  lastmatch <-- Fix]  y ~ P[x]  .", "metadata": {"source": "data/17_daniel_1977_algorithm.pdf", "page_number": 7, "chunk": 1, "title": "Algorithms for the Longest Common Subsequence Problem"}}, "585": {"idx": 585, "content": "Algortthms for the Longest Common Subsequence Problem  671  2 whiley ~ do  begin  count ~ nsk~p[y]  posttton ~ aa[y]  3  while count > 0 do  begin  SKIP[x] ~ posttlon  x~-x-1  positzon ~-- posttton - 1  count ~-- count - 1  end loop of step 3  y ~- pt[y]  end loop of step 2  4. x ~-- 1  k~-I  for t ~- 1 step 1 until lastmatch do  if~ = SKIP[x] thenx ~--x + 1  else begin  Ck 4.-- az  k~-k+l  end  5 END OF RECOVER  The loop of step 2 evaluates sets of feasible k-candidates for k = 1, 2, .... The loop  of step 4 evaluates whether there ~s a teasible k-candidate having precisely h skipped  positions, for h = 0, 1, ... , e, by using Lemma 6 to determine the j-value for a  particular t-value and then checking, by using Lemma 2, whether (i, j} is minimal, imax  is the maximum t-value of feastble k-candidates generated thus far (i.e. wtth/-values  less than the current value of i); jmm is the corresponding j-value (which is the  minimum j-value of feasible k-candidates generated thus far). If (i, j} is a feasible k-  candidate, then it is stored in the F and G arrays and information wtll be stored in P[h],  enabhng recovery of any additional skipped positions that occur between i and F[h] as  well as the skipped positions occurring before F[h] ((F[h], G[h]) is a (k - 1)-candidate  that can generate (i, ])). The h skipped positions corresponding to (F[h], G[h]) are  recoverable by accessing KEEP[P[h]].", "metadata": {"source": "data/17_daniel_1977_algorithm.pdf", "page_number": 8, "chunk": 0, "title": "Algorithms for the Longest Common Subsequence Problem"}}, "586": {"idx": 586, "content": "In general there may be more than one feasible  k-candidate that will be generated by (F[h], G[h]). Thus we must not destroy P[h] until  all required references to KEEP[P[h]] are made. For this reason, new values for the P  array are stored in the NEWP array. When we no longer need the old values of P (after  the inner loop of steps 4-9), we can then replace them with the new values, being  careful to decrement reference counts of KEEP elements that were pointed to by the  old P array  Function REMOVE(x)  decrements the reference count of KEEP[x] (unless x --< 0, in  which case nothing is done), and, if KEEP[x] now has reference count zero, then a call  will be made to REMOVE(pt[x]) after KEEP[x] has been put on the garbage linked list  by using PUTSPACE.  Implementation of NEXTB  The following should be done before using ALGE:  1 Sort the symbols m A and then construct a balanced binary search tree of symbols that appear in string A  Let there be ss such symbols (ss -< m).  2. for k ~  1 step 1 until ss do LAS'I~k] ,-- 0  3  for i ~-- 1 step 1 until n do  begin  find out that bt = 0k  1 ~ LAST[k]  LAST[k] ~ t  if I ~ 0 then NEXT[i ] ~-- t  else FIRST[k] ~-- t  end loop of step 3  .", "metadata": {"source": "data/17_daniel_1977_algorithm.pdf", "page_number": 8, "chunk": 1, "title": "Algorithms for the Longest Common Subsequence Problem"}}, "587": {"idx": 587, "content": "672  DANIEL S. HIRSCHBERG  4. start z.- 1  for k ~ 1 step 1 until ss do  bet~Jn  Place the positions j of B such that bj = 0~ into N[start] through N[start + nn - 1] where 0~ occurs nn  times in string B. The first posmon in B at which 0k occurs Is at FIRST[k]. If 0~ occurs at position j, then  the next occurrence of 0~ in B wdl be at posinon NEXT[i ] unless LAST[k] = j, in which case there arc no  more occurrences of 0~ in B.  S[k] ~ start  start ~ start + nn  end  We can find out that a, = 0k m time O(logs). N[S[1]:S~k + 1] - 1] holds the block  of positions j with b e = 0~. This block of cells can be searched by using binary search of  a hnearly ordered array [11, Sec. 6.2.1]. NEXT(a, j) can thus be executed in time  O(log n).  If s is very small, then the following alternate way of computing NEXTB(O, j) may be  preferred: Instead of constructing a compressed array in step 4, construct a NEXTB  matrix while in step 3. For each l, set NEXTB[k, t] = i forj _< t < i. This will result in  time and space complexity (of the setup) of O(sn). The function NEXTB(O, j) can be  evaluated by determining that 0 = 0k in time O(log S) and by doing a simple table look-  up.  ALGE retains k-candidates, as did ALGD, except for those candidates that cannot  lead to a sufficiently long common subsequence because too many A-positions have  already been skipped. The (k + D-candidates that can be generated by the dropped k-  candidates also skip too many A-positions.  LEMMA 7.", "metadata": {"source": "data/17_daniel_1977_algorithm.pdf", "page_number": 9, "chunk": 0, "title": "Algorithms for the Longest Common Subsequence Problem"}}, "588": {"idx": 588, "content": "ALGE retains all feasible k-candidates.  PROOF. By induction on k. It is trivially true for k = 0 (the F and G arrays are  initialized to zero in step 1). Assume that the set of feasible (k - 1)-candidates has been  evaluated and stored in arrays F and G. ALGE generates the set of feasible k-  candidates in order of increasing/-value. F[h] is to hold i = h + k if i is an/-value of a  feasible k-candidate; otherwise F[h] Is to hold the maximum i' < i such that i' is a  feasible k-candidate. G[h] is to hold the corresponding j-value, imax and jmin hold the  last-generated feasible k-candidate, which, by Lemma 2, has the maximum/-value and  minimumj-value generated thus far. Step 3 initializes them to correctly indicate that no  k-candidates have yet been generated. Step 5 evaluates the j-value for a given potential  k-candidate by using Lemma 6. Ifj _>jmin then, even though the necessary condition for  feasibility has been met, (i, ]) is not minimal since it would be ruled out by (imax, jmin).  In this case step 6 sets F[h] and G[h] to imax andjmin. Ifj < jmin, then (i,j) is minimal  since it cannot be ruled out by any previously generated k-candidate (/ < jmm) and it  cannot be ruled out by any future generated k-candidate (all future i' > i). In this case  step 8 sets F[h] and G[h] and also updates tmax and jmin.  []  THEOREM 3.  ALGE correctly computes the LCS of strings A and B if the LCS is of  length at least m - ~.  PROOF.", "metadata": {"source": "data/17_daniel_1977_algorithm.pdf", "page_number": 9, "chunk": 1, "title": "Algorithms for the Longest Common Subsequence Problem"}}, "589": {"idx": 589, "content": "By Lemma 7, ALGE correctly keeps minimal k-candidates. Thus, if there  is a common subsequence of length p .~ m - \u00a2, then there is a minimal p-candidate  which will be feasible. The data structure of ALGE keeps track, for each feasible k-  candidate (t, j), of the h = i - k positions in string A that have been skipped in the  common subsequence of length k of At, and Bls. P[h] points to the element of KEEP  that contains the necessary information. P is updated in step 7 when a feasible k-  candidate is generated. If any additional positions are skipped (between the k-candidate  (i, j) and the (k - 1)-candidate (i', j') that generated (i, ])), then that information is  recorded in an element of KEEP as well as a pointer, enabling recovery of the h -  nskip previously skipped A-positions (of (i', j')). Subroutine RECOVER recovers the  skipped posiUons of a feasible p-candidate by reversing the process in which they were  stored and then computes the LCS by deleting the skipped positions from string A.  []  THEOREM 4.  For ~ _< O(nlt2), ALGE requires space linear m n.  .", "metadata": {"source": "data/17_daniel_1977_algorithm.pdf", "page_number": 9, "chunk": 2, "title": "Algorithms for the Longest Common Subsequence Problem"}}, "590": {"idx": 590, "content": "Algorithms for the Longest Common Subsequence Problem  673  PROOF. The KEEP array requires O(e 2) space: The common subsequence implied by  k-candidate (h + k,j~ has h skipped A-positions, h <- ~, and thus can use at mosth spaces  in the KEEP array. The total number of spaces referred to by all feasible k-candidates is  thus at most ~(e + 1)/2. Adding to that the (exactly) E references to get the set of feasible  (k + 1)-candidates gives a total of no more than (e 2 + e)/2. Each element of array KEEP  requires four words (aa, nskip, pt, and a reference counter).  The arrays and space that they use are as follows: Fie], G[e], C[p], Pie], NEWP[e],  KEEP[2e 2 + 2e], FIRST[ss], NEXT[n], LAST[ss], SKIP[e], S[ss], N[n].  The NEXTB function requires at most 2n locations to store the various balanced  binary search trees.  Thus a total of at most 2e ~ + 7e + 4n + p + 3ss locations is used. Fore -< 0(nl/2), space  requirements are linear in n.  []  THEOREm 5. ALGE requires time O(pe log n).  PROOF. Preprocessing for the NEXTB function requires time O(n log m). Step 1  takes time O(e). Step 2 executes steps 3-12 p times. Step 3 takes constant time for a  total time of O(p). Step 4 executes steps 5-9 at most e times. Step 5 takes time O(log n)  for a total time of O(pe log n). Steps 6-9 take constant time for a total time of O(pe).  Steps 10-12, excluding time spent in function REMOVE, take time O(e) for a total time  of O(pe).", "metadata": {"source": "data/17_daniel_1977_algorithm.pdf", "page_number": 10, "chunk": 0, "title": "Algorithms for the Longest Common Subsequence Problem"}}, "591": {"idx": 591, "content": "Subroutine RECOVER recovers at most c skipped positrons (taking time O(e)) and  then deletes them from string A (taking time O(m)) for a total time of O(m).  The number of references (to array KEEP) removed is at most the number of  references inserted. There are at most pe references inserted (one per execution of step  7), and the amount of time (per reference removal) spent in function REMOVE is  constant. Therefore the total time spent in function REMO VE is O(pe).  Therefore the total time of execution of ALGE is O(pe log n).  []  It is noted that step 5, reqmring O(log n) time, is the bottleneck, causing total time  requirements of O(pe log n). P. van Emde Boas's recent algorithm for priority queues  [19] appears capable of solving the position-finding problem in time O(log log n). If so,  this would reduce the time bound of this problem to O(pe log log n).  ALGE assumes that c is known. If ~ is not known, then set E ~-- 2 and proceed  through the algorithm. If that value of ~ is insufficient (i.e. there is no common  subsequence of length m - e), then double the guess for e and continue iteratively until  a common subsequence is found.  Total time spent will be (letting k be the multiplicative coefficient of the time  requirement)  2pk logn + 4pk logn + ... + epk logn,  which is less than 2pek log n. Since e < 2(m + 1 - p), we can recover an LCS in time  O(p(m + 1 - p)log n).", "metadata": {"source": "data/17_daniel_1977_algorithm.pdf", "page_number": 10, "chunk": 1, "title": "Algorithms for the Longest Common Subsequence Problem"}}, "592": {"idx": 592, "content": "Other Algorithms  The only known algorithm for the LCS problem with worst-case behavior less than  quadratic is due to Paterson [14]. The algorithm has complexity O(n~log log n/log n). It  uses a \"Four Russians\" approach (see [3] or [1, pp. 244-247]). Essentially, instead of  matrix L (where L[t, j] is the length of an LCS of A1, and Btj) being calculated one  element at a time (see [7]), the matrix is broken up into boxes of some appropriate size  k. The high sides of a box (the 2k - 1 elements of L on the edges of the box with  largest indices) are computed from L-values known for boxes adjacent to it on the low  side and from the relevant symbols of A and B by using a look-up table which was  precomputed.  The algorithm assumes a fixed alphabet size although modifications to the algorithm  may be able to get around that condition.  .", "metadata": {"source": "data/17_daniel_1977_algorithm.pdf", "page_number": 10, "chunk": 2, "title": "Algorithms for the Longest Common Subsequence Problem"}}, "593": {"idx": 593, "content": "674  DANIEL S. HIRSCHBERG  There are 2k + 1 elements of L adjacent to a box on the low side. Two adjacent L-  elements can differ by either zero or one. There are thus 22k possibilities in this respect.  The A- and B-values range over an alphabet of size s for each of 2k elements, yielding a  multiplicative factor ofs zk, and the total number of boxes to be precomputed is therefore  2 ~kt~+l\u00b0gs~. Each such box can be precomputed in time O(k ~) for a total precomputing  time of O(k~22k~+l\u00b0g ~).  There are (n/k) z boxes to be looked up, each of which will require O(k log k) time to  be read, for a total time of O(n21og k/k).  The total execution time will therefore be O(k~2 ~k~+~\u00b0g s~ + n21og k/k). If we let k = log  n/2(1 + log s), we see that the total execution time will be O(n21og log n/log n).  Restrictions on the LCS Problem  Szymanski [17] shows that ff we consider the LCS problem with the restriction that no  symbol appears more than once within either input string, then this problem can be  solved in time O(n log n).  In addition if one of the input strings is the string of integers 1 - n, this problem is  equivalent to finding the longest ascending subsequence in a string of distinct integers. If  we assume that a comparison between integers can be done in unit time, this problem can  be solved in time O(n log log n) by using the techniques of van Emde Boas [18}.  ACKNOWLEDGMENT.", "metadata": {"source": "data/17_daniel_1977_algorithm.pdf", "page_number": 11, "chunk": 0, "title": "Algorithms for the Longest Common Subsequence Problem"}}, "594": {"idx": 594, "content": "I would like to thank the (anonymous) referees for their many  helpful suggestions which have led to a material improvement in the readability of this  paper.  REFERENCES  (Note  References [4-6, 8, 9, 13, 15, 20, 22, 23] are not oted m the text.)  1 AHO, A V , HOPCROFr, J E , AND ULLMAN, J D The Design and Analysis of Computer Algortthms  Addison-Wesley, Reading, Mass, 1974  2 Ario, A V, HmSCHBER6, D.S, AND ULLUAN, J D Bounds on the complextty of the longest common  subsequenee problem J ACM 23, 1 (Jan 1976), 1-12.  3 ARLAZAROV, V L,, DINIC, E A, KRONROD, M A, AND FARADZEV, I A On economic construction of  the trans~ttve closure of a dtrected graph Dokl Akad Nauk SSSR 194 (1970), 487-488 (m Russian)  Enghsh transi Jn Sovtet Math Dokl 11, 5 (1970), 1209-1210  4 CnVATAL, V, KLARNER, D A , AND KNUTrI, D E Selected combinatorial research problems STAN-  CS-72-292, Stanford U, Stanford, Cahf, 1972, p 26  5  CItVATAL, W , AND SANKOFF, D Longest common subsequences of two random sequences STAN-CS-  75-477, Stanford U, Stanford, Cahf, Jan 1975  6 HIRSCnBER6, D S On finding maximal common subsequences TR-156, Comptr So Lab, Princeton  U , Princeton, N.J , Aug 1974  7.", "metadata": {"source": "data/17_daniel_1977_algorithm.pdf", "page_number": 11, "chunk": 1, "title": "Algorithms for the Longest Common Subsequence Problem"}}, "595": {"idx": 595, "content": "HIRSCHEERG, D S A hnear space algorithm for computing maximal common subsequences Comm  ACM 18, 6 (June 1975), 341-343  8 HIESCr~EERG, D S The longest common subsequence problem Ph D Tb , Princeton U, Princeton,  NJ,Aug  1975  9 HUN1\", J W , AND SZYMANSKI, T G A fast algorithm for computing longest common suhsequences  Comm ACM 20, 5 (May 1977), 350-353.  10 KNUTn, D E The Art of Computer Programming, Vol 1. Fundamental Algorithms  Addison-Wesley,  Readmg, Mass., sec. ed , 1973  11 KNUTTn, D. E The Art of Computer Programming, Vol 3\" Sorting and Searching. Addison-Wesley,  Reading, Mass., 1973  12 LOWRANCE, R, AND WAGNER, R A An extension of the string-to-string correction problem J ACM  22, 2 (April 1975), 177-183  13 NEEDLEMAN, S B , AND WUNSCH, C D A general method applicable to the search for slmdantles m  the amino acid sequence of two proteins J. Mol Biology 48 (1970), 443-453  14 PATERSON, M.S Unpubhshed manuscript U of Warwick, Coventry, England, 1974  15 SANKOFF, D Matching sequences under deletion/insertion constraints Proc Nat Acad Sct USA 69, 1  (Jan 1974), 4-6  16 SELLERS, P H An algorithm for the distance between two fmlte sequences J. Combmatortal Theory,  Ser A, 16 (1974), 253-258  .", "metadata": {"source": "data/17_daniel_1977_algorithm.pdf", "page_number": 11, "chunk": 2, "title": "Algorithms for the Longest Common Subsequence Problem"}}, "596": {"idx": 596, "content": "Algorithms for the Longest Common Subsequence Problem  675  17 SZYMANSKI, T G  A special case of the maximal common subsequence problem TR-170, Comptr Scl  Lab , Princeton U, Princeton, N J , Jan 1975.  18  VAN EMDE BOAS, P An O(n log log n) on-hne algorithm for the insert-extract rain problem TR 74-  221, Dept Comptr Sol , CorneU U , Ithaca, N Y , Dec 1974  19 VAN EMDE BOAS, P. Preserving order in a forest m less than logarithmic time Conf Rec 16th Annual  Symp on the Foundations of Comptr SOl , Oct 1975, pp 75-84  20 WAGNER, R A  On the complexity of the extended string-to-string correction problem Proc Seventh  Annual ACM Syrup on Theory of Comping , 1975, pp 218-223  21  WAGNER, R A , AND FISCHER, M J The string-to-string correction problem J. ACM 21, 1 (Jan  1974), 168-173  22 WoNo, C K.. AND CRAtqDRA, A.K  bounds for the string editing problem .Jr ACM 23, 1 (Jan 1976),  13-16  23  YAO, C C, AND YAO, F C  On computing the rank function for a set of vectors UIUCDCS-R-75-699,  Dept Comptr Sc~ , U of lllinols at Urbana-Champalgn, Urbana, I11 , Feb 1975.  RECEIVED JUNE 1975, REVISED FEBRUARY 1977  Journal of the Association for Computing Machinery, Vol 24, No 4, October 1977  .", "metadata": {"source": "data/17_daniel_1977_algorithm.pdf", "page_number": 12, "chunk": 0, "title": "Algorithms for the Longest Common Subsequence Problem"}}}